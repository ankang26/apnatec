{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "import spacy\n",
    "import torch\n",
    "from spacy.language import Language\n",
    "from spacy import displacy\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "import statistics\n",
    "import os\n",
    "import json\n",
    "import calendar\n",
    "import holidays\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import hashlib\n",
    "from dateutil.parser import parse\n",
    "import shutil\n",
    "import ast\n",
    "from io import StringIO\n",
    "import requests\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy import displacy\n",
    "import time\n",
    "\n",
    "@Language.component(\"newsent\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        #print(token.text, token.text in (\"â€™s\", \"'s\"))\n",
    "        if token.text.upper() in (\";\", \"--\", \"\\n\\n\", \"\\n\", \"QUARTERLY\", \"STORY\", \"\\n\\n\\n\\n\", \"\\n\\n\\n\"):\n",
    "            #print(\"Detected:\", token.text)\n",
    "            doc[token.i].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "#spacy.require_gpu()\n",
    "nlp = spacy.load(\"../../Summary/NER/RelateEntity/train/model-best-local\")\n",
    "nlp.add_pipe('sentencizer')\n",
    "nlp.add_pipe('newsent', name=\"newsent\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devDataFile = \"../../Summary/DATA/ENTITYRELATION/Dev/dev.tsv\"\n",
    "trainDataFile = \"../../Summary/DATA/ENTITYRELATION/Train/train.tsv\"\n",
    "testDataFile = \"../../Summary/DATA/ENTITYRELATION/Test/test.tsv\"\n",
    "\n",
    "trainDir = \"../../Summary/DATA/ENTITYRELATION/Train\"\n",
    "devDir = \"../../Summary/DATA/ENTITYRELATION/Dev\"\n",
    "testDir = \"../../Summary/DATA/ENTITYRELATION/Test\"\n",
    "\n",
    "def writeTrainingData(writeFile, writeDir):\n",
    "    files = glob.glob(writeDir+\"/*_ER.tsv\")\n",
    "    print(files)\n",
    "    frames = list()\n",
    "\n",
    "    if(len(files) > 0):\n",
    "        for file in files:\n",
    "            print(file)\n",
    "            #df = pd.read_csv(file, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "            df = pd.read_csv(file, sep=\"\\t\", encoding = \"ISO-8859-1\").astype(str)\n",
    "            df = df.dropna()\n",
    "            df = df[df['Sentence1'].notna()]\n",
    "            #print(df)\n",
    "            frames.append(df)\n",
    "    result = pd.concat(frames)\n",
    "    print(result)\n",
    "    result.to_csv(writeFile, sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeTrainingData(trainDataFile, trainDir)\n",
    "writeTrainingData(devDataFile, devDir)\n",
    "writeTrainingData(testDataFile, testDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(trainDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "eval_df = pd.read_csv(devDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "test_df = pd.read_csv(testDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "\n",
    "train_df = train_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "eval_df = eval_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "test_df = test_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "\n",
    "train_df = train_df[[\"input_text\", \"target_text\"]]\n",
    "eval_df = eval_df[[\"input_text\", \"target_text\"]]\n",
    "test_df = test_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "#train_df[\"prefix\"] = \"paraphrase\"\n",
    "#train_df = train_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "#train_df = train_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "#eval_df[\"prefix\"] = \"paraphrase\"\n",
    "#eval_df = eval_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "#eval_df = eval_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "train_df = train_df[train_df['input_text'].notna()]\n",
    "\n",
    "eval_df = eval_df.dropna()\n",
    "eval_df = eval_df[eval_df['input_text'].notna()]\n",
    "\n",
    "test_df = test_df.dropna()\n",
    "test_df = test_df[test_df['input_text'].notna()]\n",
    "\n",
    "#train_df[\"input_text\"] = train_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "#train_df[\"target_text\"] = train_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "print(\"TRAIN DATA ..............\")\n",
    "print(train_df)\n",
    "\n",
    "#eval_df[\"input_text\"] = eval_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "#eval_df[\"target_text\"] = eval_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "print(\"EVAL DATA ..............\")\n",
    "print(eval_df)\n",
    "\n",
    "print(\"TEST DATA ..............\")\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "#original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model_name = \"facebook/bart-base\"\n",
    "\n",
    "#original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#original_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = (train_df[\"input_text\"][0])\n",
    "outputText = (train_df[\"target_text\"][0])\n",
    "\n",
    "prompt = f\"\"\"\n",
    "List named entities from following sentence.\n",
    "\n",
    "{inputText}\n",
    "\n",
    "Entities and Relations:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{outputText}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train = Dataset.from_pandas(train_df)\n",
    "valid = Dataset.from_pandas(eval_df)\n",
    "test = Dataset.from_pandas(test_df)\n",
    "\n",
    "ds = DatasetDict()\n",
    "\n",
    "ds['train'] = train\n",
    "ds['validation'] = valid\n",
    "ds['test'] = test\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    #print(example)\n",
    "    start_prompt = 'Find entities and relations from following sentence.\\n\\n'\n",
    "    end_prompt = '\\n\\nEntities and Relations: '\n",
    "    prompt = [start_prompt + sentence + end_prompt for sentence in example[\"input_text\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"target_text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    #print(example)\n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['input_text', 'target_text', '__index_level_0__',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./feroutputs\"\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    #max_steps=2,\n",
    "    save_steps=1,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./feroutputs/checkpoint-101\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "dialogue = ds['test'][index]['input_text']\n",
    "human_baseline_summary = ds['test'][index]['target_text']\n",
    "print(dialogue)\n",
    "#prompt = dialogue\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Find entities and relations from following sentence.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Entities and Relations :\n",
    "\"\"\"\n",
    "\n",
    "#input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "#original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "#original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "instruct_model_text_output = tokenizer.decode(\n",
    "    instruct_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "#instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "#print(instruct_model_outputs)\n",
    "#instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "#print(dash_line)\n",
    "#print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
