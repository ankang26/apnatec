{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "import spacy\n",
    "import torch\n",
    "from spacy.language import Language\n",
    "from spacy import displacy\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "import statistics\n",
    "import os\n",
    "import json\n",
    "import calendar\n",
    "import holidays\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import hashlib\n",
    "from dateutil.parser import parse\n",
    "import shutil\n",
    "import ast\n",
    "from io import StringIO\n",
    "import requests\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\spacy_transformers\\pipeline_component.py:406: UserWarning: Automatically converting a transformer component from spacy-transformers v1.0 to v1.1+. If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spacy-transformers version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.set_custom_boundaries(doc)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy import displacy\n",
    "import time\n",
    "\n",
    "@Language.component(\"newsent\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        #print(token.text, token.text in (\"’s\", \"'s\"))\n",
    "        if token.text.upper() in (\";\", \"--\", \"\\n\\n\", \"\\n\", \"QUARTERLY\", \"STORY\", \"\\n\\n\\n\\n\", \"\\n\\n\\n\"):\n",
    "            #print(\"Detected:\", token.text)\n",
    "            doc[token.i].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "#spacy.require_gpu()\n",
    "nlp = spacy.load(\"../../Summary/NER/RelateEntity/train/model-best-local\")\n",
    "nlp.add_pipe('sentencizer')\n",
    "nlp.add_pipe('newsent', name=\"newsent\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isScanningRqd(pfile):\n",
    "    with open(pfile, 'r', encoding = \"utf-8\") as fp:\n",
    "        for l_no, line in enumerate(fp):\n",
    "            if \"NOPAD***\" in line:\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentences(inputfile, nlp, text=None):\n",
    "    if(not text):\n",
    "        with open(inputfile, 'r', encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = [str(sent).strip() for sent in doc.sents]\n",
    "\n",
    "    #print(len(sentences))\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rplStr = [\"PG*** \", \"ED*** \", \"SCHQ*** \", \"SCBQ*** \", \"SCBF*** \", \"SCHF*** \", \"SCG*** \", \"GF*** \", \"GQ*** \", \"SC*** \", \"NOPAD*** \"]\n",
    "stag = [\"SCHQ***\", \"SCHF***\", \"SCBQ***\", \"SCBF***\", \"SCG***\"]\n",
    "gtag = [\"GF***\", \"GQ***\"]\n",
    "ptag = \"PG***\"\n",
    "\n",
    "def preProcessSent(line):\n",
    "    for s in rplStr:\n",
    "        line = line.replace(s, \"\")\n",
    "    if(\"TBLST***\" in line or \"TBLET***\" in line or \"CS***\" in line or line == \"\\n\" or line == \"\\n\\n\" or \"https://finance.yahoo.com\" in line):\n",
    "        return(line, False)\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    if(line == None or line == \"\"):\n",
    "        return(line, False)\n",
    "    return(line, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "def query1_from_list(context):\n",
    "    ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "    sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "    \n",
    "    ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "    sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "    \n",
    "    ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "    sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "    \n",
    "    tfewshot = f\"\"\"\n",
    "    Question: What are the relations present in the following text? \n",
    "    \n",
    "    Context: {\" * \".join(sent)}. \n",
    "    \n",
    "    Answer: {ans}.\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Question: What are the relations present in the following text? \n",
    "     \n",
    "    Context: {\" * \".join(sent1)}. \n",
    "    \n",
    "    Answer: {ans1}.\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Question: What are the relations present in the following text?\n",
    "    \n",
    "    Context: {\" * \".join(sent2)}. \n",
    "    \n",
    "    Answer: {ans2}\n",
    "    \n",
    "    \"\"\"\n",
    "    #print(tfewshot)\n",
    "    #print(\"\\n\\n\")\n",
    "    t5query = f\"\"\"{tfewshot}\n",
    "    Question: What are the relations present in the following text? \n",
    "    \n",
    "    Context:  {\" * \".join(context)}.\n",
    "    \n",
    "    Answer:\n",
    "    \n",
    "    \"\"\"\n",
    "    print(t5query)\n",
    "    inputs = tokenizer(t5query, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "context = [\"GAAP Gross profit for the third quarter of 2022 was $210 million\"]\n",
    "result = query1_from_list(context)\n",
    "print(f\"{result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "def query1_from_list(context):\n",
    "    ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "    sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "    \n",
    "    ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "    sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "    \n",
    "    ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "    sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "    \n",
    "    tfewshot = f\"\"\"\n",
    "    Article: {\" * \".join(sent)}\n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format. {ans}\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Article: {\" * \".join(sent1)}. \n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format. {ans1}\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Article: {\" * \".join(sent2)}.\n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format. {ans2}\n",
    "    \n",
    "    \"\"\"\n",
    "    #print(tfewshot)\n",
    "    #print(\"\\n\\n\")\n",
    "    t5query = f\"\"\"{tfewshot}\n",
    "    Article: {\" * \".join(context)}.\n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format.\"\"\"\n",
    "    print(t5query)\n",
    "    inputs = tokenizer(t5query, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "context = [\"GAAP Gross profit for the third quarter of 2022 was $210 million\"]\n",
    "result = query1_from_list(context)\n",
    "print(f\"{result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format 2, includiing entities and relations\n",
    "maxFiles = 60\n",
    "fileCnt = 0\n",
    "source = \"../../Summary/DATA/FLAN/Backup/Format-1\"\n",
    "files = glob.glob(source+\"/*_ER.tsv\")\n",
    "#print(files)\n",
    "for file in files:\n",
    "    #print(file)\n",
    "    basefile = os.path.basename(file)\n",
    "    #print(basefile)\n",
    "    outfile = \"../../Summary/DATA/FLAN/Train/\"+basefile\n",
    "    of = None\n",
    "    #print(outfile)\n",
    "    if outfile:\n",
    "        outfilePath = Path(outfile)\n",
    "        if outfilePath.is_file():\n",
    "            print(\"Output File {} already exists\".format(outfile))\n",
    "            continue\n",
    "        \n",
    "        if(maxFiles > 0):\n",
    "            fileCnt = fileCnt + 1\n",
    "            if(fileCnt > maxFiles):\n",
    "                break\n",
    "            \n",
    "        print(\"Creating Output File {}\".format(outfile))\n",
    "        of = open(outfilePath, \"w\", encoding = \"utf-8\")\n",
    "        \n",
    "        \n",
    "    with open(file, \"r\", encoding = \"ISO-8859-1\") as f:\n",
    "        line = f.readline()\n",
    "        #print(line)\n",
    "        while line:\n",
    "            if (\"RELATIONS\" not in line):\n",
    "                of.write(line)\n",
    "                #of.write(\"\\n\")\n",
    "            else:\n",
    "                relations = dict()\n",
    "                nsplit = line.split(\"\\t\")\n",
    "                relation = nsplit[2]\n",
    "                relation = relation.replace(\"\\n\",\"\")\n",
    "                sent = nsplit[1]\n",
    "                sent = sent.replace(\"\\t\",\"\")\n",
    "                #print(sent)\n",
    "                relation = json.loads(relation)\n",
    "                #print(relation)\n",
    "                sentences = getSentences(None, nlp, sent)\n",
    "                nerl = None\n",
    "                for l in sentences:\n",
    "                    text1 = list()\n",
    "                    text1.append(l)\n",
    "                    for doc in nlp.pipe(text1, disable=[\"tagger\"]):\n",
    "                        for ent in doc.ents:\n",
    "                            if(not nerl):\n",
    "                                nerl = ent.label_.replace(\":\",\"\").replace(\",\",\"\")+\"=\"+ent.text.replace(\":\",\"\").replace(\",\",\"\")\n",
    "                            else:\n",
    "                                nerl = nerl + \",\" + ent.label_.replace(\":\",\"\").replace(\",\",\"\")+\"=\"+ent.text.replace(\":\",\"\").replace(\",\",\"\")\n",
    "                #relations[\"ENTITIES\"] = None\n",
    "                relations[\"ENTITIES\"] = list()\n",
    "                relations[\"ENTITIES\"].append(nerl)\n",
    "                #rlist = list()\n",
    "                #for rstr in relation[\"RELATIONS\"]:\n",
    "                #    rstr = rstr.replace(\",MONEY\",\"!!MONEY\").replace(\",TYPE\",\"!!TYPE\").replace(\",CD\",\"!!CD\").replace(\",RELATION\",\"!!RELATION\").replace(\",PCT\",\"!!PCT\").replace(\",DATE\",\"!!DATE\").replace(\",CALENDAR\",\"!!CALENDAR\")\n",
    "                #    rlist.append(rstr)\n",
    "                relations[\"RELATIONS\"] = relation[\"RELATIONS\"]\n",
    "                #print(relations)\n",
    "                nsplit[2] = json.dumps(relations)\n",
    "                nsplit[1] = sent\n",
    "                nline = \"\\t\".join(nsplit)\n",
    "                #nline = nline + \"</s>\" #Not Required\n",
    "                of.write(nline)\n",
    "                of.write(\"\\n\")\n",
    "            line = f.readline()\n",
    "    if(of):\n",
    "        of.close()\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format 3, combining entities, relations and earning section in single training data\n",
    "maxFiles = -1\n",
    "fileCnt = 0\n",
    "source = \"../../Summary/DATA/FLAN/Backup/Format-2\"\n",
    "\n",
    "files = glob.glob(source+\"/*_ER.tsv\")\n",
    "#print(files)\n",
    "for file in files:\n",
    "    #print(file)\n",
    "    basefile = os.path.basename(file)\n",
    "    #print(basefile)\n",
    "    outfile = \"../../Summary/DATA/FLAN/Train/\"+basefile\n",
    "    of = None\n",
    "    #print(outfile)\n",
    "    if outfile:\n",
    "        outfilePath = Path(outfile)\n",
    "        if outfilePath.is_file():\n",
    "            print(\"Output File {} already exists\".format(outfile))\n",
    "            continue\n",
    "        \n",
    "        if(maxFiles > 0):\n",
    "            fileCnt = fileCnt + 1\n",
    "            if(fileCnt > maxFiles):\n",
    "                break\n",
    "            \n",
    "        print(\"Creating Output File {}\".format(outfile))\n",
    "        of = open(outfilePath, \"w\", encoding = \"utf-8\")\n",
    "        \n",
    "    \n",
    "    tag = None\n",
    "    section = None\n",
    "    ssection = None\n",
    "    header = None\n",
    "    report = \"REGULAR\"\n",
    "    \n",
    "    with open(file, \"r\", encoding = \"ISO-8859-1\") as f:\n",
    "        line = f.readline()\n",
    "        #print(line)\n",
    "        while line:\n",
    "            if (\"RELATIONS\" not in line):\n",
    "                of.write(line)\n",
    "                #of.write(\"\\n\")\n",
    "            else:\n",
    "                relations = dict()\n",
    "                nsplit = line.split(\"\\t\")\n",
    "                relation = nsplit[2]\n",
    "                relation = relation.replace(\"\\n\",\"\")\n",
    "                sent = nsplit[1]\n",
    "                sent = sent.replace(\"\\t\",\"\")\n",
    "                tag = sent.split(\" \")[0].strip()\n",
    "                #print(tag)\n",
    "                #print(sent)\n",
    "                if(tag in stag):\n",
    "                    section = sent.replace(tag, \"\").strip()\n",
    "                    header = section\n",
    "                    if(\"Q\" in tag):\n",
    "                        report = \"REGULAR\"\n",
    "                    elif(\"F\" in tag):\n",
    "                        report = \"REGULARFULL\"\n",
    "                    elif(\"G\" in tag):\n",
    "                        report = \"GUIDE\"\n",
    "                    #print(sent)\n",
    "                elif(tag in gtag):\n",
    "                    ssection = sent.replace(tag, \"\").strip()\n",
    "                    header = ssection\n",
    "                    if(\"Q\" in tag):\n",
    "                        report = \"GUIDE\"\n",
    "                    elif(\"F\" in tag):\n",
    "                        report = \"GUIDEFULL\"\n",
    "                    #if(header):\n",
    "                    #    if(\"***\" in tag):\n",
    "                    #        sent = sent.replace(tag, \"\")\n",
    "                    #        sent = tag + \" \" + header + \" \" + sent\n",
    "                    #    else:\n",
    "                    #        sent = header + \" \" + sent\n",
    "                    #if(section):\n",
    "                    #    header = section + \" \" + ssection\n",
    "                    #else:\n",
    "                    #    header = ssection\n",
    "                    #print(sent)\n",
    "                else:\n",
    "                    if(header):\n",
    "                        if(\"***\" in tag):\n",
    "                            sent = sent.replace(tag, \"\")\n",
    "                            sent = tag + \" \" + header + \" \" + sent\n",
    "                        else:\n",
    "                            sent = header + \" \" + sent\n",
    "                    #print(sent)\n",
    "                relation = json.loads(relation)\n",
    "                #print(relation)\n",
    "                relations[\"ENTITIES\"] = relation[\"ENTITIES\"]\n",
    "                rltn = relation[\"RELATIONS\"]\n",
    "                if(len(rltn) != 0 and rltn[0] != ''):\n",
    "                    rlist = list()\n",
    "                    for rstr in rltn:\n",
    "                        rstr = rstr + \"!!SECTION:\"+ report\n",
    "                        rlist.append(rstr)\n",
    "                    relations[\"RELATIONS\"] = rlist\n",
    "                else:\n",
    "                    relations[\"RELATIONS\"] = rltn \n",
    "                #print(relations)\n",
    "                nsplit[2] = json.dumps(relations)\n",
    "                nsplit[1] = sent\n",
    "                nline = \"\\t\".join(nsplit)\n",
    "                #nline = nline + \"</s>\" #Not Required\n",
    "                of.write(nline)\n",
    "                of.write(\"\\n\")\n",
    "            line = f.readline()\n",
    "    if(of):\n",
    "        of.close()\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "devDataFile = \"../../Summary/DATA/FLAN/Dev/dev.tsv\"\n",
    "trainDataFile = \"../../Summary/DATA/FLAN/Train/train.tsv\"\n",
    "testDataFile = \"../../Summary/DATA/FLAN/Test/test.tsv\"\n",
    "\n",
    "trainDir = \"../../Summary/DATA/FLAN/Train\"\n",
    "devDir = \"../../Summary/DATA/FLAN/Dev\"\n",
    "testDir = \"../../Summary/DATA/FLAN/Test\"\n",
    "\n",
    "def writeTrainingData(writeFile, writeDir):\n",
    "    files = glob.glob(writeDir+\"/*_ER.tsv\")\n",
    "    print(files)\n",
    "    frames = list()\n",
    "\n",
    "    if(len(files) > 0):\n",
    "        for file in files:\n",
    "            print(file)\n",
    "            #df = pd.read_csv(file, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "            df = pd.read_csv(file, sep=\"\\t\", encoding = \"ISO-8859-1\").astype(str)\n",
    "            df = df.dropna()\n",
    "            df = df[df['Sentence1'].notna()]\n",
    "            #print(df)\n",
    "            frames.append(df)\n",
    "    result = pd.concat(frames)\n",
    "    print(result)\n",
    "    result.to_csv(writeFile, sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeTrainingData(trainDataFile, trainDir)\n",
    "writeTrainingData(devDataFile, devDir)\n",
    "writeTrainingData(testDataFile, testDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(trainDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "eval_df = pd.read_csv(devDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "test_df = pd.read_csv(testDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "\n",
    "train_df = train_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "eval_df = eval_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "test_df = test_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "\n",
    "train_df = train_df[[\"input_text\", \"target_text\"]]\n",
    "eval_df = eval_df[[\"input_text\", \"target_text\"]]\n",
    "test_df = test_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "#train_df[\"prefix\"] = \"paraphrase\"\n",
    "#train_df = train_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "#train_df = train_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "#eval_df[\"prefix\"] = \"paraphrase\"\n",
    "#eval_df = eval_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "#eval_df = eval_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "train_df = train_df[train_df['input_text'].notna()]\n",
    "\n",
    "eval_df = eval_df.dropna()\n",
    "eval_df = eval_df[eval_df['input_text'].notna()]\n",
    "\n",
    "test_df = test_df.dropna()\n",
    "test_df = test_df[test_df['input_text'].notna()]\n",
    "\n",
    "#train_df[\"input_text\"] = train_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "#train_df[\"target_text\"] = train_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "print(\"TRAIN DATA ..............\")\n",
    "print(train_df)\n",
    "\n",
    "#eval_df[\"input_text\"] = eval_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "#eval_df[\"target_text\"] = eval_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "print(\"EVAL DATA ..............\")\n",
    "print(eval_df)\n",
    "\n",
    "print(\"TEST DATA ..............\")\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./feroutputs/checkpoint-5203\n"
     ]
    }
   ],
   "source": [
    "#modelPath = \"./feroutputs/checkpoint-5203/pytorch_model.bin\"\n",
    "modelPath = \"./feroutputs/pytorch_model.bin\"\n",
    "modelDir = \"./feroutputs/\"\n",
    "model_def = 'google/flan-t5-small'\n",
    "if os.path.isfile(modelPath):\n",
    "    model_name = \"./feroutputs/checkpoint-5203\"\n",
    "else:\n",
    "    di = sorted(os.listdir(modelDir), reverse=True)\n",
    "    if(len(di) > 0):\n",
    "        model_name = modelDir+(di[0])\n",
    "    else:\n",
    "        model_name = model_def\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model_name = \"facebook/bart-base\"\n",
    "\n",
    "#original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#original_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 76961152\n",
      "all model parameters: 76961152\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = (train_df[\"input_text\"][0])\n",
    "outputText = (train_df[\"target_text\"][0])\n",
    "\n",
    "ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "\n",
    "ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "\n",
    "ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "\n",
    "tfewshot = f\"\"\"\n",
    "Article: {\" * \".join(sent)}\n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format. {ans}\n",
    "\n",
    "\"\"\"\n",
    "tfewshot += f\"\"\"\n",
    "Article: {\" * \".join(sent1)}. \n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format. {ans1}\n",
    "\n",
    "\"\"\"\n",
    "tfewshot += f\"\"\"\n",
    "Article: {\" * \".join(sent2)}.\n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format. {ans2}\n",
    "\n",
    "\"\"\"\n",
    "#print(tfewshot)\n",
    "#print(\"\\n\\n\")\n",
    "t5query = f\"\"\"{tfewshot}\n",
    "Article: {inputText}.\n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format.\"\"\"\n",
    "#print(t5query)\n",
    "inputs = tokenizer(t5query, return_tensors=\"pt\")\n",
    "outputs = original_model.generate(**inputs, max_new_tokens=100)\n",
    "output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{t5query}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{outputText}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train = Dataset.from_pandas(train_df)\n",
    "valid = Dataset.from_pandas(eval_df)\n",
    "test = Dataset.from_pandas(test_df)\n",
    "\n",
    "ds = DatasetDict()\n",
    "\n",
    "ds['train'] = train\n",
    "ds['validation'] = valid\n",
    "ds['test'] = test\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([ds[\"train\"], ds[\"validation\"]]).map(lambda x: tokenizer(x[\"input_text\"], truncation=True), batched=True, remove_columns=[\"input_text\", \"target_text\", \"__index_level_0__\"])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([ds[\"train\"], ds[\"validation\"]]).map(lambda x: tokenizer(x[\"target_text\"], truncation=True), batched=True, remove_columns=[\"input_text\", \"target_text\", \"__index_level_0__\"])\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    #print(example)\n",
    "    start_prompt = 'Article: '\n",
    "    end_prompt = '\\n\\nQuestion: What are the relations present in the text? Display it in json format.'\n",
    "    prompt = [start_prompt + sentence + end_prompt for sentence in example[\"input_text\"]]\n",
    "    #print(prompt)\n",
    "    example['input_ids'] = tokenizer(prompt, max_length=max_source_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"target_text\"], max_length=max_target_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    #print(example)\n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['input_text', 'target_text', '__index_level_0__',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtokenize_function(example, padding=\"max_length\"):\n",
    "    #print(example)\n",
    "    start_prompt = 'Article: '\n",
    "    #end_prompt = '\\n\\nQuestion: What are the relations present in the text? Display it in json format.'\n",
    "    end_prompt = '\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.'\n",
    "    prompt = [start_prompt + sentence + end_prompt for sentence in example[\"input_text\"]]\n",
    "    #print(prompt)\n",
    "    model_inputs = tokenizer(prompt, max_length=max_source_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(text=example[\"target_text\"], max_length=max_target_length, padding=\"max_length\", truncation=True)\n",
    "    #print(example)\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = ds.map(newtokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['input_text', 'target_text', '__index_level_0__',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
    "#tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=original_model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "#data_collator = DataCollatorForSeq2Seq(tokenizer, model=original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./feroutputs\"\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    overwrite_output_dir=True,\n",
    "    fp16=False,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./feroutputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./feroutputs/checkpoint-5203\")#, torch_dtype=torch.bfloat16)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"./feroutputs/checkpoint-2028\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 15\n",
    "dialogue = ds['test'][index]['input_text']\n",
    "human_baseline_summary = ds['test'][index]['target_text']\n",
    "#dialogue = \"Net Loss: Net loss was $118.9 million , or $1.74 per share with 65 million shares outstanding\"\n",
    "print(dialogue)\n",
    "index = 3\n",
    "dialogue2 = ds['test'][index]['input_text']\n",
    "print(dialogue2)\n",
    "#prompt = dialogue\n",
    "\n",
    "ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "\n",
    "ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "\n",
    "ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "\n",
    "tfewshot = f\"\"\"\n",
    "Article: {\" * \".join(sent)}\n",
    "\n",
    "Question: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format. {ans}\n",
    "\n",
    "\"\"\"\n",
    "tfewshot += f\"\"\"\n",
    "Article: {\" * \".join(sent1)}. \n",
    "\n",
    "Question: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format. {ans1}\n",
    "\n",
    "\"\"\"\n",
    "tfewshot += f\"\"\"\n",
    "Article: {\" * \".join(sent2)}.\n",
    "\n",
    "Question: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format. {ans2}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Article: {dialogue}.\n",
    "\n",
    "Question: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.\"\"\"\n",
    "\n",
    "prompt2 = f\"\"\"\n",
    "Article: {dialogue2}.\n",
    "\n",
    "Question: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.\"\"\"\n",
    "\n",
    "\n",
    "#inputs = tokenizer([prompt], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = instruct_model.generate(**inputs, max_new_tokens=300, top_p=.9)\n",
    "#outputs = instruct_model.generate(**inputs)\n",
    "instruct_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "#print(prompt)\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(prompt)\n",
    "print(prompt2)\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"\"\n",
    "pmpt = f\"\"\"\n",
    "Article: [ARTICLE].\n",
    "\n",
    "Question: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.\"\"\"\n",
    "\n",
    "def writetofile(inputfile, to_predict, predictions, of, forTrain=False):\n",
    "    if forTrain:\n",
    "        return\n",
    "    for to_pred, preds in zip(to_predict, predictions):\n",
    "        #print(preds)\n",
    "        pred = preds\n",
    "        pred = pred.replace(\"\\n\",\"\")\n",
    "        pred = \"{\"+pred+\"}\"\n",
    "        topred = to_pred.replace(\"get relations: \",\"\")\n",
    "        #print(pred, str(type(ast.literal_eval(pred))))\n",
    "        outstr = \"\"\n",
    "        try:\n",
    "            if(str(type(ast.literal_eval(pred))) == \"<class 'dict'>\"):\n",
    "                outstr = (topred + \"\\t\" + pred)\n",
    "            else:\n",
    "                outstr = (topred + \"\\t\" + \"NO PREDICTION\")\n",
    "        except:\n",
    "            outstr = (topred + \"\\t\" + \"NO PREDICTION\")\n",
    "\n",
    "        #print(outstr)\n",
    "        #print(\"\\n\")\n",
    "        outstr = inputfile + \"\\t\" + outstr\n",
    "        of.write(outstr)\n",
    "        of.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxCount = 30\n",
    "\n",
    "def createDatawithModel(origFile, csym, nlp, erModel, forTrain=True):\n",
    "    basefile = os.path.basename(origFile)\n",
    "    inputfile = os.path.splitext(basefile)[0]\n",
    "    if(not forTrain):\n",
    "        #print(\"NOT SUPPORTED WITHOUT TRAINING FLAG AS OF NOW\")\n",
    "        #return\n",
    "        if not isScanningRqd(origFile):\n",
    "            print(\"Format not supported for file {}\".format(origFile))\n",
    "            return False\n",
    "        \n",
    "        outdir = \"../../Summary/PostRefinedV3\"\n",
    "        outfileDir = outdir+\"/\"+csym\n",
    "        if not os.path.exists(outfileDir):\n",
    "            os.makedirs(outfileDir)\n",
    "        outfilePath = outfileDir+\"/\"+inputfile+\".txt\"\n",
    "        print(outfilePath)\n",
    "        outfile = Path(outfilePath)\n",
    "        if outfile.is_file():\n",
    "            print(outfilePath + \" Already exists\")\n",
    "            return False\n",
    "        \n",
    "        print(\"Creating post refined V3 data file \" + str(outfile))\n",
    "        of = None\n",
    "        #of = open(outfile, \"w\", encoding = \"utf-8\")\n",
    "        #of.write(\"filename\\tSentence1\\tSentence2\\n\")\n",
    "        \n",
    "        header = None\n",
    "        \n",
    "        with open(origFile, \"r\", encoding = \"utf-8\") as f:\n",
    "            line = f.readline()\n",
    "            line = line.strip()\n",
    "            #line = line.replace(\"\\n\", \"\")\n",
    "            #print(line)\n",
    "            to_predict = list()\n",
    "            while line:\n",
    "                #print(line)\n",
    "                if(\"ED***\" in line):\n",
    "                    break\n",
    "                elif(\"CS***\" in line or \"TBLST***\" in line or \"TBLET***\" in line):\n",
    "                    if(len(to_predict) > 0):\n",
    "                        print(to_predict)\n",
    "                        print(\"\\n\\n\")\n",
    "                        #predictions = erModel.predict(to_predict)\n",
    "                        #print(predictions)\n",
    "                        #print(\"\\n\\n\")\n",
    "                        #writetofile(inputfile, to_predict, predictions, of, forTrain=False)\n",
    "                    to_predict = list()\n",
    "                    line = line.replace(\"\\n\",\"\")\n",
    "                    #print(line + \" #### \" + \"NOT PROCCESSED\")\n",
    "                    #print(\"\\n\")\n",
    "                    #sentences = getSentences(None, nlp, line)\n",
    "                    #for l in sentences:\n",
    "                    #    if(\"CS***\" in line):\n",
    "                    #        if(\"CS***\" in l):\n",
    "                    #            of.write(inputfile + \"\\t\" + l + \"\\t\" + \"{\\\"ENTITIES\\\": [null], \\\"RELATIONS\\\": [\\\"\\\"]}\")\n",
    "                    #        else:\n",
    "                    #            of.write(inputfile + \"\\t\" + \"CS*** \"+l + \"\\t\" + \"{\\\"ENTITIES\\\": [null], \\\"RELATIONS\\\": [\\\"\\\"]}\")\n",
    "                    #    else:\n",
    "                    #        of.write(inputfile + \"\\t\" + l + \"\\t\" + \"{\\\"ENTITIES\\\": [null], \\\"RELATIONS\\\": [\\\"\\\"]}\")\n",
    "                    #    of.write(\"\\n\\n\")\n",
    "                    line = f.readline()\n",
    "                    continue\n",
    "                #elif(\"PG***\" in line or \"NOPAD***\" in line or (\"***\" not in line)):\n",
    "                else:\n",
    "                    tag = line.split(\" \")[0].strip()\n",
    "                    #print(tag)\n",
    "                    #print(sent)\n",
    "                    if(tag in stag):\n",
    "                        section = line.replace(tag, \"\").strip()\n",
    "                        header = section\n",
    "                        #print(header)\n",
    "                        nl = pmpt.replace(\"[ARTICLE]\", line)\n",
    "                        to_predict = to_predict + [\"get relations: \"+nl]\n",
    "                    elif(tag in gtag):\n",
    "                        ssection = line.replace(tag, \"\").strip()\n",
    "                        header = ssection\n",
    "                        #print(header)\n",
    "                        nl = pmpt.replace(\"[ARTICLE]\", line)\n",
    "                        to_predict = to_predict + [\"get relations: \"+nl]\n",
    "                    else:\n",
    "                        sentences = getSentences(None, nlp, line)\n",
    "                        #to_predict = list()\n",
    "                        for l in sentences:\n",
    "                            #print(l)\n",
    "                            pline, isProcess = preProcessSent(l)\n",
    "                            if not isProcess:\n",
    "                                continue\n",
    "                            if(header):\n",
    "                                if(\"***\" in tag):\n",
    "                                    nl = l.replace(tag, \"\")\n",
    "                                    nl = tag + \" \" + header + \" -%%%- \" + nl\n",
    "                                else:\n",
    "                                    nl = header + \" -%%%- \" + l\n",
    "                            else:\n",
    "                                nl = l\n",
    "                            nl = pmpt.replace(\"[ARTICLE]\", nl)\n",
    "                            to_predict = to_predict + [nl]\n",
    "                            if(len(to_predict) >= maxCount):\n",
    "                                print(to_predict)\n",
    "                                print(\"\\n\\n\")\n",
    "                                #predictions = erModel.predict(to_predict)\n",
    "                                #print(predictions)\n",
    "                                #print(\"\\n\\n\")\n",
    "                                #writetofile(inputfile, to_predict, predictions, of, forTrain=False)\n",
    "                                to_predict = list()\n",
    "                line = f.readline()\n",
    "            \n",
    "            if(len(to_predict) > 0):\n",
    "                print(to_predict)\n",
    "                print(\"\\n\\n\")\n",
    "                #predictions = erModel.predict(to_predict)\n",
    "                #print(predictions)\n",
    "                #print(\"\\n\\n\")\n",
    "                #writetofile(inputfile, to_predict, predictions, of, forTrain=False)\n",
    "                to_predict = list()\n",
    "        if(of):\n",
    "            of.close()\n",
    "            \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Summary/Refined//APPN\\APPN_2022-08-04_EP_YH.txt\n",
      "../../Summary/PostRefinedV3/APPN/APPN_2022-08-04_EP_YH.txt\n",
      "Creating post refined V3 data file ..\\..\\Summary\\PostRefinedV3\\APPN\\APPN_2022-08-04_EP_YH.txt\n",
      "['\\nArticle: GAAP Net Loss Per Share is $(0.68) in second quarter 2022 @@@.\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: Cash And Cash Equivalents is $76185 T in second quarter 2022 @@@.\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: GAAP Gross Profit is $76770 T in second quarter 2022 @@@.\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: GAAP Gross Margin is 69.75% in second quarter 2022 @@@.\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG***.\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Appian Corporation..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Second quarter cloud subscription revenue increased 34% year-over-year to $57.1 million..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** MCLEAN, Va., Aug. 04, 2022 (GLOBE NEWSWIRE).\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: -- Appian (Nasdaq: APPN) today announced financial results for the second quarter ended June 30, 2022..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.']\n",
      "\n",
      "\n",
      "\n",
      "['get relations: \\nArticle: SCHQ*** Second Quarter 2022 Financial Highlights:.\\n.\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Second Quarter 2022 Financial Highlights:. -%%%-  Revenue: Cloud subscription revenue was $57.1 million for the second quarter of 2022, up 34% compared to the second quarter of 2021..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Second Quarter 2022 Financial Highlights:. -%%%- Total subscriptions revenue increased 35% year-over-year to $76.7 million..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Second Quarter 2022 Financial Highlights:. -%%%- Professional services revenue was $33.4 million, an increase of 28% compared to the second quarter of 2021..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Second Quarter 2022 Financial Highlights:. -%%%- Total revenue was $110.1 million, up 33% compared to the second quarter of 2021..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Second Quarter 2022 Financial Highlights:. -%%%- Cloud subscription revenue retention rate was 116% as of June 30, 2022..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Second Quarter 2022 Financial Highlights:. -%%%-  Operating loss and non-GAAP operating loss: GAAP operating loss was $(42.7) million, compared to $(24.6) million for the second quarter of 2021..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Second Quarter 2022 Financial Highlights:. -%%%- Non-GAAP operating loss was $(26.8) million, compared to $(17.6) million for the second quarter of 2021..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Second Quarter 2022 Financial Highlights:. -%%%-  Net loss and non-GAAP net loss: GAAP net loss was $(49.4) million, compared to $(23.8) million for the second quarter of 2021..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Second Quarter 2022 Financial Highlights:. -%%%- GAAP net loss per share was $(0.68) for the second quarter of 2022, compared to $(0.34) for the second quarter of 2021..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Second Quarter 2022 Financial Highlights:. -%%%- Non-GAAP net loss was $(33.4) million, compared to $(16.9) million for the second quarter of 2021..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Second Quarter 2022 Financial Highlights:. -%%%- Non-GAAP net loss per share was $(0.46) for the second quarter of 2022, compared to the $(0.24) net loss per share for the second quarter of 2021..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Second Quarter 2022 Financial Highlights:. -%%%- GAAP and non-GAAP net loss for the second quarter of 2022 included $6.5 million, or $(0.09) per share, of foreign currency exchange losses..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Second Quarter 2022 Financial Highlights:. -%%%- We do not forecast foreign exchange rate movements..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Second Quarter 2022 Financial Highlights:. -%%%-  Adjusted EBITDA: Adjusted EBITDA loss was $(25.0) million, compared to adjusted EBITDA loss of $(16.3) million for the second quarter of 2021..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Second Quarter 2022 Financial Highlights:. -%%%-  Balance sheet and cash flows: As of June 30, 2022, Appian had total cash and investments of $138.0 million..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Second Quarter 2022 Financial Highlights:. -%%%- Net cash used in operating activities was $(29.7) million for the three months ended June 30, 2022 compared to $(6.6) million of net cash used in operating activities for the same period in 2021..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Second Quarter 2022 Financial Highlights:. -%%%- .\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Second Quarter 2022 Financial Highlights:. -%%%-  A reconciliation of GAAP to non-GAAP financial measures has been provided in the tables following the financial statements in this press release..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Second Quarter 2022 Financial Highlights:. -%%%- An explanation of these measures is also included below under the heading “Non-GAAP Financial Measures.”..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', 'get relations: \\nArticle: SCBQ*** Recent Business Highlights:.\\n.\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', \"\\nArticle: PG*** Recent Business Highlights:. -%%%-  Appian named a “Customers' Choice” again in 2022 Gartner® Peer Insights™ Voice of the Customer: Enterprise Low-Code Application Platforms..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.\", '\\nArticle: PG*** Recent Business Highlights:. -%%%-  Italian Postal Service improves operational efficiency by 70% with Appian..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Recent Business Highlights:. -%%%-  Pepper Money’s SOLANA improves business volumes by 70% with the Appian Low-Code Platform..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Recent Business Highlights:. -%%%-  Appian announces 2022 “Excellence in Low-Code” award winners..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Recent Business Highlights:. -%%%-  Appian announces #lowcode4all..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Recent Business Highlights:. -%%%-  Appian names Bill McCarthy and Mark Lynch to Board of Directors..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Recent Business Highlights:. -%%%-  Appian names new Chief People Officer and new Senior Vice President of Industry Products and Solutions..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Recent Business Highlights:. -%%%-  Appian awarded $2.036 billion in damages against Pegasystems, Inc..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', 'get relations: \\nArticle: SCG*** Financial Outlook:.\\n.\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Financial Outlook:. -%%%-  As of August 4, 2022, guidance for 2022 is as follows:..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.']\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get relations: \\nArticle: GQ*** Third Quarter 2022 Guidance:.\\n.\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Third Quarter 2022 Guidance:. -%%%-  Cloud subscription revenue is expected to be between $60.8 million and $61.3 million, representing year-over-year growth of 30% to 31%..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Third Quarter 2022 Guidance:. -%%%-  Total revenue is expected to be between $115.0 million and $117.0 million, representing a year-over-year increase of 24% to 27%..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Third Quarter 2022 Guidance:. -%%%-  Adjusted EBITDA loss is expected to be between $(15.0) million and $(13.0) million..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Third Quarter 2022 Guidance:. -%%%-  Non-GAAP net loss per share is expected to be between $(0.23) and $(0.20), assuming weighted average common shares outstanding of 72.5 million..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', 'get relations: \\nArticle: GF*** Full Year 2022 Guidance:.\\n.\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Full Year 2022 Guidance:. -%%%-  Cloud subscription revenue is expected to be between $236.0 million and $238.0 million, representing year-over-year growth of 32% to 33%..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Full Year 2022 Guidance:. -%%%-  Total revenue is expected to be between $466.0 million and $470.0 million, representing a year-over-year increase of 26% to 27%..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Full Year 2022 Guidance:. -%%%-  Adjusted EBITDA loss is expected to be between $(53.0) million and $(50.0) million..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.', '\\nArticle: PG*** Full Year 2022 Guidance:. -%%%-  Non-GAAP net loss per share is expected to be between $(0.91) and $(0.86), assuming weighted average common shares outstanding of 72.5 million..\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entPath = \"../../Summary/entities/\"\n",
    "rPath = \"../../Summary/Refined/\"\n",
    "files = glob.glob(entPath+\"/*-ENTITIES.json\")   \n",
    "if(len(files) > 0):\n",
    "    for file in (files):\n",
    "        basefile = os.path.basename(file)\n",
    "        filename = os.path.splitext(basefile)[0]\n",
    "        csym = filename.split(\"-\")[0]\n",
    "        cPath = rPath + \"/\" + csym\n",
    "        #print(cPath)\n",
    "        cfiles = glob.glob(cPath+\"/*.txt\")\n",
    "        #print(cfiles)\n",
    "        if(len(cfiles) > 0):\n",
    "            for cf in cfiles:\n",
    "                if isScanningRqd(cf):\n",
    "                    print(cf)\n",
    "                    created = createDatawithModel(cf, csym, nlp, instruct_model, forTrain=False)\n",
    "                    if not created:\n",
    "                        continue\n",
    "                    break\n",
    "        if created:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
