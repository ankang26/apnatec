{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "import spacy\n",
    "import torch\n",
    "from spacy.language import Language\n",
    "from spacy import displacy\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "import statistics\n",
    "import os\n",
    "import json\n",
    "import calendar\n",
    "import holidays\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import hashlib\n",
    "from dateutil.parser import parse\n",
    "import shutil\n",
    "import ast\n",
    "from io import StringIO\n",
    "import requests\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy import displacy\n",
    "import time\n",
    "\n",
    "@Language.component(\"newsent\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        #print(token.text, token.text in (\"â€™s\", \"'s\"))\n",
    "        if token.text.upper() in (\";\", \"--\", \"\\n\\n\", \"\\n\", \"QUARTERLY\", \"STORY\", \"\\n\\n\\n\\n\", \"\\n\\n\\n\"):\n",
    "            #print(\"Detected:\", token.text)\n",
    "            doc[token.i].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "#spacy.require_gpu()\n",
    "nlp = spacy.load(\"../../Summary/NER/RelateEntity/train/model-best-local\")\n",
    "nlp.add_pipe('sentencizer')\n",
    "nlp.add_pipe('newsent', name=\"newsent\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "def query1_from_list(context):\n",
    "    ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "    sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "    \n",
    "    ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "    sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "    \n",
    "    ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "    sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "    \n",
    "    tfewshot = f\"\"\"\n",
    "    Question: What are the relations present in the following text? \n",
    "    \n",
    "    Context: {\" * \".join(sent)}. \n",
    "    \n",
    "    Answer: {ans}.\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Question: What are the relations present in the following text? \n",
    "     \n",
    "    Context: {\" * \".join(sent1)}. \n",
    "    \n",
    "    Answer: {ans1}.\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Question: What are the relations present in the following text?\n",
    "    \n",
    "    Context: {\" * \".join(sent2)}. \n",
    "    \n",
    "    Answer: {ans2}\n",
    "    \n",
    "    \"\"\"\n",
    "    #print(tfewshot)\n",
    "    #print(\"\\n\\n\")\n",
    "    t5query = f\"\"\"{tfewshot}\n",
    "    Question: What are the relations present in the following text? \n",
    "    \n",
    "    Context:  {\" * \".join(context)}.\n",
    "    \n",
    "    Answer:\n",
    "    \n",
    "    \"\"\"\n",
    "    print(t5query)\n",
    "    inputs = tokenizer(t5query, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "context = [\"GAAP Gross profit for the third quarter of 2022 was $210 million\"]\n",
    "result = query1_from_list(context)\n",
    "print(f\"{result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Article: GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\n",
      "    \n",
      "    Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\"]}\n",
      "    \n",
      "    \n",
      "    Article: Non-GAAP net income is $110.1 million in third quarter 2022 @@@. \n",
      "    \n",
      "    Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\"]}\n",
      "    \n",
      "    \n",
      "    Article: Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@.\n",
      "    \n",
      "    Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\"]}\n",
      "    \n",
      "    \n",
      "    Article: GAAP Gross profit for the third quarter of 2022 was $210 million.\n",
      "    \n",
      "    Question: What are the relations present in the text? Display it in json format.\n",
      "\"RELATIONS\": [\"KEY:GAAP Gross Profit\"]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "def query1_from_list(context):\n",
    "    ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "    sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "    \n",
    "    ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "    sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "    \n",
    "    ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "    sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "    \n",
    "    tfewshot = f\"\"\"\n",
    "    Article: {\" * \".join(sent)}\n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format. {ans}\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Article: {\" * \".join(sent1)}. \n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format. {ans1}\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Article: {\" * \".join(sent2)}.\n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format. {ans2}\n",
    "    \n",
    "    \"\"\"\n",
    "    #print(tfewshot)\n",
    "    #print(\"\\n\\n\")\n",
    "    t5query = f\"\"\"{tfewshot}\n",
    "    Article: {\" * \".join(context)}.\n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format.\"\"\"\n",
    "    print(t5query)\n",
    "    inputs = tokenizer(t5query, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "context = [\"GAAP Gross profit for the third quarter of 2022 was $210 million\"]\n",
    "result = query1_from_list(context)\n",
    "print(f\"{result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "devDataFile = \"../../Summary/DATA/FLAN/Dev/dev.tsv\"\n",
    "trainDataFile = \"../../Summary/DATA/FLAN/Train/train.tsv\"\n",
    "testDataFile = \"../../Summary/DATA/FLAN/Test/test.tsv\"\n",
    "\n",
    "trainDir = \"../../Summary/DATA/FLAN/Train\"\n",
    "devDir = \"../../Summary/DATA/FLAN/Dev\"\n",
    "testDir = \"../../Summary/DATA/FLAN/Test\"\n",
    "\n",
    "def writeTrainingData(writeFile, writeDir):\n",
    "    files = glob.glob(writeDir+\"/*_ER.tsv\")\n",
    "    print(files)\n",
    "    frames = list()\n",
    "\n",
    "    if(len(files) > 0):\n",
    "        for file in files:\n",
    "            print(file)\n",
    "            #df = pd.read_csv(file, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "            df = pd.read_csv(file, sep=\"\\t\", encoding = \"ISO-8859-1\").astype(str)\n",
    "            df = df.dropna()\n",
    "            df = df[df['Sentence1'].notna()]\n",
    "            #print(df)\n",
    "            frames.append(df)\n",
    "    result = pd.concat(frames)\n",
    "    print(result)\n",
    "    result.to_csv(writeFile, sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../Summary/DATA/FLAN/Train\\\\APPN_2023-02-16_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\APPN_2023-05-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\APPN_2023-08-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\BILL_2022-08-18_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\BILL_2023-02-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\BILL_2023-05-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\BILL_2023-08-17_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CFLT_2022-08-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CFLT_2022-11-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CFLT_2023-05-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CFLT_2023-08-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CRWD_2022-08-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CRWD_2022-11-29_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CRWD_2023-03-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CRWD_2023-08-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DDOG_2022-08-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DDOG_2022-11-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DDOG_2023-02-16_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DDOG_2023-05-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCN_2023-02-16_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCN_2023-05-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCN_2023-08-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCU_2022-09-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCU_2023-03-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCU_2023-06-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\FIVN_2022-07-28_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\FIVN_2022-11-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\FIVN_2023-05-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\FIVN_2023-08-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\HUBS_2022-08-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\HUBS_2022-11-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\HUBS_2023-02-16_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\HUBS_2023-08-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\MDB_2022-08-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\MDB_2022-12-06_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\MDB_2023-03-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\MDB_2023-06-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\NET_2022-11-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\NET_2023-02-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\NET_2023-04-27_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\NET_2023-08-03_EP_YH_ER.tsv']\n",
      "../../Summary/DATA/FLAN/Train\\APPN_2023-02-16_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\APPN_2023-05-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\APPN_2023-08-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\BILL_2022-08-18_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\BILL_2023-02-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\BILL_2023-05-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\BILL_2023-08-17_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CFLT_2022-08-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CFLT_2022-11-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CFLT_2023-05-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CFLT_2023-08-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CRWD_2022-08-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CRWD_2022-11-29_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CRWD_2023-03-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CRWD_2023-08-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DDOG_2022-08-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DDOG_2022-11-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DDOG_2023-02-16_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DDOG_2023-05-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCN_2023-02-16_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCN_2023-05-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCN_2023-08-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCU_2022-09-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCU_2023-03-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCU_2023-06-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\FIVN_2022-07-28_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\FIVN_2022-11-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\FIVN_2023-05-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\FIVN_2023-08-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\HUBS_2022-08-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\HUBS_2022-11-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\HUBS_2023-02-16_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\HUBS_2023-08-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\MDB_2022-08-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\MDB_2022-12-06_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\MDB_2023-03-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\MDB_2023-06-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\NET_2022-11-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\NET_2023-02-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\NET_2023-04-27_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\NET_2023-08-03_EP_YH_ER.tsv\n",
      "                 filename                                          Sentence1  \\\n",
      "0   APPN_2023-02-16_EP_YH  GAAP Net Loss Per Share is $(0.47) in fourth q...   \n",
      "1   APPN_2023-02-16_EP_YH  Cash And Cash Equivalents is $148132 T in four...   \n",
      "2   APPN_2023-02-16_EP_YH  GAAP Gross Profit is $90555 T in fourth quarte...   \n",
      "3   APPN_2023-02-16_EP_YH  GAAP Gross Margin is 71.99% in fourth quarter ...   \n",
      "4   APPN_2023-02-16_EP_YH                                              PG***   \n",
      "..                    ...                                                ...   \n",
      "29   NET_2023-08-03_EP_YH  PG*** Total revenue of $1,283.0 to $1,287.0 mi...   \n",
      "30   NET_2023-08-03_EP_YH  PG*** Non-GAAP income from operations is expec...   \n",
      "31   NET_2023-08-03_EP_YH  PG*** Non-GAAP net income per share is $.37, u...   \n",
      "32   NET_2023-08-03_EP_YH  PG*** These statements are forward-looking and...   \n",
      "33   NET_2023-08-03_EP_YH  Refer to the Forward-Looking Statements safe h...   \n",
      "\n",
      "                                            Sentence2  \n",
      "0   {\"RELATIONS\": [\"KEY:GAAP Net Income Per Share!...  \n",
      "1   {\"RELATIONS\": [\"KEY:Cash And Cash Equivalents!...  \n",
      "2   {\"RELATIONS\": [\"KEY:GAAP GROSS PROFIT!!TYPE:OU...  \n",
      "3   {\"RELATIONS\": [\"KEY:GAAP GROSS MARGIN!!TYPE:OU...  \n",
      "4                                 {\"RELATIONS\": [\"\"]}  \n",
      "..                                                ...  \n",
      "29  {\"RELATIONS\": [\"KEY:REVENUE!!TYPE:OUT!!MONEY:$...  \n",
      "30  {\"RELATIONS\": [\"KEY:NON-GAAP INCOME FROM OPERA...  \n",
      "31  {\"RELATIONS\": [\"KEY:Non-GAAP net income per sh...  \n",
      "32                                {\"RELATIONS\": [\"\"]}  \n",
      "33                                {\"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[2028 rows x 3 columns]\n",
      "['../../Summary/DATA/FLAN/Dev\\\\APPN_2022-11-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\BILL_2022-11-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\CFLT_2023-01-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\CRWD_2023-05-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\DDOG_2023-08-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\DOCN_2022-11-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\DOCU_2022-12-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\HUBS_2023-05-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\MDB_2023-08-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\NET_2022-08-04_EP_YH_ER.tsv']\n",
      "../../Summary/DATA/FLAN/Dev\\APPN_2022-11-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\BILL_2022-11-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\CFLT_2023-01-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\CRWD_2023-05-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\DDOG_2023-08-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\DOCN_2022-11-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\DOCU_2022-12-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\HUBS_2023-05-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\MDB_2023-08-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\NET_2022-08-04_EP_YH_ER.tsv\n",
      "                 filename                                          Sentence1  \\\n",
      "0   APPN_2022-11-03_EP_YH  GAAP Net Loss Per Share is $(0.61) in third qu...   \n",
      "1   APPN_2022-11-03_EP_YH  Cash And Cash Equivalents is $51802 T in third...   \n",
      "2   APPN_2022-11-03_EP_YH  GAAP Gross Profit is $84116 T in third quarter...   \n",
      "3   APPN_2022-11-03_EP_YH  GAAP Gross Margin is 71.36% in third quarter 2...   \n",
      "4   APPN_2022-11-03_EP_YH                                              PG***   \n",
      "..                    ...                                                ...   \n",
      "37   NET_2022-08-04_EP_YH  PG*** Non-GAAP net income per share of $0.00 t...   \n",
      "38   NET_2022-08-04_EP_YH   GF*** For the full year fiscal 2022, we expect:.   \n",
      "39   NET_2022-08-04_EP_YH   PG*** Total revenue of $968.0 to $972.0 million.   \n",
      "40   NET_2022-08-04_EP_YH  PG*** Non-GAAP income from operations of $7.0 ...   \n",
      "41   NET_2022-08-04_EP_YH  PG*** Non-GAAP net income per share of $0.03 t...   \n",
      "\n",
      "                                            Sentence2  \n",
      "0   {\"RELATIONS\": [\"KEY:GAAP Net Income Per Share!...  \n",
      "1   {\"RELATIONS\": [\"KEY:Cash And Cash Equivalents!...  \n",
      "2   {\"RELATIONS\": [\"KEY:GAAP GROSS PROFIT!!TYPE:OU...  \n",
      "3   {\"RELATIONS\": [\"KEY:GAAP GROSS MARGIN!!TYPE:OU...  \n",
      "4                                 {\"RELATIONS\": [\"\"]}  \n",
      "..                                                ...  \n",
      "37  {\"RELATIONS\": [\"KEY:Non-GAAP net income per sh...  \n",
      "38                                {\"RELATIONS\": [\"\"]}  \n",
      "39  {\"RELATIONS\": [\"KEY:REVENUE!!TYPE:OUT!!MONEY:$...  \n",
      "40  {\"RELATIONS\": [\"KEY:NON-GAAP INCOME FROM OPERA...  \n",
      "41  {\"RELATIONS\": [\"KEY:Non-GAAP net income per sh...  \n",
      "\n",
      "[495 rows x 3 columns]\n",
      "['../../Summary/DATA/FLAN/Test\\\\FIVN_2023-02-22_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Test\\\\S_2022-12-06_EP_YH_ER.tsv']\n",
      "../../Summary/DATA/FLAN/Test\\FIVN_2023-02-22_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Test\\S_2022-12-06_EP_YH_ER.tsv\n",
      "                 filename                                          Sentence1  \\\n",
      "0   FIVN_2023-02-22_EP_YH  GAAP Net Loss Per Share is $(0.19) in fourth q...   \n",
      "1   FIVN_2023-02-22_EP_YH  Cash And Cash Equivalents is $180520 T in four...   \n",
      "2   FIVN_2023-02-22_EP_YH  GAAP Gross Profit is $112051 T in fourth quart...   \n",
      "3   FIVN_2023-02-22_EP_YH  GAAP Gross Margin is 53.78% in fourth quarter ...   \n",
      "4   FIVN_2023-02-22_EP_YH  GAAP Free Cash Flow is $36.6MN in fourth quart...   \n",
      "..                    ...                                                ...   \n",
      "34     S_2022-12-06_EP_YH  PG*** These statements are forward-looking and...   \n",
      "35     S_2022-12-06_EP_YH  Refer to the below for information on the fact...   \n",
      "36     S_2022-12-06_EP_YH  PG*** Guidance for non-GAAP financial measures...   \n",
      "37     S_2022-12-06_EP_YH  We have not provided the most directly compara...   \n",
      "38     S_2022-12-06_EP_YH  Accordingly, a reconciliation of non-GAAP gros...   \n",
      "\n",
      "                                            Sentence2  \n",
      "0   {\"RELATIONS\": [\"KEY:GAAP net income per share!...  \n",
      "1   {\"RELATIONS\": [\"KEY:CASH AND EQUIVALENTS!!TYPE...  \n",
      "2   {\"RELATIONS\": [\"KEY:GAAP GROSS PROFIT!!TYPE:OU...  \n",
      "3   {\"RELATIONS\": [\"KEY:GAAP GROSS MARGIN!!TYPE:OU...  \n",
      "4   {\"RELATIONS\": [\"KEY:GAAP FREE CASH FLOW!!TYPE:...  \n",
      "..                                                ...  \n",
      "34                                {\"RELATIONS\": [\"\"]}  \n",
      "35                                {\"RELATIONS\": [\"\"]}  \n",
      "36                                {\"RELATIONS\": [\"\"]}  \n",
      "37                                {\"RELATIONS\": [\"\"]}  \n",
      "38                                {\"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[82 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "writeTrainingData(trainDataFile, trainDir)\n",
    "writeTrainingData(devDataFile, devDir)\n",
    "writeTrainingData(testDataFile, testDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATA ..............\n",
      "                                             input_text  \\\n",
      "0     GAAP Net Loss Per Share is $(0.47) in fourth q...   \n",
      "1     Cash And Cash Equivalents is $148132 T in four...   \n",
      "2     GAAP Gross Profit is $90555 T in fourth quarte...   \n",
      "3     GAAP Gross Margin is 71.99% in fourth quarter ...   \n",
      "4                                                 PG***   \n",
      "...                                                 ...   \n",
      "2023  PG*** Total revenue of $1,283.0 to $1,287.0 mi...   \n",
      "2024  PG*** Non-GAAP income from operations is expec...   \n",
      "2025  PG*** Non-GAAP net income per share is $.37, u...   \n",
      "2026  PG*** These statements are forward-looking and...   \n",
      "2027  Refer to the Forward-Looking Statements safe h...   \n",
      "\n",
      "                                            target_text  \n",
      "0     {\"RELATIONS\": [\"KEY:GAAP Net Income Per Share!...  \n",
      "1     {\"RELATIONS\": [\"KEY:Cash And Cash Equivalents!...  \n",
      "2     {\"RELATIONS\": [\"KEY:GAAP GROSS PROFIT!!TYPE:OU...  \n",
      "3     {\"RELATIONS\": [\"KEY:GAAP GROSS MARGIN!!TYPE:OU...  \n",
      "4                                   {\"RELATIONS\": [\"\"]}  \n",
      "...                                                 ...  \n",
      "2023  {\"RELATIONS\": [\"KEY:REVENUE!!TYPE:OUT!!MONEY:$...  \n",
      "2024  {\"RELATIONS\": [\"KEY:NON-GAAP INCOME FROM OPERA...  \n",
      "2025  {\"RELATIONS\": [\"KEY:Non-GAAP net income per sh...  \n",
      "2026                                {\"RELATIONS\": [\"\"]}  \n",
      "2027                                {\"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[2028 rows x 2 columns]\n",
      "EVAL DATA ..............\n",
      "                                            input_text  \\\n",
      "0    GAAP Net Loss Per Share is $(0.61) in third qu...   \n",
      "1    Cash And Cash Equivalents is $51802 T in third...   \n",
      "2    GAAP Gross Profit is $84116 T in third quarter...   \n",
      "3    GAAP Gross Margin is 71.36% in third quarter 2...   \n",
      "4                                                PG***   \n",
      "..                                                 ...   \n",
      "490  PG*** Non-GAAP net income per share of $0.00 t...   \n",
      "491   GF*** For the full year fiscal 2022, we expect:.   \n",
      "492   PG*** Total revenue of $968.0 to $972.0 million.   \n",
      "493  PG*** Non-GAAP income from operations of $7.0 ...   \n",
      "494  PG*** Non-GAAP net income per share of $0.03 t...   \n",
      "\n",
      "                                           target_text  \n",
      "0    {\"RELATIONS\": [\"KEY:GAAP Net Income Per Share!...  \n",
      "1    {\"RELATIONS\": [\"KEY:Cash And Cash Equivalents!...  \n",
      "2    {\"RELATIONS\": [\"KEY:GAAP GROSS PROFIT!!TYPE:OU...  \n",
      "3    {\"RELATIONS\": [\"KEY:GAAP GROSS MARGIN!!TYPE:OU...  \n",
      "4                                  {\"RELATIONS\": [\"\"]}  \n",
      "..                                                 ...  \n",
      "490  {\"RELATIONS\": [\"KEY:Non-GAAP net income per sh...  \n",
      "491                                {\"RELATIONS\": [\"\"]}  \n",
      "492  {\"RELATIONS\": [\"KEY:REVENUE!!TYPE:OUT!!MONEY:$...  \n",
      "493  {\"RELATIONS\": [\"KEY:NON-GAAP INCOME FROM OPERA...  \n",
      "494  {\"RELATIONS\": [\"KEY:Non-GAAP net income per sh...  \n",
      "\n",
      "[495 rows x 2 columns]\n",
      "TEST DATA ..............\n",
      "                                           input_text  \\\n",
      "0   GAAP Net Loss Per Share is $(0.19) in fourth q...   \n",
      "1   Cash And Cash Equivalents is $180520 T in four...   \n",
      "2   GAAP Gross Profit is $112051 T in fourth quart...   \n",
      "3   GAAP Gross Margin is 53.78% in fourth quarter ...   \n",
      "4   GAAP Free Cash Flow is $36.6MN in fourth quart...   \n",
      "..                                                ...   \n",
      "77  PG*** These statements are forward-looking and...   \n",
      "78  Refer to the below for information on the fact...   \n",
      "79  PG*** Guidance for non-GAAP financial measures...   \n",
      "80  We have not provided the most directly compara...   \n",
      "81  Accordingly, a reconciliation of non-GAAP gros...   \n",
      "\n",
      "                                          target_text  \n",
      "0   {\"RELATIONS\": [\"KEY:GAAP net income per share!...  \n",
      "1   {\"RELATIONS\": [\"KEY:CASH AND EQUIVALENTS!!TYPE...  \n",
      "2   {\"RELATIONS\": [\"KEY:GAAP GROSS PROFIT!!TYPE:OU...  \n",
      "3   {\"RELATIONS\": [\"KEY:GAAP GROSS MARGIN!!TYPE:OU...  \n",
      "4   {\"RELATIONS\": [\"KEY:GAAP FREE CASH FLOW!!TYPE:...  \n",
      "..                                                ...  \n",
      "77                                {\"RELATIONS\": [\"\"]}  \n",
      "78                                {\"RELATIONS\": [\"\"]}  \n",
      "79                                {\"RELATIONS\": [\"\"]}  \n",
      "80                                {\"RELATIONS\": [\"\"]}  \n",
      "81                                {\"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[82 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(trainDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "eval_df = pd.read_csv(devDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "test_df = pd.read_csv(testDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "\n",
    "train_df = train_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "eval_df = eval_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "test_df = test_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "\n",
    "train_df = train_df[[\"input_text\", \"target_text\"]]\n",
    "eval_df = eval_df[[\"input_text\", \"target_text\"]]\n",
    "test_df = test_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "#train_df[\"prefix\"] = \"paraphrase\"\n",
    "#train_df = train_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "#train_df = train_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "#eval_df[\"prefix\"] = \"paraphrase\"\n",
    "#eval_df = eval_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "#eval_df = eval_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "train_df = train_df[train_df['input_text'].notna()]\n",
    "\n",
    "eval_df = eval_df.dropna()\n",
    "eval_df = eval_df[eval_df['input_text'].notna()]\n",
    "\n",
    "test_df = test_df.dropna()\n",
    "test_df = test_df[test_df['input_text'].notna()]\n",
    "\n",
    "#train_df[\"input_text\"] = train_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "#train_df[\"target_text\"] = train_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "print(\"TRAIN DATA ..............\")\n",
    "print(train_df)\n",
    "\n",
    "#eval_df[\"input_text\"] = eval_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "#eval_df[\"target_text\"] = eval_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "print(\"EVAL DATA ..............\")\n",
    "print(eval_df)\n",
    "\n",
    "print(\"TEST DATA ..............\")\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-small'\n",
    "\n",
    "#original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model_name = \"facebook/bart-base\"\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "#original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#original_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 76961152\n",
      "all model parameters: 76961152\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = (train_df[\"input_text\"][0])\n",
    "outputText = (train_df[\"target_text\"][0])\n",
    "\n",
    "ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "\n",
    "ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "\n",
    "ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "\n",
    "tfewshot = f\"\"\"\n",
    "Article: {\" * \".join(sent)}\n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format. {ans}\n",
    "\n",
    "\"\"\"\n",
    "tfewshot += f\"\"\"\n",
    "Article: {\" * \".join(sent1)}. \n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format. {ans1}\n",
    "\n",
    "\"\"\"\n",
    "tfewshot += f\"\"\"\n",
    "Article: {\" * \".join(sent2)}.\n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format. {ans2}\n",
    "\n",
    "\"\"\"\n",
    "#print(tfewshot)\n",
    "#print(\"\\n\\n\")\n",
    "t5query = f\"\"\"{tfewshot}\n",
    "Article: {inputText}.\n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format.\"\"\"\n",
    "#print(t5query)\n",
    "inputs = tokenizer(t5query, return_tensors=\"pt\")\n",
    "outputs = original_model.generate(**inputs, max_new_tokens=100)\n",
    "output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{t5query}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{outputText}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'target_text', '__index_level_0__'],\n",
      "        num_rows: 2028\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_text', 'target_text', '__index_level_0__'],\n",
      "        num_rows: 495\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_text', 'target_text', '__index_level_0__'],\n",
      "        num_rows: 82\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train = Dataset.from_pandas(train_df)\n",
    "valid = Dataset.from_pandas(eval_df)\n",
    "test = Dataset.from_pandas(test_df)\n",
    "\n",
    "ds = DatasetDict()\n",
    "\n",
    "ds['train'] = train\n",
    "ds['validation'] = valid\n",
    "ds['test'] = test\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2028 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/495 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/82 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    #print(example)\n",
    "    start_prompt = 'Article: '\n",
    "    end_prompt = '\\n\\nQuestion: What are the relations present in the text? Display it in json format.'\n",
    "    prompt = [start_prompt + sentence + end_prompt for sentence in example[\"input_text\"]]\n",
    "    #print(prompt)\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"target_text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    #print(example)\n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['input_text', 'target_text', '__index_level_0__',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
    "#tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (2028, 2)\n",
      "Validation: (495, 2)\n",
      "Test: (82, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 2028\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 495\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 82\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "#data_collator = DataCollatorForSeq2Seq(tokenizer, model=original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./feroutputs\"\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    predict_with_generate=True,\n",
    "    overwrite_output_dir=True,\n",
    "    #fp16=True,\n",
    "    #max_steps=2,\n",
    "    save_steps=1,\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 2028\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2028\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='2028' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 100/2028 01:59 < 39:19, 0.82 it/s, Epoch 0.05/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./feroutputs\\checkpoint-1\n",
      "Configuration saved in ./feroutputs\\checkpoint-1\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-1\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-1\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-1\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-1\\spiece.model\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-2\n",
      "Configuration saved in ./feroutputs\\checkpoint-2\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-2\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-2\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-2\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-2\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-1] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-3\n",
      "Configuration saved in ./feroutputs\\checkpoint-3\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-3\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-3\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-3\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-3\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-2] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-4\n",
      "Configuration saved in ./feroutputs\\checkpoint-4\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-4\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-4\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-4\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-4\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-3] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-5\n",
      "Configuration saved in ./feroutputs\\checkpoint-5\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-5\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-5\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-5\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-5\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-4] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-6\n",
      "Configuration saved in ./feroutputs\\checkpoint-6\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-6\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-6\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-6\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-6\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-5] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-7\n",
      "Configuration saved in ./feroutputs\\checkpoint-7\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-7\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-7\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-7\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-7\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-6] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-8\n",
      "Configuration saved in ./feroutputs\\checkpoint-8\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-8\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-8\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-8\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-8\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-7] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-9\n",
      "Configuration saved in ./feroutputs\\checkpoint-9\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-9\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-9\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-9\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-9\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-8] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-10\n",
      "Configuration saved in ./feroutputs\\checkpoint-10\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-10\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-10\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-9] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-11\n",
      "Configuration saved in ./feroutputs\\checkpoint-11\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-11\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-11\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-11\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-11\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-10] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-12\n",
      "Configuration saved in ./feroutputs\\checkpoint-12\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-12\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-12\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-12\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-12\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-11] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-13\n",
      "Configuration saved in ./feroutputs\\checkpoint-13\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-13\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-13\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-13\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-13\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-12] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-14\n",
      "Configuration saved in ./feroutputs\\checkpoint-14\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-14\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-14\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-14\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-14\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-13] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-15\n",
      "Configuration saved in ./feroutputs\\checkpoint-15\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-15\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-15\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-15\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-15\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-14] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-16\n",
      "Configuration saved in ./feroutputs\\checkpoint-16\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-16\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-16\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-16\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-16\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-15] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-17\n",
      "Configuration saved in ./feroutputs\\checkpoint-17\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-17\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-17\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-17\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-17\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-16] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-18\n",
      "Configuration saved in ./feroutputs\\checkpoint-18\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./feroutputs\\checkpoint-18\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-18\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-18\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-18\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-17] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-19\n",
      "Configuration saved in ./feroutputs\\checkpoint-19\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-19\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-19\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-19\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-19\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-18] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-20\n",
      "Configuration saved in ./feroutputs\\checkpoint-20\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-20\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-20\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-19] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-21\n",
      "Configuration saved in ./feroutputs\\checkpoint-21\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-21\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-21\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-21\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-21\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-20] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-22\n",
      "Configuration saved in ./feroutputs\\checkpoint-22\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-22\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-22\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-22\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-22\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-21] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-23\n",
      "Configuration saved in ./feroutputs\\checkpoint-23\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-23\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-23\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-23\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-23\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-22] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-24\n",
      "Configuration saved in ./feroutputs\\checkpoint-24\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-24\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-24\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-24\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-24\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-23] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-25\n",
      "Configuration saved in ./feroutputs\\checkpoint-25\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-25\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-25\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-25\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-25\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-24] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-26\n",
      "Configuration saved in ./feroutputs\\checkpoint-26\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-26\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-26\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-26\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-26\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-25] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-27\n",
      "Configuration saved in ./feroutputs\\checkpoint-27\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-27\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-27\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-27\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-27\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-26] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-28\n",
      "Configuration saved in ./feroutputs\\checkpoint-28\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-28\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-28\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-28\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-28\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-27] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-29\n",
      "Configuration saved in ./feroutputs\\checkpoint-29\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-29\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-29\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-29\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-29\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-28] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-30\n",
      "Configuration saved in ./feroutputs\\checkpoint-30\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-30\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-30\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-29] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-31\n",
      "Configuration saved in ./feroutputs\\checkpoint-31\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-31\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-31\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-31\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-31\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-30] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-32\n",
      "Configuration saved in ./feroutputs\\checkpoint-32\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-32\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-32\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-32\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-32\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-31] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-33\n",
      "Configuration saved in ./feroutputs\\checkpoint-33\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-33\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-33\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-33\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-33\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-32] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-34\n",
      "Configuration saved in ./feroutputs\\checkpoint-34\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-34\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-34\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-34\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-34\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-33] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-35\n",
      "Configuration saved in ./feroutputs\\checkpoint-35\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./feroutputs\\checkpoint-35\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-35\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-35\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-35\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-34] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-36\n",
      "Configuration saved in ./feroutputs\\checkpoint-36\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-36\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-36\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-36\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-36\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-35] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-37\n",
      "Configuration saved in ./feroutputs\\checkpoint-37\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-37\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-37\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-37\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-37\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-36] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-38\n",
      "Configuration saved in ./feroutputs\\checkpoint-38\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-38\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-38\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-38\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-38\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-37] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-39\n",
      "Configuration saved in ./feroutputs\\checkpoint-39\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-39\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-39\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-39\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-39\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-38] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-40\n",
      "Configuration saved in ./feroutputs\\checkpoint-40\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-40\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-40\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-39] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-41\n",
      "Configuration saved in ./feroutputs\\checkpoint-41\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-41\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-41\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-41\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-41\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-40] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-42\n",
      "Configuration saved in ./feroutputs\\checkpoint-42\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-42\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-42\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-42\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-42\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-41] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-43\n",
      "Configuration saved in ./feroutputs\\checkpoint-43\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-43\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-43\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-43\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-43\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-42] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-44\n",
      "Configuration saved in ./feroutputs\\checkpoint-44\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-44\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-44\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-44\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-44\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-43] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-45\n",
      "Configuration saved in ./feroutputs\\checkpoint-45\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-45\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-45\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-45\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-45\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-44] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-46\n",
      "Configuration saved in ./feroutputs\\checkpoint-46\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-46\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-46\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-46\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-46\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-45] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-47\n",
      "Configuration saved in ./feroutputs\\checkpoint-47\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-47\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-47\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-47\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-47\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-46] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-48\n",
      "Configuration saved in ./feroutputs\\checkpoint-48\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-48\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-48\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-48\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-48\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-47] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-49\n",
      "Configuration saved in ./feroutputs\\checkpoint-49\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-49\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-49\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-49\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-49\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-48] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-50\n",
      "Configuration saved in ./feroutputs\\checkpoint-50\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-50\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-50\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-49] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-51\n",
      "Configuration saved in ./feroutputs\\checkpoint-51\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-51\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-51\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-51\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-51\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-50] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-52\n",
      "Configuration saved in ./feroutputs\\checkpoint-52\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./feroutputs\\checkpoint-52\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-52\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-52\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-52\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-51] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-53\n",
      "Configuration saved in ./feroutputs\\checkpoint-53\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-53\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-53\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-53\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-53\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-52] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-54\n",
      "Configuration saved in ./feroutputs\\checkpoint-54\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-54\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-54\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-54\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-54\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-53] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-55\n",
      "Configuration saved in ./feroutputs\\checkpoint-55\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-55\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-55\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-55\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-55\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-54] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-56\n",
      "Configuration saved in ./feroutputs\\checkpoint-56\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-56\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-56\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-56\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-56\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-55] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-57\n",
      "Configuration saved in ./feroutputs\\checkpoint-57\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-57\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-57\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-57\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-57\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-56] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-58\n",
      "Configuration saved in ./feroutputs\\checkpoint-58\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-58\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-58\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-58\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-58\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-57] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-59\n",
      "Configuration saved in ./feroutputs\\checkpoint-59\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-59\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-59\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-59\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-59\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-58] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-60\n",
      "Configuration saved in ./feroutputs\\checkpoint-60\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-60\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-60\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-59] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-61\n",
      "Configuration saved in ./feroutputs\\checkpoint-61\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-61\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-61\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-61\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-61\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-60] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-62\n",
      "Configuration saved in ./feroutputs\\checkpoint-62\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-62\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-62\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-62\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-62\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-61] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-63\n",
      "Configuration saved in ./feroutputs\\checkpoint-63\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-63\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-63\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-63\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-63\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-62] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-64\n",
      "Configuration saved in ./feroutputs\\checkpoint-64\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-64\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-64\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-64\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-64\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-63] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-65\n",
      "Configuration saved in ./feroutputs\\checkpoint-65\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-65\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-65\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-65\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-65\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-64] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-66\n",
      "Configuration saved in ./feroutputs\\checkpoint-66\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-66\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-66\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-66\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-66\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-65] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-67\n",
      "Configuration saved in ./feroutputs\\checkpoint-67\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-67\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-67\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-67\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-67\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-66] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-68\n",
      "Configuration saved in ./feroutputs\\checkpoint-68\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-68\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-68\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-68\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-68\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-67] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-69\n",
      "Configuration saved in ./feroutputs\\checkpoint-69\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./feroutputs\\checkpoint-69\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-69\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-69\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-69\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-68] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-70\n",
      "Configuration saved in ./feroutputs\\checkpoint-70\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-70\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-70\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-69] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-71\n",
      "Configuration saved in ./feroutputs\\checkpoint-71\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-71\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-71\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-71\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-71\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-70] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-72\n",
      "Configuration saved in ./feroutputs\\checkpoint-72\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-72\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-72\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-72\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-72\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-71] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-73\n",
      "Configuration saved in ./feroutputs\\checkpoint-73\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-73\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-73\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-73\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-73\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-72] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-74\n",
      "Configuration saved in ./feroutputs\\checkpoint-74\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-74\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-74\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-74\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-74\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-73] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-75\n",
      "Configuration saved in ./feroutputs\\checkpoint-75\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-75\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-75\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-75\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-75\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-74] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-76\n",
      "Configuration saved in ./feroutputs\\checkpoint-76\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-76\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-76\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-76\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-76\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-75] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-77\n",
      "Configuration saved in ./feroutputs\\checkpoint-77\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-77\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-77\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-77\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-77\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-76] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-78\n",
      "Configuration saved in ./feroutputs\\checkpoint-78\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-78\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-78\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-78\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-78\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-77] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-79\n",
      "Configuration saved in ./feroutputs\\checkpoint-79\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-79\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-79\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-79\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-79\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-78] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-80\n",
      "Configuration saved in ./feroutputs\\checkpoint-80\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-80\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-80\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-79] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-81\n",
      "Configuration saved in ./feroutputs\\checkpoint-81\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-81\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-81\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-81\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-81\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-80] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-82\n",
      "Configuration saved in ./feroutputs\\checkpoint-82\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-82\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-82\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-82\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-82\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-81] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-83\n",
      "Configuration saved in ./feroutputs\\checkpoint-83\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-83\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-83\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-83\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-83\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-82] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-84\n",
      "Configuration saved in ./feroutputs\\checkpoint-84\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-84\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-84\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-84\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-84\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-83] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-85\n",
      "Configuration saved in ./feroutputs\\checkpoint-85\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-85\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-85\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-85\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-85\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-84] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-86\n",
      "Configuration saved in ./feroutputs\\checkpoint-86\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./feroutputs\\checkpoint-86\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-86\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-86\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-86\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-85] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-87\n",
      "Configuration saved in ./feroutputs\\checkpoint-87\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-87\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-87\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-87\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-87\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-86] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-88\n",
      "Configuration saved in ./feroutputs\\checkpoint-88\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-88\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-88\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-88\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-88\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-87] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-89\n",
      "Configuration saved in ./feroutputs\\checkpoint-89\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-89\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-89\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-89\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-89\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-88] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-90\n",
      "Configuration saved in ./feroutputs\\checkpoint-90\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-90\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-90\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-89] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-91\n",
      "Configuration saved in ./feroutputs\\checkpoint-91\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-91\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-91\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-91\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-91\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-90] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-92\n",
      "Configuration saved in ./feroutputs\\checkpoint-92\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-92\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-92\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-92\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-92\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-91] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-93\n",
      "Configuration saved in ./feroutputs\\checkpoint-93\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-93\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-93\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-93\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-93\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-92] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-94\n",
      "Configuration saved in ./feroutputs\\checkpoint-94\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-94\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-94\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-94\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-94\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-93] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-95\n",
      "Configuration saved in ./feroutputs\\checkpoint-95\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-95\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-95\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-95\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-95\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-94] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-96\n",
      "Configuration saved in ./feroutputs\\checkpoint-96\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-96\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-96\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-96\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-96\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-95] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-97\n",
      "Configuration saved in ./feroutputs\\checkpoint-97\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-97\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-97\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-97\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-97\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-96] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-98\n",
      "Configuration saved in ./feroutputs\\checkpoint-98\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-98\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-98\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-98\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-98\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-97] due to args.save_total_limit\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-99\n",
      "Configuration saved in ./feroutputs\\checkpoint-99\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-99\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-99\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-99\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-99\\spiece.model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1322\u001b[0m         )\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1627\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1629\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1630\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1799\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1800\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1801\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   1900\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_save\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1901\u001b[0m             \u001b[1;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1902\u001b[1;33m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOPTIMIZER_NAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1903\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcaught_warnings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1904\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSCHEDULER_NAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m                 \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    587\u001b[0m     \u001b[0mpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[0mpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m     \u001b[0mpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m     \u001b[0mdata_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_buf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m     \u001b[0mzip_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36m__reduce_ex__\u001b[1;34m(self, proto)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce_ex__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reduce_ex_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_unary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__reduce_ex__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36m_reduce_ex_internal\u001b[1;34m(self, proto)\u001b[0m\n\u001b[0;32m    215\u001b[0m             )\n\u001b[0;32m    216\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rebuild_meta_tensor_no_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_meta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_quantized\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m             \u001b[1;31m# quantizer_params can be different type based on torch attribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[0mquantizer_params\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqscheme\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./feroutputs/checkpoint-101\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "dialogue = ds['test'][index]['input_text']\n",
    "human_baseline_summary = ds['test'][index]['target_text']\n",
    "print(dialogue)\n",
    "#prompt = dialogue\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Find entities and relations from following sentence.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Entities and Relations :\n",
    "\"\"\"\n",
    "\n",
    "#input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "#original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "#original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "instruct_model_text_output = tokenizer.decode(\n",
    "    instruct_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "#instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "#print(instruct_model_outputs)\n",
    "#instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "#print(dash_line)\n",
    "#print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
