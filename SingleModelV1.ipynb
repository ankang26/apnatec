{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "import spacy\n",
    "import torch\n",
    "from spacy.language import Language\n",
    "from spacy import displacy\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "import statistics\n",
    "import os\n",
    "import json\n",
    "import calendar\n",
    "import holidays\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import hashlib\n",
    "from dateutil.parser import parse\n",
    "import shutil\n",
    "import ast\n",
    "from io import StringIO\n",
    "import requests\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\spacy_transformers\\pipeline_component.py:406: UserWarning: Automatically converting a transformer component from spacy-transformers v1.0 to v1.1+. If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spacy-transformers version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.set_custom_boundaries(doc)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy import displacy\n",
    "import time\n",
    "\n",
    "@Language.component(\"newsent\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        #print(token.text, token.text in (\"â€™s\", \"'s\"))\n",
    "        if token.text.upper() in (\";\", \"--\", \"\\n\\n\", \"\\n\", \"QUARTERLY\", \"STORY\", \"\\n\\n\\n\\n\", \"\\n\\n\\n\"):\n",
    "            #print(\"Detected:\", token.text)\n",
    "            doc[token.i].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "#spacy.require_gpu()\n",
    "nlp = spacy.load(\"../../Summary/NER/RelateEntity/train/model-best-local\")\n",
    "nlp.add_pipe('sentencizer')\n",
    "nlp.add_pipe('newsent', name=\"newsent\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentences(inputfile, nlp, text=None):\n",
    "    if(not text):\n",
    "        with open(inputfile, 'r', encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = [str(sent).strip() for sent in doc.sents]\n",
    "\n",
    "    #print(len(sentences))\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "def query1_from_list(context):\n",
    "    ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "    sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "    \n",
    "    ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "    sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "    \n",
    "    ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "    sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "    \n",
    "    tfewshot = f\"\"\"\n",
    "    Question: What are the relations present in the following text? \n",
    "    \n",
    "    Context: {\" * \".join(sent)}. \n",
    "    \n",
    "    Answer: {ans}.\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Question: What are the relations present in the following text? \n",
    "     \n",
    "    Context: {\" * \".join(sent1)}. \n",
    "    \n",
    "    Answer: {ans1}.\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Question: What are the relations present in the following text?\n",
    "    \n",
    "    Context: {\" * \".join(sent2)}. \n",
    "    \n",
    "    Answer: {ans2}\n",
    "    \n",
    "    \"\"\"\n",
    "    #print(tfewshot)\n",
    "    #print(\"\\n\\n\")\n",
    "    t5query = f\"\"\"{tfewshot}\n",
    "    Question: What are the relations present in the following text? \n",
    "    \n",
    "    Context:  {\" * \".join(context)}.\n",
    "    \n",
    "    Answer:\n",
    "    \n",
    "    \"\"\"\n",
    "    print(t5query)\n",
    "    inputs = tokenizer(t5query, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "context = [\"GAAP Gross profit for the third quarter of 2022 was $210 million\"]\n",
    "result = query1_from_list(context)\n",
    "print(f\"{result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Article: GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\n",
      "    \n",
      "    Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\"]}\n",
      "    \n",
      "    \n",
      "    Article: Non-GAAP net income is $110.1 million in third quarter 2022 @@@. \n",
      "    \n",
      "    Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\"]}\n",
      "    \n",
      "    \n",
      "    Article: Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@.\n",
      "    \n",
      "    Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\"]}\n",
      "    \n",
      "    \n",
      "    Article: GAAP Gross profit for the third quarter of 2022 was $210 million.\n",
      "    \n",
      "    Question: What are the relations present in the text? Display it in json format.\n",
      "\"RELATIONS\": [\"KEY:GAAP Gross Profit\"]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "def query1_from_list(context):\n",
    "    ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "    sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "    \n",
    "    ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "    sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "    \n",
    "    ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "    sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "    \n",
    "    tfewshot = f\"\"\"\n",
    "    Article: {\" * \".join(sent)}\n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format. {ans}\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Article: {\" * \".join(sent1)}. \n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format. {ans1}\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Article: {\" * \".join(sent2)}.\n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format. {ans2}\n",
    "    \n",
    "    \"\"\"\n",
    "    #print(tfewshot)\n",
    "    #print(\"\\n\\n\")\n",
    "    t5query = f\"\"\"{tfewshot}\n",
    "    Article: {\" * \".join(context)}.\n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format.\"\"\"\n",
    "    print(t5query)\n",
    "    inputs = tokenizer(t5query, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "context = [\"GAAP Gross profit for the third quarter of 2022 was $210 million\"]\n",
    "result = query1_from_list(context)\n",
    "print(f\"{result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output File ../../Summary/DATA/FLAN/Train/APPN_2022-11-03_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/APPN_2023-02-16_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/APPN_2023-05-09_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/APPN_2023-08-03_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/BILL_2022-08-18_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/BILL_2022-11-03_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/BILL_2023-02-02_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/BILL_2023-05-04_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/BILL_2023-08-17_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/CFLT_2022-08-03_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/CFLT_2022-11-02_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/CFLT_2023-01-30_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/CFLT_2023-05-03_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/CFLT_2023-08-02_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/CRWD_2022-08-30_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/CRWD_2022-11-29_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/CRWD_2023-03-07_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/CRWD_2023-05-31_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/CRWD_2023-08-30_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DDOG_2022-08-04_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DDOG_2022-11-02_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DDOG_2023-02-16_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DDOG_2023-05-04_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DDOG_2023-08-08_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DOCN_2022-11-07_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DOCN_2023-02-16_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DOCN_2023-05-09_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DOCN_2023-08-03_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DOCU_2022-09-08_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DOCU_2022-12-08_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DOCU_2023-03-09_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DOCU_2023-06-08_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/FIVN_2022-07-28_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/FIVN_2022-11-07_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/FIVN_2023-02-22_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/FIVN_2023-05-04_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/FIVN_2023-08-07_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/HUBS_2022-08-04_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/HUBS_2022-11-02_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/HUBS_2023-02-16_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/HUBS_2023-05-03_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/HUBS_2023-08-02_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/MDB_2022-08-31_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/MDB_2022-12-06_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/MDB_2023-03-08_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/MDB_2023-06-01_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/MDB_2023-08-31_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/NET_2022-08-04_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/NET_2022-11-03_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/NET_2023-02-09_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/NET_2023-04-27_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/NET_2023-08-03_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/OKTA_2022-08-31_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/OKTA_2022-11-30_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/OKTA_2023-03-02_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/OKTA_2023-05-31_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/OKTA_2023-08-30_EP_YH_ER.tsv already exists\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/OTHER_DEV_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/OTHER_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PATH_2022-09-06_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PATH_2022-12-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PATH_2023-03-15_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PATH_2023-05-24_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PAYC_2022-08-02_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PAYC_2022-11-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PAYC_2023-02-07_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PAYC_2023-05-02_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PAYC_2023-08-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PLTR_2022-08-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PLTR_2022-11-07_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PLTR_2023-02-13_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PLTR_2023-05-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PLTR_2023-08-07_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/RNG_2022-08-02_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/RNG_2022-11-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/RNG_2023-02-15_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/RNG_2023-05-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/RNG_2023-08-07_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/SNOW_2022-08-24_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/SNOW_2022-11-30_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/SNOW_2023-03-02_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/SNOW_2023-05-24_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/SNOW_2023-08-23_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/S_2022-08-31_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/S_2022-12-06_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/S_2023-03-14_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/S_2023-06-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/S_2023-08-31_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TTD_2022-08-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TTD_2022-11-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TTD_2023-02-15_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TTD_2023-05-10_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TTD_2023-08-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TWLO_2022-08-04_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TWLO_2022-11-03_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TWLO_2023-02-15_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TWLO_2023-05-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TWLO_2023-08-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/UPST_2022-08-08_EP_YH_ER.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Output File ../../Summary/DATA/FLAN/Train/UPST_2022-11-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/UPST_2023-02-14_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/UPST_2023-05-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/UPST_2023-08-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZI_2022-08-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZI_2022-11-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZI_2023-02-06_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZI_2023-05-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZI_2023-07-31_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZM_2022-08-22_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZM_2022-11-21_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZM_2023-02-28_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZM_2023-05-22_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZM_2023-08-21_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZS_2022-09-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZS_2022-12-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZS_2023-03-03_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZS_2023-06-01_EP_YH_ER.tsv\n"
     ]
    }
   ],
   "source": [
    "# Format 2, includiing entities and relations\n",
    "maxFiles = 60\n",
    "fileCnt = 0\n",
    "source = \"../../Summary/DATA/FLAN/Backup/Format-1\"\n",
    "files = glob.glob(source+\"/*_ER.tsv\")\n",
    "#print(files)\n",
    "for file in files:\n",
    "    #print(file)\n",
    "    basefile = os.path.basename(file)\n",
    "    #print(basefile)\n",
    "    outfile = \"../../Summary/DATA/FLAN/Train/\"+basefile\n",
    "    of = None\n",
    "    #print(outfile)\n",
    "    if outfile:\n",
    "        outfilePath = Path(outfile)\n",
    "        if outfilePath.is_file():\n",
    "            print(\"Output File {} already exists\".format(outfile))\n",
    "            continue\n",
    "        \n",
    "        if(maxFiles > 0):\n",
    "            fileCnt = fileCnt + 1\n",
    "            if(fileCnt > maxFiles):\n",
    "                break\n",
    "            \n",
    "        print(\"Creating Output File {}\".format(outfile))\n",
    "        of = open(outfilePath, \"w\", encoding = \"utf-8\")\n",
    "        \n",
    "        \n",
    "    with open(file, \"r\", encoding = \"ISO-8859-1\") as f:\n",
    "        line = f.readline()\n",
    "        #print(line)\n",
    "        while line:\n",
    "            if (\"RELATIONS\" not in line):\n",
    "                of.write(line)\n",
    "                #of.write(\"\\n\")\n",
    "            else:\n",
    "                relations = dict()\n",
    "                nsplit = line.split(\"\\t\")\n",
    "                relation = nsplit[2]\n",
    "                relation = relation.replace(\"\\n\",\"\")\n",
    "                sent = nsplit[1]\n",
    "                sent = sent.replace(\"\\t\",\"\")\n",
    "                #print(sent)\n",
    "                relation = json.loads(relation)\n",
    "                #print(relation)\n",
    "                sentences = getSentences(None, nlp, sent)\n",
    "                nerl = None\n",
    "                for l in sentences:\n",
    "                    text1 = list()\n",
    "                    text1.append(l)\n",
    "                    for doc in nlp.pipe(text1, disable=[\"tagger\"]):\n",
    "                        for ent in doc.ents:\n",
    "                            if(not nerl):\n",
    "                                nerl = ent.label_.replace(\":\",\"\").replace(\",\",\"\")+\"=\"+ent.text.replace(\":\",\"\").replace(\",\",\"\")\n",
    "                            else:\n",
    "                                nerl = nerl + \",\" + ent.label_.replace(\":\",\"\").replace(\",\",\"\")+\"=\"+ent.text.replace(\":\",\"\").replace(\",\",\"\")\n",
    "                #relations[\"ENTITIES\"] = None\n",
    "                relations[\"ENTITIES\"] = list()\n",
    "                relations[\"ENTITIES\"].append(nerl)\n",
    "                #rlist = list()\n",
    "                #for rstr in relation[\"RELATIONS\"]:\n",
    "                #    rstr = rstr.replace(\",MONEY\",\"!!MONEY\").replace(\",TYPE\",\"!!TYPE\").replace(\",CD\",\"!!CD\").replace(\",RELATION\",\"!!RELATION\").replace(\",PCT\",\"!!PCT\").replace(\",DATE\",\"!!DATE\").replace(\",CALENDAR\",\"!!CALENDAR\")\n",
    "                #    rlist.append(rstr)\n",
    "                relations[\"RELATIONS\"] = relation[\"RELATIONS\"]\n",
    "                #print(relations)\n",
    "                nsplit[2] = json.dumps(relations)\n",
    "                nsplit[1] = sent\n",
    "                nline = \"\\t\".join(nsplit)\n",
    "                #nline = nline + \"</s>\" #Not Required\n",
    "                of.write(nline)\n",
    "                of.write(\"\\n\")\n",
    "            line = f.readline()\n",
    "    if(of):\n",
    "        of.close()\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "devDataFile = \"../../Summary/DATA/FLAN/Dev/dev.tsv\"\n",
    "trainDataFile = \"../../Summary/DATA/FLAN/Train/train.tsv\"\n",
    "testDataFile = \"../../Summary/DATA/FLAN/Test/test.tsv\"\n",
    "\n",
    "trainDir = \"../../Summary/DATA/FLAN/Train\"\n",
    "devDir = \"../../Summary/DATA/FLAN/Dev\"\n",
    "testDir = \"../../Summary/DATA/FLAN/Test\"\n",
    "\n",
    "def writeTrainingData(writeFile, writeDir):\n",
    "    files = glob.glob(writeDir+\"/*_ER.tsv\")\n",
    "    print(files)\n",
    "    frames = list()\n",
    "\n",
    "    if(len(files) > 0):\n",
    "        for file in files:\n",
    "            print(file)\n",
    "            #df = pd.read_csv(file, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "            df = pd.read_csv(file, sep=\"\\t\", encoding = \"ISO-8859-1\").astype(str)\n",
    "            df = df.dropna()\n",
    "            df = df[df['Sentence1'].notna()]\n",
    "            #print(df)\n",
    "            frames.append(df)\n",
    "    result = pd.concat(frames)\n",
    "    print(result)\n",
    "    result.to_csv(writeFile, sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../Summary/DATA/FLAN/Train\\\\APPN_2023-02-16_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\APPN_2023-05-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\APPN_2023-08-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\BILL_2022-08-18_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\BILL_2023-02-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\BILL_2023-05-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\BILL_2023-08-17_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CFLT_2022-08-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CFLT_2022-11-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CFLT_2023-05-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CFLT_2023-08-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CRWD_2022-08-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CRWD_2022-11-29_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CRWD_2023-03-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CRWD_2023-08-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DDOG_2022-08-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DDOG_2022-11-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DDOG_2023-02-16_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DDOG_2023-05-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCN_2023-02-16_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCN_2023-05-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCN_2023-08-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCU_2022-09-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCU_2022-12-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCU_2023-06-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\FIVN_2022-07-28_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\FIVN_2022-11-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\FIVN_2023-05-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\FIVN_2023-08-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\HUBS_2022-08-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\HUBS_2022-11-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\HUBS_2023-02-16_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\HUBS_2023-08-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\MDB_2022-08-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\MDB_2022-12-06_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\MDB_2023-03-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\MDB_2023-06-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\NET_2022-11-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\NET_2023-02-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\NET_2023-04-27_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\NET_2023-08-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\OKTA_2022-08-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\OKTA_2023-03-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\OKTA_2023-05-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\OKTA_2023-08-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\OTHER_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PATH_2022-09-06_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PATH_2022-12-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PATH_2023-05-24_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PAYC_2022-08-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PAYC_2022-11-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PAYC_2023-02-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PAYC_2023-05-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PLTR_2022-08-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PLTR_2022-11-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PLTR_2023-02-13_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PLTR_2023-05-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\RNG_2022-11-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\RNG_2023-02-15_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\RNG_2023-05-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\RNG_2023-08-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\SNOW_2022-08-24_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\SNOW_2022-11-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\SNOW_2023-05-24_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\SNOW_2023-08-23_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\S_2022-08-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\S_2023-03-14_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\S_2023-06-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\S_2023-08-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TTD_2022-08-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TTD_2022-11-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TTD_2023-02-15_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TTD_2023-08-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TWLO_2022-08-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TWLO_2022-11-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TWLO_2023-02-15_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TWLO_2023-05-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\UPST_2022-11-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\UPST_2023-02-14_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\UPST_2023-05-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\UPST_2023-08-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZI_2022-08-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZI_2023-02-06_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZI_2023-05-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZI_2023-07-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZM_2022-08-22_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZM_2022-11-21_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZM_2023-05-22_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZM_2023-08-21_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZS_2022-09-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZS_2022-12-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZS_2023-06-01_EP_YH_ER.tsv']\n",
      "../../Summary/DATA/FLAN/Train\\APPN_2023-02-16_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\APPN_2023-05-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\APPN_2023-08-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\BILL_2022-08-18_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\BILL_2023-02-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\BILL_2023-05-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\BILL_2023-08-17_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CFLT_2022-08-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CFLT_2022-11-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CFLT_2023-05-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CFLT_2023-08-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CRWD_2022-08-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CRWD_2022-11-29_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CRWD_2023-03-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CRWD_2023-08-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DDOG_2022-08-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DDOG_2022-11-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DDOG_2023-02-16_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DDOG_2023-05-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCN_2023-02-16_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCN_2023-05-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCN_2023-08-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCU_2022-09-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCU_2022-12-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCU_2023-06-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\FIVN_2022-07-28_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\FIVN_2022-11-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\FIVN_2023-05-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\FIVN_2023-08-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\HUBS_2022-08-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\HUBS_2022-11-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\HUBS_2023-02-16_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\HUBS_2023-08-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\MDB_2022-08-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\MDB_2022-12-06_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\MDB_2023-03-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\MDB_2023-06-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\NET_2022-11-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\NET_2023-02-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\NET_2023-04-27_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\NET_2023-08-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\OKTA_2022-08-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\OKTA_2023-03-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\OKTA_2023-05-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\OKTA_2023-08-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\OTHER_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PATH_2022-09-06_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PATH_2022-12-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PATH_2023-05-24_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PAYC_2022-08-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PAYC_2022-11-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PAYC_2023-02-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PAYC_2023-05-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PLTR_2022-08-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PLTR_2022-11-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PLTR_2023-02-13_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PLTR_2023-05-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\RNG_2022-11-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\RNG_2023-02-15_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\RNG_2023-05-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\RNG_2023-08-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\SNOW_2022-08-24_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\SNOW_2022-11-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\SNOW_2023-05-24_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\SNOW_2023-08-23_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\S_2022-08-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\S_2023-03-14_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\S_2023-06-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\S_2023-08-31_EP_YH_ER.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Summary/DATA/FLAN/Train\\TTD_2022-08-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TTD_2022-11-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TTD_2023-02-15_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TTD_2023-08-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TWLO_2022-08-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TWLO_2022-11-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TWLO_2023-02-15_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TWLO_2023-05-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\UPST_2022-11-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\UPST_2023-02-14_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\UPST_2023-05-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\UPST_2023-08-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZI_2022-08-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZI_2023-02-06_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZI_2023-05-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZI_2023-07-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZM_2022-08-22_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZM_2022-11-21_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZM_2023-05-22_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZM_2023-08-21_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZS_2022-09-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZS_2022-12-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZS_2023-06-01_EP_YH_ER.tsv\n",
      "                 filename                                          Sentence1  \\\n",
      "0   APPN_2023-02-16_EP_YH  GAAP Net Loss Per Share is $(0.47) in fourth q...   \n",
      "1   APPN_2023-02-16_EP_YH  Cash And Cash Equivalents is $148132 T in four...   \n",
      "2   APPN_2023-02-16_EP_YH  GAAP Gross Profit is $90555 T in fourth quarte...   \n",
      "3   APPN_2023-02-16_EP_YH  GAAP Gross Margin is 71.99% in fourth quarter ...   \n",
      "4   APPN_2023-02-16_EP_YH                                              PG***   \n",
      "..                    ...                                                ...   \n",
      "61    ZS_2023-06-01_EP_YH  We have not reconciled our expectations to non...   \n",
      "62    ZS_2023-06-01_EP_YH  For those reasons, we are also unable to addre...   \n",
      "63    ZS_2023-06-01_EP_YH  Accordingly, a reconciliation for the guidance...   \n",
      "64    ZS_2023-06-01_EP_YH  PG*** In the third quarter of fiscal 2023, we ...   \n",
      "65    ZS_2023-06-01_EP_YH  PG*** For further information regarding why we...   \n",
      "\n",
      "                                            Sentence2  \n",
      "0   {\"ENTITIES\": [\"METRIC=GAAP Net Loss Per Share,...  \n",
      "1   {\"ENTITIES\": [\"METRIC=Cash,METRIC=Cash Equival...  \n",
      "2   {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Profit...  \n",
      "3   {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Margin...  \n",
      "4             {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "..                                                ...  \n",
      "61  {\"ENTITIES\": [\"METRIC=non-GAAP,METRIC=income f...  \n",
      "62            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "63            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "64  {\"ENTITIES\": [\"CALENDAR=third quarter,METRIC=i...  \n",
      "65            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[5285 rows x 3 columns]\n",
      "['../../Summary/DATA/FLAN/Dev\\\\APPN_2022-11-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\BILL_2022-11-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\CFLT_2023-01-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\CRWD_2023-05-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\DDOG_2023-08-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\DOCN_2022-11-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\DOCU_2023-03-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\HUBS_2023-05-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\MDB_2023-08-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\NET_2022-08-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\OKTA_2022-11-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\OTHER_DEV_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\PATH_2023-03-15_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\PAYC_2023-08-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\PLTR_2023-08-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\RNG_2022-08-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\SNOW_2023-03-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\TTD_2023-05-10_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\TWLO_2023-08-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\UPST_2022-08-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\ZI_2022-11-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\ZM_2023-02-28_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\ZS_2023-03-03_EP_YH_ER.tsv']\n",
      "../../Summary/DATA/FLAN/Dev\\APPN_2022-11-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\BILL_2022-11-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\CFLT_2023-01-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\CRWD_2023-05-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\DDOG_2023-08-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\DOCN_2022-11-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\DOCU_2023-03-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\HUBS_2023-05-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\MDB_2023-08-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\NET_2022-08-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\OKTA_2022-11-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\OTHER_DEV_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\PATH_2023-03-15_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\PAYC_2023-08-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\PLTR_2023-08-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\RNG_2022-08-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\SNOW_2023-03-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\TTD_2023-05-10_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\TWLO_2023-08-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\UPST_2022-08-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\ZI_2022-11-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\ZM_2023-02-28_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\ZS_2023-03-03_EP_YH_ER.tsv\n",
      "                 filename                                          Sentence1  \\\n",
      "0   APPN_2022-11-03_EP_YH  GAAP Net Loss Per Share is $(0.61) in third qu...   \n",
      "1   APPN_2022-11-03_EP_YH  Cash And Cash Equivalents is $51802 T in third...   \n",
      "2   APPN_2022-11-03_EP_YH  GAAP Gross Profit is $84116 T in third quarter...   \n",
      "3   APPN_2022-11-03_EP_YH  GAAP Gross Margin is 71.36% in third quarter 2...   \n",
      "4   APPN_2022-11-03_EP_YH                                              PG***   \n",
      "..                    ...                                                ...   \n",
      "63    ZS_2023-03-03_EP_YH  Accordingly, we are required to add back the n...   \n",
      "64    ZS_2023-03-03_EP_YH  Additionally, we include the anti-dilutive imp...   \n",
      "65    ZS_2023-03-03_EP_YH  We have not reconciled our expectations to non...   \n",
      "66    ZS_2023-03-03_EP_YH  For those reasons, we are also unable to addre...   \n",
      "67    ZS_2023-03-03_EP_YH  Accordingly, a reconciliation for the guidance...   \n",
      "\n",
      "                                            Sentence2  \n",
      "0   {\"ENTITIES\": [\"METRIC=GAAP Net Loss Per Share,...  \n",
      "1   {\"ENTITIES\": [\"METRIC=Cash,METRIC=Cash Equival...  \n",
      "2   {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Profit...  \n",
      "3   {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Margin...  \n",
      "4             {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "..                                                ...  \n",
      "63  {\"ENTITIES\": [\"METRIC=non-GAAP,METRIC=net inco...  \n",
      "64            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "65  {\"ENTITIES\": [\"METRIC=non-GAAP,METRIC=income f...  \n",
      "66            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "67            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[1462 rows x 3 columns]\n",
      "['../../Summary/DATA/FLAN/Test\\\\FIVN_2023-02-22_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Test\\\\S_2022-12-06_EP_YH_ER.tsv']\n",
      "../../Summary/DATA/FLAN/Test\\FIVN_2023-02-22_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Test\\S_2022-12-06_EP_YH_ER.tsv\n",
      "                 filename                                          Sentence1  \\\n",
      "0   FIVN_2023-02-22_EP_YH  GAAP Net Loss Per Share is $(0.19) in fourth q...   \n",
      "1   FIVN_2023-02-22_EP_YH  Cash And Cash Equivalents is $180520 T in four...   \n",
      "2   FIVN_2023-02-22_EP_YH  GAAP Gross Profit is $112051 T in fourth quart...   \n",
      "3   FIVN_2023-02-22_EP_YH  GAAP Gross Margin is 53.78% in fourth quarter ...   \n",
      "4   FIVN_2023-02-22_EP_YH  GAAP Free Cash Flow is $36.6MN in fourth quart...   \n",
      "..                    ...                                                ...   \n",
      "34     S_2022-12-06_EP_YH  PG*** These statements are forward-looking and...   \n",
      "35     S_2022-12-06_EP_YH  Refer to the below for information on the fact...   \n",
      "36     S_2022-12-06_EP_YH  PG*** Guidance for non-GAAP financial measures...   \n",
      "37     S_2022-12-06_EP_YH  We have not provided the most directly compara...   \n",
      "38     S_2022-12-06_EP_YH  Accordingly, a reconciliation of non-GAAP gros...   \n",
      "\n",
      "                                            Sentence2  \n",
      "0   {\"ENTITIES\": [\"METRIC=GAAP Net Loss Per Share,...  \n",
      "1   {\"ENTITIES\": [\"METRIC=Cash,METRIC=Cash Equival...  \n",
      "2   {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Profit...  \n",
      "3   {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Margin...  \n",
      "4   {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Free Cash Fl...  \n",
      "..                                                ...  \n",
      "34            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "35            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "36            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "37            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "38            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[82 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "writeTrainingData(trainDataFile, trainDir)\n",
    "writeTrainingData(devDataFile, devDir)\n",
    "writeTrainingData(testDataFile, testDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATA ..............\n",
      "                                             input_text  \\\n",
      "0     GAAP Net Loss Per Share is $(0.47) in fourth q...   \n",
      "1     Cash And Cash Equivalents is $148132 T in four...   \n",
      "2     GAAP Gross Profit is $90555 T in fourth quarte...   \n",
      "3     GAAP Gross Margin is 71.99% in fourth quarter ...   \n",
      "4                                                 PG***   \n",
      "...                                                 ...   \n",
      "5280  We have not reconciled our expectations to non...   \n",
      "5281  For those reasons, we are also unable to addre...   \n",
      "5282  Accordingly, a reconciliation for the guidance...   \n",
      "5283  PG*** In the third quarter of fiscal 2023, we ...   \n",
      "5284  PG*** For further information regarding why we...   \n",
      "\n",
      "                                            target_text  \n",
      "0     {\"ENTITIES\": [\"METRIC=GAAP Net Loss Per Share,...  \n",
      "1     {\"ENTITIES\": [\"METRIC=Cash,METRIC=Cash Equival...  \n",
      "2     {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Profit...  \n",
      "3     {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Margin...  \n",
      "4               {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "...                                                 ...  \n",
      "5280  {\"ENTITIES\": [\"METRIC=non-GAAP,METRIC=income f...  \n",
      "5281            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "5282            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "5283  {\"ENTITIES\": [\"CALENDAR=third quarter,METRIC=i...  \n",
      "5284            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[5285 rows x 2 columns]\n",
      "EVAL DATA ..............\n",
      "                                             input_text  \\\n",
      "0     GAAP Net Loss Per Share is $(0.61) in third qu...   \n",
      "1     Cash And Cash Equivalents is $51802 T in third...   \n",
      "2     GAAP Gross Profit is $84116 T in third quarter...   \n",
      "3     GAAP Gross Margin is 71.36% in third quarter 2...   \n",
      "4                                                 PG***   \n",
      "...                                                 ...   \n",
      "1457  Accordingly, we are required to add back the n...   \n",
      "1458  Additionally, we include the anti-dilutive imp...   \n",
      "1459  We have not reconciled our expectations to non...   \n",
      "1460  For those reasons, we are also unable to addre...   \n",
      "1461  Accordingly, a reconciliation for the guidance...   \n",
      "\n",
      "                                            target_text  \n",
      "0     {\"ENTITIES\": [\"METRIC=GAAP Net Loss Per Share,...  \n",
      "1     {\"ENTITIES\": [\"METRIC=Cash,METRIC=Cash Equival...  \n",
      "2     {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Profit...  \n",
      "3     {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Margin...  \n",
      "4               {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "...                                                 ...  \n",
      "1457  {\"ENTITIES\": [\"METRIC=non-GAAP,METRIC=net inco...  \n",
      "1458            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "1459  {\"ENTITIES\": [\"METRIC=non-GAAP,METRIC=income f...  \n",
      "1460            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "1461            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[1462 rows x 2 columns]\n",
      "TEST DATA ..............\n",
      "                                           input_text  \\\n",
      "0   GAAP Net Loss Per Share is $(0.19) in fourth q...   \n",
      "1   Cash And Cash Equivalents is $180520 T in four...   \n",
      "2   GAAP Gross Profit is $112051 T in fourth quart...   \n",
      "3   GAAP Gross Margin is 53.78% in fourth quarter ...   \n",
      "4   GAAP Free Cash Flow is $36.6MN in fourth quart...   \n",
      "..                                                ...   \n",
      "77  PG*** These statements are forward-looking and...   \n",
      "78  Refer to the below for information on the fact...   \n",
      "79  PG*** Guidance for non-GAAP financial measures...   \n",
      "80  We have not provided the most directly compara...   \n",
      "81  Accordingly, a reconciliation of non-GAAP gros...   \n",
      "\n",
      "                                          target_text  \n",
      "0   {\"ENTITIES\": [\"METRIC=GAAP Net Loss Per Share,...  \n",
      "1   {\"ENTITIES\": [\"METRIC=Cash,METRIC=Cash Equival...  \n",
      "2   {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Profit...  \n",
      "3   {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Margin...  \n",
      "4   {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Free Cash Fl...  \n",
      "..                                                ...  \n",
      "77            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "78            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "79            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "80            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "81            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[82 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(trainDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "eval_df = pd.read_csv(devDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "test_df = pd.read_csv(testDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "\n",
    "train_df = train_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "eval_df = eval_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "test_df = test_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "\n",
    "train_df = train_df[[\"input_text\", \"target_text\"]]\n",
    "eval_df = eval_df[[\"input_text\", \"target_text\"]]\n",
    "test_df = test_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "#train_df[\"prefix\"] = \"paraphrase\"\n",
    "#train_df = train_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "#train_df = train_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "#eval_df[\"prefix\"] = \"paraphrase\"\n",
    "#eval_df = eval_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "#eval_df = eval_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "train_df = train_df[train_df['input_text'].notna()]\n",
    "\n",
    "eval_df = eval_df.dropna()\n",
    "eval_df = eval_df[eval_df['input_text'].notna()]\n",
    "\n",
    "test_df = test_df.dropna()\n",
    "test_df = test_df[test_df['input_text'].notna()]\n",
    "\n",
    "#train_df[\"input_text\"] = train_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "#train_df[\"target_text\"] = train_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "print(\"TRAIN DATA ..............\")\n",
    "print(train_df)\n",
    "\n",
    "#eval_df[\"input_text\"] = eval_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "#eval_df[\"target_text\"] = eval_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "print(\"EVAL DATA ..............\")\n",
    "print(eval_df)\n",
    "\n",
    "print(\"TEST DATA ..............\")\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/flan-t5-small\n"
     ]
    }
   ],
   "source": [
    "modelPath = \"./feroutputs/pytorch_model.bin\"\n",
    "model_name='google/flan-t5-small'\n",
    "if os.path.isfile(modelPath):\n",
    "    model_name = \"./feroutputs/\"\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model_name = \"facebook/bart-base\"\n",
    "\n",
    "#original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#original_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 76961152\n",
      "all model parameters: 76961152\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Article: GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\n",
      "\n",
      "Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\"]}\n",
      "\n",
      "\n",
      "Article: Non-GAAP net income is $110.1 million in third quarter 2022 @@@. \n",
      "\n",
      "Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\"]}\n",
      "\n",
      "\n",
      "Article: Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@.\n",
      "\n",
      "Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\"]}\n",
      "\n",
      "\n",
      "Article: GAAP Net Loss Per Share is $(0.47) in fourth quarter 2022 @@@.\n",
      "\n",
      "Question: What are the relations present in the text? Display it in json format.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "{\"RELATIONS\": [\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.47!!LINK:KV\"]}\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "['\"RELATIONS\": [\"KEY:GAAP Net Loss Per Share\": -$0.47 MN!!LINK:KV\"]']\n"
     ]
    }
   ],
   "source": [
    "inputText = (train_df[\"input_text\"][0])\n",
    "outputText = (train_df[\"target_text\"][0])\n",
    "\n",
    "ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "\n",
    "ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "\n",
    "ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "\n",
    "tfewshot = f\"\"\"\n",
    "Article: {\" * \".join(sent)}\n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format. {ans}\n",
    "\n",
    "\"\"\"\n",
    "tfewshot += f\"\"\"\n",
    "Article: {\" * \".join(sent1)}. \n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format. {ans1}\n",
    "\n",
    "\"\"\"\n",
    "tfewshot += f\"\"\"\n",
    "Article: {\" * \".join(sent2)}.\n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format. {ans2}\n",
    "\n",
    "\"\"\"\n",
    "#print(tfewshot)\n",
    "#print(\"\\n\\n\")\n",
    "t5query = f\"\"\"{tfewshot}\n",
    "Article: {inputText}.\n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format.\"\"\"\n",
    "#print(t5query)\n",
    "inputs = tokenizer(t5query, return_tensors=\"pt\")\n",
    "outputs = original_model.generate(**inputs, max_new_tokens=100)\n",
    "output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{t5query}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{outputText}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'target_text', '__index_level_0__'],\n",
      "        num_rows: 5285\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_text', 'target_text', '__index_level_0__'],\n",
      "        num_rows: 1462\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_text', 'target_text', '__index_level_0__'],\n",
      "        num_rows: 82\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train = Dataset.from_pandas(train_df)\n",
    "valid = Dataset.from_pandas(eval_df)\n",
    "test = Dataset.from_pandas(test_df)\n",
    "\n",
    "ds = DatasetDict()\n",
    "\n",
    "ds['train'] = train\n",
    "ds['validation'] = valid\n",
    "ds['test'] = test\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6747 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 177\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6747 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 228\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([ds[\"train\"], ds[\"validation\"]]).map(lambda x: tokenizer(x[\"input_text\"], truncation=True), batched=True, remove_columns=[\"input_text\", \"target_text\", \"__index_level_0__\"])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([ds[\"train\"], ds[\"validation\"]]).map(lambda x: tokenizer(x[\"target_text\"], truncation=True), batched=True, remove_columns=[\"input_text\", \"target_text\", \"__index_level_0__\"])\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5283 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1464 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/82 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    #print(example)\n",
    "    start_prompt = 'Article: '\n",
    "    end_prompt = '\\n\\nQuestion: What are the relations present in the text? Display it in json format.'\n",
    "    prompt = [start_prompt + sentence + end_prompt for sentence in example[\"input_text\"]]\n",
    "    #print(prompt)\n",
    "    example['input_ids'] = tokenizer(prompt, max_length=max_source_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"target_text\"], max_length=max_target_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    #print(example)\n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['input_text', 'target_text', '__index_level_0__',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1462 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/82 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def newtokenize_function(example, padding=\"max_length\"):\n",
    "    #print(example)\n",
    "    start_prompt = 'Article: '\n",
    "    #end_prompt = '\\n\\nQuestion: What are the relations present in the text? Display it in json format.'\n",
    "    end_prompt = '\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.'\n",
    "    prompt = [start_prompt + sentence + end_prompt for sentence in example[\"input_text\"]]\n",
    "    #print(prompt)\n",
    "    model_inputs = tokenizer(prompt, max_length=max_source_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(text=example[\"target_text\"], max_length=max_target_length, padding=\"max_length\", truncation=True)\n",
    "    #print(example)\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = ds.map(newtokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['input_text', 'target_text', '__index_level_0__',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
    "#tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (5285, 3)\n",
      "Validation: (1462, 3)\n",
      "Test: (82, 3)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5285\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1462\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 82\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ankan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=original_model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "#data_collator = DataCollatorForSeq2Seq(tokenizer, model=original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./feroutputs\"\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    overwrite_output_dir=True,\n",
    "    fp16=False,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 5285\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 52850\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='52850' max='52850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [52850/52850 10:53:55, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.184000</td>\n",
       "      <td>0.121518</td>\n",
       "      <td>42.243400</td>\n",
       "      <td>32.765900</td>\n",
       "      <td>42.068300</td>\n",
       "      <td>42.116600</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.130100</td>\n",
       "      <td>0.082890</td>\n",
       "      <td>44.059700</td>\n",
       "      <td>37.413200</td>\n",
       "      <td>43.932000</td>\n",
       "      <td>43.977700</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.092100</td>\n",
       "      <td>0.064758</td>\n",
       "      <td>44.550300</td>\n",
       "      <td>38.660300</td>\n",
       "      <td>44.442200</td>\n",
       "      <td>44.479500</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.061713</td>\n",
       "      <td>44.302200</td>\n",
       "      <td>38.644900</td>\n",
       "      <td>44.259400</td>\n",
       "      <td>44.306400</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.055486</td>\n",
       "      <td>44.799800</td>\n",
       "      <td>39.312900</td>\n",
       "      <td>44.758200</td>\n",
       "      <td>44.796700</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.054627</td>\n",
       "      <td>44.846000</td>\n",
       "      <td>39.397600</td>\n",
       "      <td>44.808100</td>\n",
       "      <td>44.864900</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.045600</td>\n",
       "      <td>0.054422</td>\n",
       "      <td>44.969900</td>\n",
       "      <td>39.505500</td>\n",
       "      <td>44.922500</td>\n",
       "      <td>44.985600</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.046400</td>\n",
       "      <td>0.051444</td>\n",
       "      <td>45.150000</td>\n",
       "      <td>39.814500</td>\n",
       "      <td>45.127500</td>\n",
       "      <td>45.166500</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>0.051077</td>\n",
       "      <td>45.103400</td>\n",
       "      <td>39.732000</td>\n",
       "      <td>45.077500</td>\n",
       "      <td>45.111700</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.050814</td>\n",
       "      <td>45.162700</td>\n",
       "      <td>39.809600</td>\n",
       "      <td>45.119700</td>\n",
       "      <td>45.175100</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1462\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-5285\n",
      "Configuration saved in ./feroutputs\\checkpoint-5285\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-5285\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-5285\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-5285\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-5285\\spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1462\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-10570\n",
      "Configuration saved in ./feroutputs\\checkpoint-10570\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-10570\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-10570\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-10570\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-10570\\spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1462\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-15855\n",
      "Configuration saved in ./feroutputs\\checkpoint-15855\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-15855\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-15855\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-15855\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-15855\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-5285] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1462\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-21140\n",
      "Configuration saved in ./feroutputs\\checkpoint-21140\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-21140\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-21140\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-21140\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-21140\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-10570] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1462\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-26425\n",
      "Configuration saved in ./feroutputs\\checkpoint-26425\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-26425\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-26425\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-26425\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-26425\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-15855] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1462\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-31710\n",
      "Configuration saved in ./feroutputs\\checkpoint-31710\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-31710\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-31710\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-31710\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-31710\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-21140] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1462\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-36995\n",
      "Configuration saved in ./feroutputs\\checkpoint-36995\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-36995\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-36995\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-36995\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-36995\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-26425] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1462\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-42280\n",
      "Configuration saved in ./feroutputs\\checkpoint-42280\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-42280\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-42280\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-42280\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-42280\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-31710] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1462\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-47565\n",
      "Configuration saved in ./feroutputs\\checkpoint-47565\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-47565\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-47565\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-47565\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-47565\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-36995] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1462\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-52850\n",
      "Configuration saved in ./feroutputs\\checkpoint-52850\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-52850\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-52850\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-52850\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-52850\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-42280] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=52850, training_loss=0.09799851548367214, metrics={'train_runtime': 39238.6391, 'train_samples_per_second': 1.347, 'train_steps_per_second': 1.347, 'total_flos': 3530612546764800.0, 'train_loss': 0.09799851548367214, 'epoch': 10.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1462\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1462' max='1462' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1462/1462 08:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.05081362649798393,\n",
       " 'eval_rouge1': 45.1627,\n",
       " 'eval_rouge2': 39.8096,\n",
       " 'eval_rougeL': 45.1197,\n",
       " 'eval_rougeLsum': 45.1751,\n",
       " 'eval_gen_len': 19.0,\n",
       " 'eval_runtime': 502.4792,\n",
       " 'eval_samples_per_second': 2.91,\n",
       " 'eval_steps_per_second': 2.91,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./feroutputs\n",
      "Configuration saved in ./feroutputs\\config.json\n",
      "Model weights saved in ./feroutputs\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\spiece.model\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('./feroutputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./feroutputs\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"./feroutputs\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file ./feroutputs\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ./feroutputs.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./feroutputs\")#, torch_dtype=torch.bfloat16)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"./feroutputs/checkpoint-2028\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAAP Gross Profit is $112051 T in fourth quarter 2022 @@@\n",
      "GAAP Gross Margin is 53.78% in fourth quarter 2022 @@@\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "Article: GAAP Gross Profit is $112051 T in fourth quarter 2022 @@@.\n",
      "\n",
      "Question: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.\n",
      "\n",
      "Article: GAAP Gross Margin is 53.78% in fourth quarter 2022 @@@.\n",
      "\n",
      "Question: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "{\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Profit,MONEY=$112051 T,CALENDAR=fourth quarter,YEAR=2022\"], \"RELATIONS\": [\"KEY:GAAP GROSS PROFIT!!TYPE:OUT!!MONEY:$112051 T!!LINK:KV\"]}\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "['\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Profit,MONEY=$12051 T,CALENDAR=fourth quarter,YEAR=2022\"], \"RELATIONS\": [\"KEY:GAAP GROSS PROFIT!!TYPE:OUT!!MONEY:$12051 T!!LINK:KV\"]']\n"
     ]
    }
   ],
   "source": [
    "index = 2\n",
    "dialogue = ds['test'][index]['input_text']\n",
    "human_baseline_summary = ds['test'][index]['target_text']\n",
    "#dialogue = \"Net Loss: Net loss was $118.9 million , or $1.74 per share with 65 million shares outstanding\"\n",
    "print(dialogue)\n",
    "index = 3\n",
    "dialogue2 = ds['test'][index]['input_text']\n",
    "print(dialogue2)\n",
    "#prompt = dialogue\n",
    "\n",
    "ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "\n",
    "ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "\n",
    "ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "\n",
    "tfewshot = f\"\"\"\n",
    "Article: {\" * \".join(sent)}\n",
    "\n",
    "Question: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format. {ans}\n",
    "\n",
    "\"\"\"\n",
    "tfewshot += f\"\"\"\n",
    "Article: {\" * \".join(sent1)}. \n",
    "\n",
    "Question: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format. {ans1}\n",
    "\n",
    "\"\"\"\n",
    "tfewshot += f\"\"\"\n",
    "Article: {\" * \".join(sent2)}.\n",
    "\n",
    "Question: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format. {ans2}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Article: {dialogue}.\n",
    "\n",
    "Question: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.\"\"\"\n",
    "\n",
    "prompt2 = f\"\"\"\n",
    "Article: {dialogue2}.\n",
    "\n",
    "Question: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.\"\"\"\n",
    "\n",
    "\n",
    "#inputs = tokenizer([prompt], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = instruct_model.generate(**inputs, max_new_tokens=300, top_p=.9)\n",
    "#outputs = instruct_model.generate(**inputs)\n",
    "instruct_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "#print(prompt)\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(prompt)\n",
    "print(prompt2)\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_output}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
