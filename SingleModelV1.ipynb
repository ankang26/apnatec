{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "import spacy\n",
    "import torch\n",
    "from spacy.language import Language\n",
    "from spacy import displacy\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "import statistics\n",
    "import os\n",
    "import json\n",
    "import calendar\n",
    "import holidays\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import hashlib\n",
    "from dateutil.parser import parse\n",
    "import shutil\n",
    "import ast\n",
    "from io import StringIO\n",
    "import requests\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy import displacy\n",
    "import time\n",
    "\n",
    "@Language.component(\"newsent\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        #print(token.text, token.text in (\"â€™s\", \"'s\"))\n",
    "        if token.text.upper() in (\";\", \"--\", \"\\n\\n\", \"\\n\", \"QUARTERLY\", \"STORY\", \"\\n\\n\\n\\n\", \"\\n\\n\\n\"):\n",
    "            #print(\"Detected:\", token.text)\n",
    "            doc[token.i].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "#spacy.require_gpu()\n",
    "nlp = spacy.load(\"../../Summary/NER/RelateEntity/train/model-best-local\")\n",
    "nlp.add_pipe('sentencizer')\n",
    "nlp.add_pipe('newsent', name=\"newsent\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "def query1_from_list(context):\n",
    "    ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "    sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "    \n",
    "    ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "    sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "    \n",
    "    ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "    sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "    \n",
    "    tfewshot = f\"\"\"\n",
    "    Question: What are the relations present in the following text? \n",
    "    \n",
    "    Context: {\" * \".join(sent)}. \n",
    "    \n",
    "    Answer: {ans}.\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Question: What are the relations present in the following text? \n",
    "     \n",
    "    Context: {\" * \".join(sent1)}. \n",
    "    \n",
    "    Answer: {ans1}.\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Question: What are the relations present in the following text?\n",
    "    \n",
    "    Context: {\" * \".join(sent2)}. \n",
    "    \n",
    "    Answer: {ans2}\n",
    "    \n",
    "    \"\"\"\n",
    "    #print(tfewshot)\n",
    "    #print(\"\\n\\n\")\n",
    "    t5query = f\"\"\"{tfewshot}\n",
    "    Question: What are the relations present in the following text? \n",
    "    \n",
    "    Context:  {\" * \".join(context)}.\n",
    "    \n",
    "    Answer:\n",
    "    \n",
    "    \"\"\"\n",
    "    print(t5query)\n",
    "    inputs = tokenizer(t5query, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "context = [\"GAAP Gross profit for the third quarter of 2022 was $210 million\"]\n",
    "result = query1_from_list(context)\n",
    "print(f\"{result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Article: GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\n",
      "    \n",
      "    Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\"]}\n",
      "    \n",
      "    \n",
      "    Article: Non-GAAP net income is $110.1 million in third quarter 2022 @@@. \n",
      "    \n",
      "    Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\"]}\n",
      "    \n",
      "    \n",
      "    Article: Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@.\n",
      "    \n",
      "    Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\"]}\n",
      "    \n",
      "    \n",
      "    Article: GAAP Gross profit for the third quarter of 2022 was $210 million.\n",
      "    \n",
      "    Question: What are the relations present in the text? Display it in json format.\n",
      "\"RELATIONS\": [\"KEY:GAAP Gross Profit\"]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "def query1_from_list(context):\n",
    "    ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "    sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "    \n",
    "    ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "    sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "    \n",
    "    ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "    sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "    \n",
    "    tfewshot = f\"\"\"\n",
    "    Article: {\" * \".join(sent)}\n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format. {ans}\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Article: {\" * \".join(sent1)}. \n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format. {ans1}\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Article: {\" * \".join(sent2)}.\n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format. {ans2}\n",
    "    \n",
    "    \"\"\"\n",
    "    #print(tfewshot)\n",
    "    #print(\"\\n\\n\")\n",
    "    t5query = f\"\"\"{tfewshot}\n",
    "    Article: {\" * \".join(context)}.\n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format.\"\"\"\n",
    "    print(t5query)\n",
    "    inputs = tokenizer(t5query, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "context = [\"GAAP Gross profit for the third quarter of 2022 was $210 million\"]\n",
    "result = query1_from_list(context)\n",
    "print(f\"{result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "devDataFile = \"../../Summary/DATA/FLAN/Dev/dev.tsv\"\n",
    "trainDataFile = \"../../Summary/DATA/FLAN/Train/train.tsv\"\n",
    "testDataFile = \"../../Summary/DATA/FLAN/Test/test.tsv\"\n",
    "\n",
    "trainDir = \"../../Summary/DATA/FLAN/Train\"\n",
    "devDir = \"../../Summary/DATA/FLAN/Dev\"\n",
    "testDir = \"../../Summary/DATA/FLAN/Test\"\n",
    "\n",
    "def writeTrainingData(writeFile, writeDir):\n",
    "    files = glob.glob(writeDir+\"/*_ER.tsv\")\n",
    "    print(files)\n",
    "    frames = list()\n",
    "\n",
    "    if(len(files) > 0):\n",
    "        for file in files:\n",
    "            print(file)\n",
    "            #df = pd.read_csv(file, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "            df = pd.read_csv(file, sep=\"\\t\", encoding = \"ISO-8859-1\").astype(str)\n",
    "            df = df.dropna()\n",
    "            df = df[df['Sentence1'].notna()]\n",
    "            #print(df)\n",
    "            frames.append(df)\n",
    "    result = pd.concat(frames)\n",
    "    print(result)\n",
    "    result.to_csv(writeFile, sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../Summary/DATA/FLAN/Train\\\\APPN_2023-02-16_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\APPN_2023-05-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\APPN_2023-08-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\BILL_2022-08-18_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\BILL_2023-02-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\BILL_2023-05-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\BILL_2023-08-17_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CFLT_2022-08-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CFLT_2022-11-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CFLT_2023-05-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CFLT_2023-08-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CRWD_2022-08-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CRWD_2022-11-29_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CRWD_2023-03-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CRWD_2023-08-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DDOG_2022-08-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DDOG_2022-11-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DDOG_2023-02-16_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DDOG_2023-05-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCN_2023-02-16_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCN_2023-05-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCN_2023-08-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCU_2022-09-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCU_2023-03-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCU_2023-06-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\FIVN_2022-07-28_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\FIVN_2022-11-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\FIVN_2023-05-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\FIVN_2023-08-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\HUBS_2022-08-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\HUBS_2022-11-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\HUBS_2023-02-16_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\HUBS_2023-08-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\MDB_2022-08-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\MDB_2022-12-06_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\MDB_2023-03-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\MDB_2023-06-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\NET_2022-11-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\NET_2023-02-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\NET_2023-04-27_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\NET_2023-08-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\OKTA_2022-08-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\OKTA_2023-03-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\OKTA_2023-05-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\OKTA_2023-08-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\OTHER_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PATH_2022-09-06_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PATH_2022-12-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PATH_2023-05-24_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PAYC_2022-08-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PAYC_2022-11-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PAYC_2023-05-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PAYC_2023-08-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PLTR_2022-08-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PLTR_2022-11-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PLTR_2023-02-13_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PLTR_2023-05-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\RNG_2022-11-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\RNG_2023-02-15_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\RNG_2023-05-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\RNG_2023-08-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\SNOW_2022-08-24_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\SNOW_2022-11-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\SNOW_2023-05-24_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\SNOW_2023-08-23_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\S_2022-08-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\S_2023-03-14_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\S_2023-06-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\S_2023-08-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TTD_2022-08-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TTD_2022-11-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TTD_2023-02-15_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TTD_2023-08-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TWLO_2022-08-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TWLO_2022-11-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TWLO_2023-02-15_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TWLO_2023-05-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\UPST_2022-11-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\UPST_2023-02-14_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\UPST_2023-05-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\UPST_2023-08-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZI_2022-08-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZI_2023-02-06_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZI_2023-05-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZI_2023-07-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZM_2022-08-22_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZM_2022-11-21_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZM_2023-05-22_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZM_2023-08-21_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZS_2022-09-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZS_2022-12-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZS_2023-06-01_EP_YH_ER.tsv']\n",
      "../../Summary/DATA/FLAN/Train\\APPN_2023-02-16_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\APPN_2023-05-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\APPN_2023-08-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\BILL_2022-08-18_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\BILL_2023-02-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\BILL_2023-05-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\BILL_2023-08-17_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CFLT_2022-08-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CFLT_2022-11-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CFLT_2023-05-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CFLT_2023-08-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CRWD_2022-08-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CRWD_2022-11-29_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CRWD_2023-03-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CRWD_2023-08-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DDOG_2022-08-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DDOG_2022-11-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DDOG_2023-02-16_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DDOG_2023-05-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCN_2023-02-16_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCN_2023-05-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCN_2023-08-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCU_2022-09-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCU_2023-03-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCU_2023-06-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\FIVN_2022-07-28_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\FIVN_2022-11-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\FIVN_2023-05-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\FIVN_2023-08-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\HUBS_2022-08-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\HUBS_2022-11-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\HUBS_2023-02-16_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\HUBS_2023-08-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\MDB_2022-08-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\MDB_2022-12-06_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\MDB_2023-03-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\MDB_2023-06-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\NET_2022-11-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\NET_2023-02-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\NET_2023-04-27_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\NET_2023-08-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\OKTA_2022-08-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\OKTA_2023-03-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\OKTA_2023-05-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\OKTA_2023-08-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\OTHER_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PATH_2022-09-06_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PATH_2022-12-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PATH_2023-05-24_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PAYC_2022-08-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PAYC_2022-11-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PAYC_2023-05-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PAYC_2023-08-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PLTR_2022-08-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PLTR_2022-11-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PLTR_2023-02-13_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PLTR_2023-05-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\RNG_2022-11-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\RNG_2023-02-15_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\RNG_2023-05-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\RNG_2023-08-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\SNOW_2022-08-24_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\SNOW_2022-11-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\SNOW_2023-05-24_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\SNOW_2023-08-23_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\S_2022-08-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\S_2023-03-14_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\S_2023-06-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\S_2023-08-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TTD_2022-08-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TTD_2022-11-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TTD_2023-02-15_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TTD_2023-08-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TWLO_2022-08-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TWLO_2022-11-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TWLO_2023-02-15_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TWLO_2023-05-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\UPST_2022-11-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\UPST_2023-02-14_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\UPST_2023-05-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\UPST_2023-08-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZI_2022-08-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZI_2023-02-06_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZI_2023-05-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZI_2023-07-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZM_2022-08-22_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZM_2022-11-21_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZM_2023-05-22_EP_YH_ER.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Summary/DATA/FLAN/Train\\ZM_2023-08-21_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZS_2022-09-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZS_2022-12-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZS_2023-06-01_EP_YH_ER.tsv\n",
      "                 filename                                          Sentence1  \\\n",
      "0   APPN_2023-02-16_EP_YH  GAAP Net Loss Per Share is $(0.47) in fourth q...   \n",
      "1   APPN_2023-02-16_EP_YH  Cash And Cash Equivalents is $148132 T in four...   \n",
      "2   APPN_2023-02-16_EP_YH  GAAP Gross Profit is $90555 T in fourth quarte...   \n",
      "3   APPN_2023-02-16_EP_YH  GAAP Gross Margin is 71.99% in fourth quarter ...   \n",
      "4   APPN_2023-02-16_EP_YH                                              PG***   \n",
      "..                    ...                                                ...   \n",
      "61    ZS_2023-06-01_EP_YH  We have not reconciled our expectations to non...   \n",
      "62    ZS_2023-06-01_EP_YH  For those reasons, we are also unable to addre...   \n",
      "63    ZS_2023-06-01_EP_YH  Accordingly, a reconciliation for the guidance...   \n",
      "64    ZS_2023-06-01_EP_YH  PG*** In the third quarter of fiscal 2023, we ...   \n",
      "65    ZS_2023-06-01_EP_YH  PG*** For further information regarding why we...   \n",
      "\n",
      "                                            Sentence2  \n",
      "0   {\"RELATIONS\": [\"KEY:GAAP Net Income Per Share!...  \n",
      "1   {\"RELATIONS\": [\"KEY:Cash And Cash Equivalents!...  \n",
      "2   {\"RELATIONS\": [\"KEY:GAAP GROSS PROFIT!!TYPE:OU...  \n",
      "3   {\"RELATIONS\": [\"KEY:GAAP GROSS MARGIN!!TYPE:OU...  \n",
      "4                                 {\"RELATIONS\": [\"\"]}  \n",
      "..                                                ...  \n",
      "61                                {\"RELATIONS\": [\"\"]}  \n",
      "62                                {\"RELATIONS\": [\"\"]}  \n",
      "63                                {\"RELATIONS\": [\"\"]}  \n",
      "64                                {\"RELATIONS\": [\"\"]}  \n",
      "65                                {\"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[5283 rows x 3 columns]\n",
      "['../../Summary/DATA/FLAN/Dev\\\\APPN_2022-11-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\BILL_2022-11-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\CFLT_2023-01-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\CRWD_2023-05-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\DDOG_2023-08-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\DOCN_2022-11-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\DOCU_2022-12-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\HUBS_2023-05-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\MDB_2023-08-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\NET_2022-08-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\OKTA_2022-11-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\OTHER_DEV_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\PATH_2023-03-15_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\PAYC_2023-02-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\PLTR_2023-08-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\RNG_2022-08-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\SNOW_2023-03-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\TTD_2023-05-10_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\TWLO_2023-08-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\UPST_2022-08-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\ZI_2022-11-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\ZM_2023-02-28_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\ZS_2023-03-03_EP_YH_ER.tsv']\n",
      "../../Summary/DATA/FLAN/Dev\\APPN_2022-11-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\BILL_2022-11-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\CFLT_2023-01-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\CRWD_2023-05-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\DDOG_2023-08-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\DOCN_2022-11-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\DOCU_2022-12-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\HUBS_2023-05-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\MDB_2023-08-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\NET_2022-08-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\OKTA_2022-11-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\OTHER_DEV_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\PATH_2023-03-15_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\PAYC_2023-02-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\PLTR_2023-08-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\RNG_2022-08-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\SNOW_2023-03-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\TTD_2023-05-10_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\TWLO_2023-08-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\UPST_2022-08-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\ZI_2022-11-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\ZM_2023-02-28_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\ZS_2023-03-03_EP_YH_ER.tsv\n",
      "                 filename                                          Sentence1  \\\n",
      "0   APPN_2022-11-03_EP_YH  GAAP Net Loss Per Share is $(0.61) in third qu...   \n",
      "1   APPN_2022-11-03_EP_YH  Cash And Cash Equivalents is $51802 T in third...   \n",
      "2   APPN_2022-11-03_EP_YH  GAAP Gross Profit is $84116 T in third quarter...   \n",
      "3   APPN_2022-11-03_EP_YH  GAAP Gross Margin is 71.36% in third quarter 2...   \n",
      "4   APPN_2022-11-03_EP_YH                                              PG***   \n",
      "..                    ...                                                ...   \n",
      "63    ZS_2023-03-03_EP_YH  Accordingly, we are required to add back the n...   \n",
      "64    ZS_2023-03-03_EP_YH  Additionally, we include the anti-dilutive imp...   \n",
      "65    ZS_2023-03-03_EP_YH  We have not reconciled our expectations to non...   \n",
      "66    ZS_2023-03-03_EP_YH  For those reasons, we are also unable to addre...   \n",
      "67    ZS_2023-03-03_EP_YH  Accordingly, a reconciliation for the guidance...   \n",
      "\n",
      "                                            Sentence2  \n",
      "0   {\"RELATIONS\": [\"KEY:GAAP Net Income Per Share!...  \n",
      "1   {\"RELATIONS\": [\"KEY:Cash And Cash Equivalents!...  \n",
      "2   {\"RELATIONS\": [\"KEY:GAAP GROSS PROFIT!!TYPE:OU...  \n",
      "3   {\"RELATIONS\": [\"KEY:GAAP GROSS MARGIN!!TYPE:OU...  \n",
      "4                                 {\"RELATIONS\": [\"\"]}  \n",
      "..                                                ...  \n",
      "63                                {\"RELATIONS\": [\"\"]}  \n",
      "64                                {\"RELATIONS\": [\"\"]}  \n",
      "65                                {\"RELATIONS\": [\"\"]}  \n",
      "66                                {\"RELATIONS\": [\"\"]}  \n",
      "67                                {\"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[1464 rows x 3 columns]\n",
      "['../../Summary/DATA/FLAN/Test\\\\FIVN_2023-02-22_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Test\\\\S_2022-12-06_EP_YH_ER.tsv']\n",
      "../../Summary/DATA/FLAN/Test\\FIVN_2023-02-22_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Test\\S_2022-12-06_EP_YH_ER.tsv\n",
      "                 filename                                          Sentence1  \\\n",
      "0   FIVN_2023-02-22_EP_YH  GAAP Net Loss Per Share is $(0.19) in fourth q...   \n",
      "1   FIVN_2023-02-22_EP_YH  Cash And Cash Equivalents is $180520 T in four...   \n",
      "2   FIVN_2023-02-22_EP_YH  GAAP Gross Profit is $112051 T in fourth quart...   \n",
      "3   FIVN_2023-02-22_EP_YH  GAAP Gross Margin is 53.78% in fourth quarter ...   \n",
      "4   FIVN_2023-02-22_EP_YH  GAAP Free Cash Flow is $36.6MN in fourth quart...   \n",
      "..                    ...                                                ...   \n",
      "34     S_2022-12-06_EP_YH  PG*** These statements are forward-looking and...   \n",
      "35     S_2022-12-06_EP_YH  Refer to the below for information on the fact...   \n",
      "36     S_2022-12-06_EP_YH  PG*** Guidance for non-GAAP financial measures...   \n",
      "37     S_2022-12-06_EP_YH  We have not provided the most directly compara...   \n",
      "38     S_2022-12-06_EP_YH  Accordingly, a reconciliation of non-GAAP gros...   \n",
      "\n",
      "                                            Sentence2  \n",
      "0   {\"RELATIONS\": [\"KEY:GAAP net income per share!...  \n",
      "1   {\"RELATIONS\": [\"KEY:CASH AND EQUIVALENTS!!TYPE...  \n",
      "2   {\"RELATIONS\": [\"KEY:GAAP GROSS PROFIT!!TYPE:OU...  \n",
      "3   {\"RELATIONS\": [\"KEY:GAAP GROSS MARGIN!!TYPE:OU...  \n",
      "4   {\"RELATIONS\": [\"KEY:GAAP FREE CASH FLOW!!TYPE:...  \n",
      "..                                                ...  \n",
      "34                                {\"RELATIONS\": [\"\"]}  \n",
      "35                                {\"RELATIONS\": [\"\"]}  \n",
      "36                                {\"RELATIONS\": [\"\"]}  \n",
      "37                                {\"RELATIONS\": [\"\"]}  \n",
      "38                                {\"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[82 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "writeTrainingData(trainDataFile, trainDir)\n",
    "writeTrainingData(devDataFile, devDir)\n",
    "writeTrainingData(testDataFile, testDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATA ..............\n",
      "                                             input_text  \\\n",
      "0     GAAP Net Loss Per Share is $(0.47) in fourth q...   \n",
      "1     Cash And Cash Equivalents is $148132 T in four...   \n",
      "2     GAAP Gross Profit is $90555 T in fourth quarte...   \n",
      "3     GAAP Gross Margin is 71.99% in fourth quarter ...   \n",
      "4                                                 PG***   \n",
      "...                                                 ...   \n",
      "5278  We have not reconciled our expectations to non...   \n",
      "5279  For those reasons, we are also unable to addre...   \n",
      "5280  Accordingly, a reconciliation for the guidance...   \n",
      "5281  PG*** In the third quarter of fiscal 2023, we ...   \n",
      "5282  PG*** For further information regarding why we...   \n",
      "\n",
      "                                            target_text  \n",
      "0     {\"RELATIONS\": [\"KEY:GAAP Net Income Per Share!...  \n",
      "1     {\"RELATIONS\": [\"KEY:Cash And Cash Equivalents!...  \n",
      "2     {\"RELATIONS\": [\"KEY:GAAP GROSS PROFIT!!TYPE:OU...  \n",
      "3     {\"RELATIONS\": [\"KEY:GAAP GROSS MARGIN!!TYPE:OU...  \n",
      "4                                   {\"RELATIONS\": [\"\"]}  \n",
      "...                                                 ...  \n",
      "5278                                {\"RELATIONS\": [\"\"]}  \n",
      "5279                                {\"RELATIONS\": [\"\"]}  \n",
      "5280                                {\"RELATIONS\": [\"\"]}  \n",
      "5281                                {\"RELATIONS\": [\"\"]}  \n",
      "5282                                {\"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[5283 rows x 2 columns]\n",
      "EVAL DATA ..............\n",
      "                                             input_text  \\\n",
      "0     GAAP Net Loss Per Share is $(0.61) in third qu...   \n",
      "1     Cash And Cash Equivalents is $51802 T in third...   \n",
      "2     GAAP Gross Profit is $84116 T in third quarter...   \n",
      "3     GAAP Gross Margin is 71.36% in third quarter 2...   \n",
      "4                                                 PG***   \n",
      "...                                                 ...   \n",
      "1459  Accordingly, we are required to add back the n...   \n",
      "1460  Additionally, we include the anti-dilutive imp...   \n",
      "1461  We have not reconciled our expectations to non...   \n",
      "1462  For those reasons, we are also unable to addre...   \n",
      "1463  Accordingly, a reconciliation for the guidance...   \n",
      "\n",
      "                                            target_text  \n",
      "0     {\"RELATIONS\": [\"KEY:GAAP Net Income Per Share!...  \n",
      "1     {\"RELATIONS\": [\"KEY:Cash And Cash Equivalents!...  \n",
      "2     {\"RELATIONS\": [\"KEY:GAAP GROSS PROFIT!!TYPE:OU...  \n",
      "3     {\"RELATIONS\": [\"KEY:GAAP GROSS MARGIN!!TYPE:OU...  \n",
      "4                                   {\"RELATIONS\": [\"\"]}  \n",
      "...                                                 ...  \n",
      "1459                                {\"RELATIONS\": [\"\"]}  \n",
      "1460                                {\"RELATIONS\": [\"\"]}  \n",
      "1461                                {\"RELATIONS\": [\"\"]}  \n",
      "1462                                {\"RELATIONS\": [\"\"]}  \n",
      "1463                                {\"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[1464 rows x 2 columns]\n",
      "TEST DATA ..............\n",
      "                                           input_text  \\\n",
      "0   GAAP Net Loss Per Share is $(0.19) in fourth q...   \n",
      "1   Cash And Cash Equivalents is $180520 T in four...   \n",
      "2   GAAP Gross Profit is $112051 T in fourth quart...   \n",
      "3   GAAP Gross Margin is 53.78% in fourth quarter ...   \n",
      "4   GAAP Free Cash Flow is $36.6MN in fourth quart...   \n",
      "..                                                ...   \n",
      "77  PG*** These statements are forward-looking and...   \n",
      "78  Refer to the below for information on the fact...   \n",
      "79  PG*** Guidance for non-GAAP financial measures...   \n",
      "80  We have not provided the most directly compara...   \n",
      "81  Accordingly, a reconciliation of non-GAAP gros...   \n",
      "\n",
      "                                          target_text  \n",
      "0   {\"RELATIONS\": [\"KEY:GAAP net income per share!...  \n",
      "1   {\"RELATIONS\": [\"KEY:CASH AND EQUIVALENTS!!TYPE...  \n",
      "2   {\"RELATIONS\": [\"KEY:GAAP GROSS PROFIT!!TYPE:OU...  \n",
      "3   {\"RELATIONS\": [\"KEY:GAAP GROSS MARGIN!!TYPE:OU...  \n",
      "4   {\"RELATIONS\": [\"KEY:GAAP FREE CASH FLOW!!TYPE:...  \n",
      "..                                                ...  \n",
      "77                                {\"RELATIONS\": [\"\"]}  \n",
      "78                                {\"RELATIONS\": [\"\"]}  \n",
      "79                                {\"RELATIONS\": [\"\"]}  \n",
      "80                                {\"RELATIONS\": [\"\"]}  \n",
      "81                                {\"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[82 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(trainDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "eval_df = pd.read_csv(devDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "test_df = pd.read_csv(testDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "\n",
    "train_df = train_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "eval_df = eval_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "test_df = test_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "\n",
    "train_df = train_df[[\"input_text\", \"target_text\"]]\n",
    "eval_df = eval_df[[\"input_text\", \"target_text\"]]\n",
    "test_df = test_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "#train_df[\"prefix\"] = \"paraphrase\"\n",
    "#train_df = train_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "#train_df = train_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "#eval_df[\"prefix\"] = \"paraphrase\"\n",
    "#eval_df = eval_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "#eval_df = eval_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "train_df = train_df[train_df['input_text'].notna()]\n",
    "\n",
    "eval_df = eval_df.dropna()\n",
    "eval_df = eval_df[eval_df['input_text'].notna()]\n",
    "\n",
    "test_df = test_df.dropna()\n",
    "test_df = test_df[test_df['input_text'].notna()]\n",
    "\n",
    "#train_df[\"input_text\"] = train_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "#train_df[\"target_text\"] = train_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "print(\"TRAIN DATA ..............\")\n",
    "print(train_df)\n",
    "\n",
    "#eval_df[\"input_text\"] = eval_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "#eval_df[\"target_text\"] = eval_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "print(\"EVAL DATA ..............\")\n",
    "print(eval_df)\n",
    "\n",
    "print(\"TEST DATA ..............\")\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-small'\n",
    "\n",
    "#original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model_name = \"facebook/bart-base\"\n",
    "\n",
    "#original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#original_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 76961152\n",
      "all model parameters: 76961152\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Article: GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\n",
      "\n",
      "Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\"]}\n",
      "\n",
      "\n",
      "Article: Non-GAAP net income is $110.1 million in third quarter 2022 @@@. \n",
      "\n",
      "Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\"]}\n",
      "\n",
      "\n",
      "Article: Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@.\n",
      "\n",
      "Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\"]}\n",
      "\n",
      "\n",
      "Article: GAAP Net Loss Per Share is $(0.47) in fourth quarter 2022 @@@.\n",
      "\n",
      "Question: What are the relations present in the text? Display it in json format.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "{\"RELATIONS\": [\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.47!!LINK:KV\"]}\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "['\"RELATIONS\": [\"KEY:GAAP Net Loss Per Share\": -$0.47 MN!!LINK:KV\"]']\n"
     ]
    }
   ],
   "source": [
    "inputText = (train_df[\"input_text\"][0])\n",
    "outputText = (train_df[\"target_text\"][0])\n",
    "\n",
    "ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "\n",
    "ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "\n",
    "ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "\n",
    "tfewshot = f\"\"\"\n",
    "Article: {\" * \".join(sent)}\n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format. {ans}\n",
    "\n",
    "\"\"\"\n",
    "tfewshot += f\"\"\"\n",
    "Article: {\" * \".join(sent1)}. \n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format. {ans1}\n",
    "\n",
    "\"\"\"\n",
    "tfewshot += f\"\"\"\n",
    "Article: {\" * \".join(sent2)}.\n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format. {ans2}\n",
    "\n",
    "\"\"\"\n",
    "#print(tfewshot)\n",
    "#print(\"\\n\\n\")\n",
    "t5query = f\"\"\"{tfewshot}\n",
    "Article: {inputText}.\n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format.\"\"\"\n",
    "#print(t5query)\n",
    "inputs = tokenizer(t5query, return_tensors=\"pt\")\n",
    "outputs = original_model.generate(**inputs, max_new_tokens=100)\n",
    "output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{t5query}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{outputText}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'target_text', '__index_level_0__'],\n",
      "        num_rows: 5283\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_text', 'target_text', '__index_level_0__'],\n",
      "        num_rows: 1464\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_text', 'target_text', '__index_level_0__'],\n",
      "        num_rows: 82\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train = Dataset.from_pandas(train_df)\n",
    "valid = Dataset.from_pandas(eval_df)\n",
    "test = Dataset.from_pandas(test_df)\n",
    "\n",
    "ds = DatasetDict()\n",
    "\n",
    "ds['train'] = train\n",
    "ds['validation'] = valid\n",
    "ds['test'] = test\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6747 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 177\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6747 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 116\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([ds[\"train\"], ds[\"validation\"]]).map(lambda x: tokenizer(x[\"input_text\"], truncation=True), batched=True, remove_columns=[\"input_text\", \"target_text\", \"__index_level_0__\"])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([ds[\"train\"], ds[\"validation\"]]).map(lambda x: tokenizer(x[\"target_text\"], truncation=True), batched=True, remove_columns=[\"input_text\", \"target_text\", \"__index_level_0__\"])\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5283 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1464 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/82 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    #print(example)\n",
    "    start_prompt = 'Article: '\n",
    "    end_prompt = '\\n\\nQuestion: What are the relations present in the text? Display it in json format.'\n",
    "    prompt = [start_prompt + sentence + end_prompt for sentence in example[\"input_text\"]]\n",
    "    #print(prompt)\n",
    "    example['input_ids'] = tokenizer(prompt, max_length=max_source_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"target_text\"], max_length=max_target_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    #print(example)\n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['input_text', 'target_text', '__index_level_0__',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5283 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1464 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/82 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def newtokenize_function(example, padding=\"max_length\"):\n",
    "    #print(example)\n",
    "    start_prompt = 'Article: '\n",
    "    end_prompt = '\\n\\nQuestion: What are the relations present in the text? Display it in json format.'\n",
    "    prompt = [start_prompt + sentence + end_prompt for sentence in example[\"input_text\"]]\n",
    "    #print(prompt)\n",
    "    model_inputs = tokenizer(prompt, max_length=max_source_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(text=example[\"target_text\"], max_length=max_target_length, padding=\"max_length\", truncation=True)\n",
    "    #print(example)\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = ds.map(newtokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['input_text', 'target_text', '__index_level_0__',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
    "#tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (5283, 3)\n",
      "Validation: (1464, 3)\n",
      "Test: (82, 3)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5283\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1464\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 82\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=original_model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "#data_collator = DataCollatorForSeq2Seq(tokenizer, model=original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./feroutputs\"\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    overwrite_output_dir=True,\n",
    "    fp16=False,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 5283\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 26415\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26415' max='26415' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26415/26415 4:01:06, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.097983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.062212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.079600</td>\n",
       "      <td>0.049556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.071400</td>\n",
       "      <td>0.045215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>0.044027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1464\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-5283\n",
      "Configuration saved in ./feroutputs\\checkpoint-5283\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-5283\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-5283\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-5283\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-5283\\spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1464\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-10566\n",
      "Configuration saved in ./feroutputs\\checkpoint-10566\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-10566\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-10566\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-10566\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-10566\\spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1464\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-15849\n",
      "Configuration saved in ./feroutputs\\checkpoint-15849\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-15849\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-15849\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-15849\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-15849\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-5283] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1464\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-21132\n",
      "Configuration saved in ./feroutputs\\checkpoint-21132\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-21132\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-21132\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-21132\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-21132\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-10566] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1464\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-26415\n",
      "Configuration saved in ./feroutputs\\checkpoint-26415\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-26415\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-26415\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-26415\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-26415\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-15849] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=26415, training_loss=0.14368619476555197, metrics={'train_runtime': 14469.8502, 'train_samples_per_second': 1.826, 'train_steps_per_second': 1.826, 'total_flos': 1764638229381120.0, 'train_loss': 0.14368619476555197, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./feroutputs/checkpoint-26415\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"./feroutputs/checkpoint-26415\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file ./feroutputs/checkpoint-26415\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ./feroutputs/checkpoint-26415.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./feroutputs/checkpoint-26415\")#, torch_dtype=torch.bfloat16)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"./feroutputs/checkpoint-2028\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PG*** Adjusted gross margin was 62.3% for the fourth quarter of 2022, compared to 62.8% for the fourth quarter of 2021.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "Article: PG*** Adjusted gross margin was 62.3% for the fourth quarter of 2022, compared to 62.8% for the fourth quarter of 2021..\n",
      "\n",
      "Question: What are the relations present in the text? Display it in json format.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "{\"RELATIONS\": [\"KEY:NON-GAAP GROSS MARGIN!!TYPE:OUT!!PCT:62.3%!!LINK:KV\"]}\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "\"RELATIONS\": [\"KEY:NON-GAAP GROSS MARGIN!!TYPE:OUT!!PCT:62.3%!!LINK:KV\"]\n"
     ]
    }
   ],
   "source": [
    "index = 14\n",
    "dialogue = ds['test'][index]['input_text']\n",
    "human_baseline_summary = ds['test'][index]['target_text']\n",
    "print(dialogue)\n",
    "#prompt = dialogue\n",
    "\n",
    "ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "\n",
    "ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "\n",
    "ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "\n",
    "tfewshot = f\"\"\"\n",
    "Article: {\" * \".join(sent)}\n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format. {ans}\n",
    "\n",
    "\"\"\"\n",
    "tfewshot += f\"\"\"\n",
    "Article: {\" * \".join(sent1)}. \n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format. {ans1}\n",
    "\n",
    "\"\"\"\n",
    "tfewshot += f\"\"\"\n",
    "Article: {\" * \".join(sent2)}.\n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format. {ans2}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Article: {dialogue}.\n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format.\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = instruct_model.generate(**inputs, max_new_tokens=100)\n",
    "#outputs = instruct_model.generate(**inputs)\n",
    "instruct_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "#original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "#inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "#outputs = original_model.generate(**inputs, max_new_tokens=100)\n",
    "#original_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "#print(prompt)\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(prompt)\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "#print(dash_line)\n",
    "#print(f'ORIGINAL MODEL:\\n{original_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_output[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
