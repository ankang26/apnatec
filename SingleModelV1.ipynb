{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "import spacy\n",
    "import torch\n",
    "from spacy.language import Language\n",
    "from spacy import displacy\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "import statistics\n",
    "import os\n",
    "import json\n",
    "import calendar\n",
    "import holidays\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import hashlib\n",
    "from dateutil.parser import parse\n",
    "import shutil\n",
    "import ast\n",
    "from io import StringIO\n",
    "import requests\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy import displacy\n",
    "import time\n",
    "\n",
    "@Language.component(\"newsent\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        #print(token.text, token.text in (\"â€™s\", \"'s\"))\n",
    "        if token.text.upper() in (\";\", \"--\", \"\\n\\n\", \"\\n\", \"QUARTERLY\", \"STORY\", \"\\n\\n\\n\\n\", \"\\n\\n\\n\"):\n",
    "            #print(\"Detected:\", token.text)\n",
    "            doc[token.i].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "#spacy.require_gpu()\n",
    "nlp = spacy.load(\"../../Summary/NER/RelateEntity/train/model-best-local\")\n",
    "nlp.add_pipe('sentencizer')\n",
    "nlp.add_pipe('newsent', name=\"newsent\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first one            Barbecue Chicken\n",
      "the fish                 Smoked Salmon\n",
      "the chicken              Barbecue Chicken\n",
      "the smoke                Smoked Salmon\n",
      "bbq                      Barbecue Chicken\n",
      "salmon                   salmon\n",
      "roasted turkey           Barbecue Chicken\n",
      "dried halibut            Smoked Salmon\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "def query_from_list(query, options):\n",
    "    t5query = f\"\"\"Question: Select the item from this list which is \"{query}\". Context: * {\" * \".join(options)}\"\"\"\n",
    "    #print(t5query)\n",
    "    inputs = tokenizer(t5query, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "tests = [\"the first one\", \"the fish\", \"the chicken\", \"the smoke\", \"bbq\", \"salmon\", \"roasted turkey\", \"dried halibut\"]\n",
    "options = [\"Barbecue Chicken\", \"Smoked Salmon\"]\n",
    "for t in tests:\n",
    "    result = query_from_list(t, options)\n",
    "    print(f\"{t:<24} {result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Question: What are the relations present in the following text? \n",
      "    \n",
      "    Context: GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@. \n",
      "    \n",
      "    Answer: {\"RELATIONS\": [\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\"]}.\n",
      "    \n",
      "    \n",
      "    Question: What are the relations present in the following text? \n",
      "     \n",
      "    Context: Non-GAAP net income is $110.1 million in third quarter 2022 @@@. \n",
      "    \n",
      "    Answer: {\"RELATIONS\": [\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\"]}.\n",
      "    \n",
      "    \n",
      "    Question: What are the relations present in the following text?\n",
      "    \n",
      "    Context: Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@. \n",
      "    \n",
      "    Answer: {\"RELATIONS\": [\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\"]}\n",
      "    \n",
      "    \n",
      "    Question: What are the relations present in the following text? \n",
      "    \n",
      "    Context:  GAAP Gross profit for the third quarter of 2022 was $210 million.\n",
      "    \n",
      "    Answer:\n",
      "    \n",
      "    \n",
      "[\"RELATIONS\": [\"KEY:GAAP Gross profit!!TYPE:OUT!!MONEY:$210 million!!LINK:KV\"]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "def query1_from_list(context):\n",
    "    ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "    sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "    \n",
    "    ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "    sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "    \n",
    "    ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "    sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "    \n",
    "    tfewshot = f\"\"\"\n",
    "    Question: What are the relations present in the following text? \n",
    "    \n",
    "    Context: {\" * \".join(sent)}. \n",
    "    \n",
    "    Answer: {ans}.\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Question: What are the relations present in the following text? \n",
    "     \n",
    "    Context: {\" * \".join(sent1)}. \n",
    "    \n",
    "    Answer: {ans1}.\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Question: What are the relations present in the following text?\n",
    "    \n",
    "    Context: {\" * \".join(sent2)}. \n",
    "    \n",
    "    Answer: {ans2}\n",
    "    \n",
    "    \"\"\"\n",
    "    #print(tfewshot)\n",
    "    #print(\"\\n\\n\")\n",
    "    t5query = f\"\"\"{tfewshot}\n",
    "    Question: What are the relations present in the following text? \n",
    "    \n",
    "    Context:  {\" * \".join(context)}.\n",
    "    \n",
    "    Answer:\n",
    "    \n",
    "    \"\"\"\n",
    "    print(t5query)\n",
    "    inputs = tokenizer(t5query, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "context = [\"GAAP Gross profit for the third quarter of 2022 was $210 million\"]\n",
    "result = query1_from_list(context)\n",
    "print(f\"{result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Article: GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\n",
      "    \n",
      "    Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\"]}\n",
      "    \n",
      "    \n",
      "    Article: Non-GAAP net income is $110.1 million in third quarter 2022 @@@. \n",
      "    \n",
      "    Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\"]}\n",
      "    \n",
      "    \n",
      "    Article: Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@.\n",
      "    \n",
      "    Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\"]}\n",
      "    \n",
      "    \n",
      "    Article: GAAP Gross profit for the third quarter of 2022 was $210 million.\n",
      "    \n",
      "    Question: What are the relations present in the text? Display it in json format.\n",
      "\"RELATIONS\": [\"KEY:GAAP Gross profit!!TYPE:OUT!!MONEY:$210 million!!LINK:KV\"]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "def query1_from_list(context):\n",
    "    ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "    sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "    \n",
    "    ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "    sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "    \n",
    "    ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "    sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "    \n",
    "    tfewshot = f\"\"\"\n",
    "    Article: {\" * \".join(sent)}\n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format. {ans}\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Article: {\" * \".join(sent1)}. \n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format. {ans1}\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Article: {\" * \".join(sent2)}.\n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format. {ans2}\n",
    "    \n",
    "    \"\"\"\n",
    "    #print(tfewshot)\n",
    "    #print(\"\\n\\n\")\n",
    "    t5query = f\"\"\"{tfewshot}\n",
    "    Article: {\" * \".join(context)}.\n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format.\"\"\"\n",
    "    print(t5query)\n",
    "    inputs = tokenizer(t5query, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "context = [\"GAAP Gross profit for the third quarter of 2022 was $210 million\"]\n",
    "result = query1_from_list(context)\n",
    "print(f\"{result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devDataFile = \"../../Summary/DATA/ENTITYRELATION/Dev/dev.tsv\"\n",
    "trainDataFile = \"../../Summary/DATA/ENTITYRELATION/Train/train.tsv\"\n",
    "testDataFile = \"../../Summary/DATA/ENTITYRELATION/Test/test.tsv\"\n",
    "\n",
    "trainDir = \"../../Summary/DATA/ENTITYRELATION/Train\"\n",
    "devDir = \"../../Summary/DATA/ENTITYRELATION/Dev\"\n",
    "testDir = \"../../Summary/DATA/ENTITYRELATION/Test\"\n",
    "\n",
    "def writeTrainingData(writeFile, writeDir):\n",
    "    files = glob.glob(writeDir+\"/*_ER.tsv\")\n",
    "    print(files)\n",
    "    frames = list()\n",
    "\n",
    "    if(len(files) > 0):\n",
    "        for file in files:\n",
    "            print(file)\n",
    "            #df = pd.read_csv(file, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "            df = pd.read_csv(file, sep=\"\\t\", encoding = \"ISO-8859-1\").astype(str)\n",
    "            df = df.dropna()\n",
    "            df = df[df['Sentence1'].notna()]\n",
    "            #print(df)\n",
    "            frames.append(df)\n",
    "    result = pd.concat(frames)\n",
    "    print(result)\n",
    "    result.to_csv(writeFile, sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeTrainingData(trainDataFile, trainDir)\n",
    "writeTrainingData(devDataFile, devDir)\n",
    "writeTrainingData(testDataFile, testDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(trainDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "eval_df = pd.read_csv(devDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "test_df = pd.read_csv(testDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "\n",
    "train_df = train_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "eval_df = eval_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "test_df = test_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "\n",
    "train_df = train_df[[\"input_text\", \"target_text\"]]\n",
    "eval_df = eval_df[[\"input_text\", \"target_text\"]]\n",
    "test_df = test_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "#train_df[\"prefix\"] = \"paraphrase\"\n",
    "#train_df = train_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "#train_df = train_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "#eval_df[\"prefix\"] = \"paraphrase\"\n",
    "#eval_df = eval_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "#eval_df = eval_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "train_df = train_df[train_df['input_text'].notna()]\n",
    "\n",
    "eval_df = eval_df.dropna()\n",
    "eval_df = eval_df[eval_df['input_text'].notna()]\n",
    "\n",
    "test_df = test_df.dropna()\n",
    "test_df = test_df[test_df['input_text'].notna()]\n",
    "\n",
    "#train_df[\"input_text\"] = train_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "#train_df[\"target_text\"] = train_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "print(\"TRAIN DATA ..............\")\n",
    "print(train_df)\n",
    "\n",
    "#eval_df[\"input_text\"] = eval_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "#eval_df[\"target_text\"] = eval_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "print(\"EVAL DATA ..............\")\n",
    "print(eval_df)\n",
    "\n",
    "print(\"TEST DATA ..............\")\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "#original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model_name = \"facebook/bart-base\"\n",
    "\n",
    "#original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#original_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = (train_df[\"input_text\"][0])\n",
    "outputText = (train_df[\"target_text\"][0])\n",
    "\n",
    "prompt = f\"\"\"\n",
    "List named entities from following sentence.\n",
    "\n",
    "{inputText}\n",
    "\n",
    "Entities and Relations:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{outputText}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train = Dataset.from_pandas(train_df)\n",
    "valid = Dataset.from_pandas(eval_df)\n",
    "test = Dataset.from_pandas(test_df)\n",
    "\n",
    "ds = DatasetDict()\n",
    "\n",
    "ds['train'] = train\n",
    "ds['validation'] = valid\n",
    "ds['test'] = test\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    #print(example)\n",
    "    start_prompt = 'Find entities and relations from following sentence.\\n\\n'\n",
    "    end_prompt = '\\n\\nEntities and Relations: '\n",
    "    prompt = [start_prompt + sentence + end_prompt for sentence in example[\"input_text\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"target_text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    #print(example)\n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['input_text', 'target_text', '__index_level_0__',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./feroutputs\"\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    #max_steps=2,\n",
    "    save_steps=1,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./feroutputs/checkpoint-101\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "dialogue = ds['test'][index]['input_text']\n",
    "human_baseline_summary = ds['test'][index]['target_text']\n",
    "print(dialogue)\n",
    "#prompt = dialogue\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Find entities and relations from following sentence.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Entities and Relations :\n",
    "\"\"\"\n",
    "\n",
    "#input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "#original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "#original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "instruct_model_text_output = tokenizer.decode(\n",
    "    instruct_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "#instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "#print(instruct_model_outputs)\n",
    "#instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "#print(dash_line)\n",
    "#print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
