{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "import spacy\n",
    "import torch\n",
    "from spacy.language import Language\n",
    "from spacy import displacy\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "import statistics\n",
    "import os\n",
    "import json\n",
    "import calendar\n",
    "import holidays\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import hashlib\n",
    "from dateutil.parser import parse\n",
    "import shutil\n",
    "import ast\n",
    "from io import StringIO\n",
    "import requests\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\spacy_transformers\\pipeline_component.py:406: UserWarning: Automatically converting a transformer component from spacy-transformers v1.0 to v1.1+. If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spacy-transformers version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.set_custom_boundaries(doc)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy import displacy\n",
    "import time\n",
    "\n",
    "@Language.component(\"newsent\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        #print(token.text, token.text in (\"â€™s\", \"'s\"))\n",
    "        if token.text.upper() in (\";\", \"--\", \"\\n\\n\", \"\\n\", \"QUARTERLY\", \"STORY\", \"\\n\\n\\n\\n\", \"\\n\\n\\n\"):\n",
    "            #print(\"Detected:\", token.text)\n",
    "            doc[token.i].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "#spacy.require_gpu()\n",
    "nlp = spacy.load(\"../../Summary/NER/RelateEntity/train/model-best-local\")\n",
    "nlp.add_pipe('sentencizer')\n",
    "nlp.add_pipe('newsent', name=\"newsent\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentences(inputfile, nlp, text=None):\n",
    "    if(not text):\n",
    "        with open(inputfile, 'r', encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = [str(sent).strip() for sent in doc.sents]\n",
    "\n",
    "    #print(len(sentences))\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "def query1_from_list(context):\n",
    "    ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "    sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "    \n",
    "    ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "    sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "    \n",
    "    ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "    sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "    \n",
    "    tfewshot = f\"\"\"\n",
    "    Question: What are the relations present in the following text? \n",
    "    \n",
    "    Context: {\" * \".join(sent)}. \n",
    "    \n",
    "    Answer: {ans}.\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Question: What are the relations present in the following text? \n",
    "     \n",
    "    Context: {\" * \".join(sent1)}. \n",
    "    \n",
    "    Answer: {ans1}.\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Question: What are the relations present in the following text?\n",
    "    \n",
    "    Context: {\" * \".join(sent2)}. \n",
    "    \n",
    "    Answer: {ans2}\n",
    "    \n",
    "    \"\"\"\n",
    "    #print(tfewshot)\n",
    "    #print(\"\\n\\n\")\n",
    "    t5query = f\"\"\"{tfewshot}\n",
    "    Question: What are the relations present in the following text? \n",
    "    \n",
    "    Context:  {\" * \".join(context)}.\n",
    "    \n",
    "    Answer:\n",
    "    \n",
    "    \"\"\"\n",
    "    print(t5query)\n",
    "    inputs = tokenizer(t5query, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "context = [\"GAAP Gross profit for the third quarter of 2022 was $210 million\"]\n",
    "result = query1_from_list(context)\n",
    "print(f\"{result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Article: GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\n",
      "    \n",
      "    Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\"]}\n",
      "    \n",
      "    \n",
      "    Article: Non-GAAP net income is $110.1 million in third quarter 2022 @@@. \n",
      "    \n",
      "    Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\"]}\n",
      "    \n",
      "    \n",
      "    Article: Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@.\n",
      "    \n",
      "    Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\"]}\n",
      "    \n",
      "    \n",
      "    Article: GAAP Gross profit for the third quarter of 2022 was $210 million.\n",
      "    \n",
      "    Question: What are the relations present in the text? Display it in json format.\n",
      "\"RELATIONS\": [\"KEY:GAAP Gross Profit\"]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "def query1_from_list(context):\n",
    "    ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "    sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "    \n",
    "    ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "    sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "    \n",
    "    ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "    sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "    \n",
    "    tfewshot = f\"\"\"\n",
    "    Article: {\" * \".join(sent)}\n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format. {ans}\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Article: {\" * \".join(sent1)}. \n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format. {ans1}\n",
    "    \n",
    "    \"\"\"\n",
    "    tfewshot += f\"\"\"\n",
    "    Article: {\" * \".join(sent2)}.\n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format. {ans2}\n",
    "    \n",
    "    \"\"\"\n",
    "    #print(tfewshot)\n",
    "    #print(\"\\n\\n\")\n",
    "    t5query = f\"\"\"{tfewshot}\n",
    "    Article: {\" * \".join(context)}.\n",
    "    \n",
    "    Question: What are the relations present in the text? Display it in json format.\"\"\"\n",
    "    print(t5query)\n",
    "    inputs = tokenizer(t5query, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "context = [\"GAAP Gross profit for the third quarter of 2022 was $210 million\"]\n",
    "result = query1_from_list(context)\n",
    "print(f\"{result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output File ../../Summary/DATA/FLAN/Train/APPN_2022-11-03_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/APPN_2023-02-16_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/APPN_2023-05-09_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/APPN_2023-08-03_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/BILL_2022-08-18_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/BILL_2022-11-03_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/BILL_2023-02-02_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/BILL_2023-05-04_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/BILL_2023-08-17_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/CFLT_2022-08-03_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/CFLT_2022-11-02_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/CFLT_2023-01-30_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/CFLT_2023-05-03_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/CFLT_2023-08-02_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/CRWD_2022-08-30_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/CRWD_2022-11-29_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/CRWD_2023-03-07_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/CRWD_2023-05-31_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/CRWD_2023-08-30_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DDOG_2022-08-04_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DDOG_2022-11-02_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DDOG_2023-02-16_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DDOG_2023-05-04_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DDOG_2023-08-08_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DOCN_2022-11-07_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DOCN_2023-02-16_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DOCN_2023-05-09_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DOCN_2023-08-03_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DOCU_2022-09-08_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DOCU_2022-12-08_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DOCU_2023-03-09_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/DOCU_2023-06-08_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/FIVN_2022-07-28_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/FIVN_2022-11-07_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/FIVN_2023-02-22_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/FIVN_2023-05-04_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/FIVN_2023-08-07_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/HUBS_2022-08-04_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/HUBS_2022-11-02_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/HUBS_2023-02-16_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/HUBS_2023-05-03_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/HUBS_2023-08-02_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/MDB_2022-08-31_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/MDB_2022-12-06_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/MDB_2023-03-08_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/MDB_2023-06-01_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/MDB_2023-08-31_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/NET_2022-08-04_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/NET_2022-11-03_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/NET_2023-02-09_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/NET_2023-04-27_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/NET_2023-08-03_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/OKTA_2022-08-31_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/OKTA_2022-11-30_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/OKTA_2023-03-02_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/OKTA_2023-05-31_EP_YH_ER.tsv already exists\n",
      "Output File ../../Summary/DATA/FLAN/Train/OKTA_2023-08-30_EP_YH_ER.tsv already exists\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/OTHER_DEV_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/OTHER_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PATH_2022-09-06_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PATH_2022-12-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PATH_2023-03-15_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PATH_2023-05-24_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PAYC_2022-08-02_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PAYC_2022-11-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PAYC_2023-02-07_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PAYC_2023-05-02_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PAYC_2023-08-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PLTR_2022-08-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PLTR_2022-11-07_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PLTR_2023-02-13_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PLTR_2023-05-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PLTR_2023-08-07_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/RNG_2022-08-02_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/RNG_2022-11-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/RNG_2023-02-15_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/RNG_2023-05-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/RNG_2023-08-07_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/SNOW_2022-08-24_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/SNOW_2022-11-30_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/SNOW_2023-03-02_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/SNOW_2023-05-24_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/SNOW_2023-08-23_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/S_2022-08-31_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/S_2022-12-06_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/S_2023-03-14_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/S_2023-06-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/S_2023-08-31_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TTD_2022-08-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TTD_2022-11-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TTD_2023-02-15_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TTD_2023-05-10_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TTD_2023-08-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TWLO_2022-08-04_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TWLO_2022-11-03_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TWLO_2023-02-15_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TWLO_2023-05-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TWLO_2023-08-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/UPST_2022-08-08_EP_YH_ER.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Output File ../../Summary/DATA/FLAN/Train/UPST_2022-11-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/UPST_2023-02-14_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/UPST_2023-05-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/UPST_2023-08-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZI_2022-08-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZI_2022-11-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZI_2023-02-06_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZI_2023-05-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZI_2023-07-31_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZM_2022-08-22_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZM_2022-11-21_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZM_2023-02-28_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZM_2023-05-22_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZM_2023-08-21_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZS_2022-09-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZS_2022-12-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZS_2023-03-03_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZS_2023-06-01_EP_YH_ER.tsv\n"
     ]
    }
   ],
   "source": [
    "# Format 2, includiing entities and relations\n",
    "maxFiles = 60\n",
    "fileCnt = 0\n",
    "source = \"../../Summary/DATA/FLAN/Backup/Format-1\"\n",
    "files = glob.glob(source+\"/*_ER.tsv\")\n",
    "#print(files)\n",
    "for file in files:\n",
    "    #print(file)\n",
    "    basefile = os.path.basename(file)\n",
    "    #print(basefile)\n",
    "    outfile = \"../../Summary/DATA/FLAN/Train/\"+basefile\n",
    "    of = None\n",
    "    #print(outfile)\n",
    "    if outfile:\n",
    "        outfilePath = Path(outfile)\n",
    "        if outfilePath.is_file():\n",
    "            print(\"Output File {} already exists\".format(outfile))\n",
    "            continue\n",
    "        \n",
    "        if(maxFiles > 0):\n",
    "            fileCnt = fileCnt + 1\n",
    "            if(fileCnt > maxFiles):\n",
    "                break\n",
    "            \n",
    "        print(\"Creating Output File {}\".format(outfile))\n",
    "        of = open(outfilePath, \"w\", encoding = \"utf-8\")\n",
    "        \n",
    "        \n",
    "    with open(file, \"r\", encoding = \"ISO-8859-1\") as f:\n",
    "        line = f.readline()\n",
    "        #print(line)\n",
    "        while line:\n",
    "            if (\"RELATIONS\" not in line):\n",
    "                of.write(line)\n",
    "                #of.write(\"\\n\")\n",
    "            else:\n",
    "                relations = dict()\n",
    "                nsplit = line.split(\"\\t\")\n",
    "                relation = nsplit[2]\n",
    "                relation = relation.replace(\"\\n\",\"\")\n",
    "                sent = nsplit[1]\n",
    "                sent = sent.replace(\"\\t\",\"\")\n",
    "                #print(sent)\n",
    "                relation = json.loads(relation)\n",
    "                #print(relation)\n",
    "                sentences = getSentences(None, nlp, sent)\n",
    "                nerl = None\n",
    "                for l in sentences:\n",
    "                    text1 = list()\n",
    "                    text1.append(l)\n",
    "                    for doc in nlp.pipe(text1, disable=[\"tagger\"]):\n",
    "                        for ent in doc.ents:\n",
    "                            if(not nerl):\n",
    "                                nerl = ent.label_.replace(\":\",\"\").replace(\",\",\"\")+\"=\"+ent.text.replace(\":\",\"\").replace(\",\",\"\")\n",
    "                            else:\n",
    "                                nerl = nerl + \",\" + ent.label_.replace(\":\",\"\").replace(\",\",\"\")+\"=\"+ent.text.replace(\":\",\"\").replace(\",\",\"\")\n",
    "                #relations[\"ENTITIES\"] = None\n",
    "                relations[\"ENTITIES\"] = list()\n",
    "                relations[\"ENTITIES\"].append(nerl)\n",
    "                #rlist = list()\n",
    "                #for rstr in relation[\"RELATIONS\"]:\n",
    "                #    rstr = rstr.replace(\",MONEY\",\"!!MONEY\").replace(\",TYPE\",\"!!TYPE\").replace(\",CD\",\"!!CD\").replace(\",RELATION\",\"!!RELATION\").replace(\",PCT\",\"!!PCT\").replace(\",DATE\",\"!!DATE\").replace(\",CALENDAR\",\"!!CALENDAR\")\n",
    "                #    rlist.append(rstr)\n",
    "                relations[\"RELATIONS\"] = relation[\"RELATIONS\"]\n",
    "                #print(relations)\n",
    "                nsplit[2] = json.dumps(relations)\n",
    "                nsplit[1] = sent\n",
    "                nline = \"\\t\".join(nsplit)\n",
    "                #nline = nline + \"</s>\" #Not Required\n",
    "                of.write(nline)\n",
    "                of.write(\"\\n\")\n",
    "            line = f.readline()\n",
    "    if(of):\n",
    "        of.close()\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output File ../../Summary/DATA/FLAN/Train/APPN_2022-11-03_EP_YH_ER.tsv already exists\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/APPN_2023-02-16_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/APPN_2023-05-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/APPN_2023-08-03_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/BILL_2022-08-18_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/BILL_2022-11-03_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/BILL_2023-02-02_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/BILL_2023-05-04_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/BILL_2023-08-17_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/CFLT_2022-08-03_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/CFLT_2022-11-02_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/CFLT_2023-01-30_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/CFLT_2023-05-03_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/CFLT_2023-08-02_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/CRWD_2022-08-30_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/CRWD_2022-11-29_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/CRWD_2023-03-07_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/CRWD_2023-05-31_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/CRWD_2023-08-30_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/DDOG_2022-08-04_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/DDOG_2022-11-02_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/DDOG_2023-02-16_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/DDOG_2023-05-04_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/DDOG_2023-08-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/DOCN_2022-11-07_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/DOCN_2023-02-16_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/DOCN_2023-05-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/DOCN_2023-08-03_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/DOCU_2022-09-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/DOCU_2022-12-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/DOCU_2023-03-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/DOCU_2023-06-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/FIVN_2022-07-28_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/FIVN_2022-11-07_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/FIVN_2023-02-22_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/FIVN_2023-05-04_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/FIVN_2023-08-07_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/HUBS_2022-08-04_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/HUBS_2022-11-02_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/HUBS_2023-02-16_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/HUBS_2023-05-03_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/HUBS_2023-08-02_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/MDB_2022-08-31_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/MDB_2022-12-06_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/MDB_2023-03-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/MDB_2023-06-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/MDB_2023-08-31_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/NET_2022-08-04_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/NET_2022-11-03_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/NET_2023-02-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/NET_2023-04-27_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/NET_2023-08-03_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/OKTA_2022-08-31_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/OKTA_2022-11-30_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/OKTA_2023-03-02_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/OKTA_2023-05-31_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/OKTA_2023-08-30_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/OTHER_DEV_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/OTHER_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PATH_2022-09-06_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PATH_2022-12-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PATH_2023-03-15_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PATH_2023-05-24_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PAYC_2022-08-02_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PAYC_2022-11-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PAYC_2023-02-07_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PAYC_2023-05-02_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PAYC_2023-08-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PLTR_2022-08-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PLTR_2022-11-07_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PLTR_2023-02-13_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PLTR_2023-05-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/PLTR_2023-08-07_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/RNG_2022-08-02_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/RNG_2022-11-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/RNG_2023-02-15_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/RNG_2023-05-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/RNG_2023-08-07_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/SNOW_2022-08-24_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/SNOW_2022-11-30_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/SNOW_2023-03-02_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/SNOW_2023-05-24_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/SNOW_2023-08-23_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/S_2022-08-31_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/S_2022-12-06_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/S_2023-03-14_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/S_2023-06-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/S_2023-08-31_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TTD_2022-08-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TTD_2022-11-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TTD_2023-02-15_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TTD_2023-05-10_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TTD_2023-08-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TWLO_2022-08-04_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TWLO_2022-11-03_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TWLO_2023-02-15_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TWLO_2023-05-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/TWLO_2023-08-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/UPST_2022-08-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/UPST_2022-11-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/UPST_2023-02-14_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/UPST_2023-05-09_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/UPST_2023-08-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZI_2022-08-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZI_2022-11-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZI_2023-02-06_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZI_2023-05-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZI_2023-07-31_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZM_2022-08-22_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZM_2022-11-21_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZM_2023-02-28_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZM_2023-05-22_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZM_2023-08-21_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZS_2022-09-08_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZS_2022-12-01_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZS_2023-03-03_EP_YH_ER.tsv\n",
      "Creating Output File ../../Summary/DATA/FLAN/Train/ZS_2023-06-01_EP_YH_ER.tsv\n"
     ]
    }
   ],
   "source": [
    "# Format 3, combining entities, relations and earning section in single training data\n",
    "maxFiles = -1\n",
    "fileCnt = 0\n",
    "source = \"../../Summary/DATA/FLAN/Backup/Format-2\"\n",
    "stag = [\"SCHQ***\", \"SCHF***\", \"SCBQ***\", \"SCBF***\", \"SCG***\"]\n",
    "gtag = [\"GF***\", \"GQ***\"]\n",
    "ptag = \"PG***\"\n",
    "\n",
    "files = glob.glob(source+\"/*_ER.tsv\")\n",
    "#print(files)\n",
    "for file in files:\n",
    "    #print(file)\n",
    "    basefile = os.path.basename(file)\n",
    "    #print(basefile)\n",
    "    outfile = \"../../Summary/DATA/FLAN/Train/\"+basefile\n",
    "    of = None\n",
    "    #print(outfile)\n",
    "    if outfile:\n",
    "        outfilePath = Path(outfile)\n",
    "        if outfilePath.is_file():\n",
    "            print(\"Output File {} already exists\".format(outfile))\n",
    "            continue\n",
    "        \n",
    "        if(maxFiles > 0):\n",
    "            fileCnt = fileCnt + 1\n",
    "            if(fileCnt > maxFiles):\n",
    "                break\n",
    "            \n",
    "        print(\"Creating Output File {}\".format(outfile))\n",
    "        of = open(outfilePath, \"w\", encoding = \"utf-8\")\n",
    "        \n",
    "    \n",
    "    tag = None\n",
    "    section = None\n",
    "    ssection = None\n",
    "    header = None\n",
    "    report = \"REGULAR\"\n",
    "    \n",
    "    with open(file, \"r\", encoding = \"ISO-8859-1\") as f:\n",
    "        line = f.readline()\n",
    "        #print(line)\n",
    "        while line:\n",
    "            if (\"RELATIONS\" not in line):\n",
    "                of.write(line)\n",
    "                #of.write(\"\\n\")\n",
    "            else:\n",
    "                relations = dict()\n",
    "                nsplit = line.split(\"\\t\")\n",
    "                relation = nsplit[2]\n",
    "                relation = relation.replace(\"\\n\",\"\")\n",
    "                sent = nsplit[1]\n",
    "                sent = sent.replace(\"\\t\",\"\")\n",
    "                tag = sent.split(\" \")[0].strip()\n",
    "                #print(tag)\n",
    "                #print(sent)\n",
    "                if(tag in stag):\n",
    "                    section = sent.replace(tag, \"\").strip()\n",
    "                    header = section\n",
    "                    if(\"Q\" in tag):\n",
    "                        report = \"REGULAR\"\n",
    "                    elif(\"F\" in tag):\n",
    "                        report = \"REGULARFULL\"\n",
    "                    elif(\"G\" in tag):\n",
    "                        report = \"GUIDE\"\n",
    "                    #print(sent)\n",
    "                elif(tag in gtag):\n",
    "                    ssection = sent.replace(tag, \"\").strip()\n",
    "                    header = ssection\n",
    "                    if(\"Q\" in tag):\n",
    "                        report = \"GUIDE\"\n",
    "                    elif(\"F\" in tag):\n",
    "                        report = \"GUIDEFULL\"\n",
    "                    #if(header):\n",
    "                    #    if(\"***\" in tag):\n",
    "                    #        sent = sent.replace(tag, \"\")\n",
    "                    #        sent = tag + \" \" + header + \" \" + sent\n",
    "                    #    else:\n",
    "                    #        sent = header + \" \" + sent\n",
    "                    #if(section):\n",
    "                    #    header = section + \" \" + ssection\n",
    "                    #else:\n",
    "                    #    header = ssection\n",
    "                    #print(sent)\n",
    "                else:\n",
    "                    if(header):\n",
    "                        if(\"***\" in tag):\n",
    "                            sent = sent.replace(tag, \"\")\n",
    "                            sent = tag + \" \" + header + \" \" + sent\n",
    "                        else:\n",
    "                            sent = header + \" \" + sent\n",
    "                    #print(sent)\n",
    "                relation = json.loads(relation)\n",
    "                #print(relation)\n",
    "                relations[\"ENTITIES\"] = relation[\"ENTITIES\"]\n",
    "                rltn = relation[\"RELATIONS\"]\n",
    "                if(len(rltn) != 0 and rltn[0] != ''):\n",
    "                    rlist = list()\n",
    "                    for rstr in rltn:\n",
    "                        rstr = rstr + \"!!SECTION:\"+ report\n",
    "                        rlist.append(rstr)\n",
    "                    relations[\"RELATIONS\"] = rlist\n",
    "                else:\n",
    "                    relations[\"RELATIONS\"] = rltn \n",
    "                #print(relations)\n",
    "                nsplit[2] = json.dumps(relations)\n",
    "                nsplit[1] = sent\n",
    "                nline = \"\\t\".join(nsplit)\n",
    "                #nline = nline + \"</s>\" #Not Required\n",
    "                of.write(nline)\n",
    "                of.write(\"\\n\")\n",
    "            line = f.readline()\n",
    "    if(of):\n",
    "        of.close()\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "devDataFile = \"../../Summary/DATA/FLAN/Dev/dev.tsv\"\n",
    "trainDataFile = \"../../Summary/DATA/FLAN/Train/train.tsv\"\n",
    "testDataFile = \"../../Summary/DATA/FLAN/Test/test.tsv\"\n",
    "\n",
    "trainDir = \"../../Summary/DATA/FLAN/Train\"\n",
    "devDir = \"../../Summary/DATA/FLAN/Dev\"\n",
    "testDir = \"../../Summary/DATA/FLAN/Test\"\n",
    "\n",
    "def writeTrainingData(writeFile, writeDir):\n",
    "    files = glob.glob(writeDir+\"/*_ER.tsv\")\n",
    "    print(files)\n",
    "    frames = list()\n",
    "\n",
    "    if(len(files) > 0):\n",
    "        for file in files:\n",
    "            print(file)\n",
    "            #df = pd.read_csv(file, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "            df = pd.read_csv(file, sep=\"\\t\", encoding = \"ISO-8859-1\").astype(str)\n",
    "            df = df.dropna()\n",
    "            df = df[df['Sentence1'].notna()]\n",
    "            #print(df)\n",
    "            frames.append(df)\n",
    "    result = pd.concat(frames)\n",
    "    print(result)\n",
    "    result.to_csv(writeFile, sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../Summary/DATA/FLAN/Train\\\\APPN_2023-02-16_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\APPN_2023-05-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\APPN_2023-08-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\BILL_2022-08-18_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\BILL_2023-02-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\BILL_2023-05-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\BILL_2023-08-17_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CFLT_2022-08-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CFLT_2022-11-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CFLT_2023-05-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CFLT_2023-08-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CRWD_2022-08-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CRWD_2022-11-29_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CRWD_2023-03-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\CRWD_2023-05-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DDOG_2022-08-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DDOG_2022-11-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DDOG_2023-02-16_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DDOG_2023-05-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCN_2023-02-16_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCN_2023-05-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCN_2023-08-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCU_2022-09-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCU_2023-03-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\DOCU_2023-06-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\FIVN_2022-07-28_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\FIVN_2022-11-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\FIVN_2023-05-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\FIVN_2023-08-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\HUBS_2022-08-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\HUBS_2022-11-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\HUBS_2023-02-16_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\HUBS_2023-08-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\MDB_2022-08-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\MDB_2022-12-06_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\MDB_2023-03-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\MDB_2023-06-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\NET_2022-11-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\NET_2023-02-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\NET_2023-04-27_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\NET_2023-08-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\OKTA_2022-08-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\OKTA_2023-03-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\OKTA_2023-05-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\OKTA_2023-08-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\OTHER_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PATH_2022-09-06_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PATH_2022-12-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PATH_2023-05-24_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PAYC_2022-08-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PAYC_2022-11-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PAYC_2023-02-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PAYC_2023-08-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PLTR_2022-08-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PLTR_2022-11-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PLTR_2023-02-13_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\PLTR_2023-05-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\RNG_2022-11-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\RNG_2023-02-15_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\RNG_2023-05-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\RNG_2023-08-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\SNOW_2022-08-24_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\SNOW_2022-11-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\SNOW_2023-05-24_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\SNOW_2023-08-23_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\S_2022-08-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\S_2023-03-14_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\S_2023-06-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\S_2023-08-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TTD_2022-08-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TTD_2022-11-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TTD_2023-02-15_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TTD_2023-08-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TWLO_2022-08-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TWLO_2022-11-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TWLO_2023-02-15_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\TWLO_2023-05-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\UPST_2022-11-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\UPST_2023-02-14_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\UPST_2023-05-09_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\UPST_2023-08-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZI_2022-08-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZI_2023-02-06_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZI_2023-05-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZI_2023-07-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZM_2022-08-22_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZM_2022-11-21_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZM_2023-05-22_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZM_2023-08-21_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZS_2022-09-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZS_2022-12-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Train\\\\ZS_2023-03-03_EP_YH_ER.tsv']\n",
      "../../Summary/DATA/FLAN/Train\\APPN_2023-02-16_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\APPN_2023-05-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\APPN_2023-08-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\BILL_2022-08-18_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\BILL_2023-02-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\BILL_2023-05-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\BILL_2023-08-17_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CFLT_2022-08-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CFLT_2022-11-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CFLT_2023-05-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CFLT_2023-08-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CRWD_2022-08-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CRWD_2022-11-29_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CRWD_2023-03-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\CRWD_2023-05-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DDOG_2022-08-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DDOG_2022-11-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DDOG_2023-02-16_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DDOG_2023-05-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCN_2023-02-16_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCN_2023-05-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCN_2023-08-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCU_2022-09-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCU_2023-03-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\DOCU_2023-06-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\FIVN_2022-07-28_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\FIVN_2022-11-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\FIVN_2023-05-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\FIVN_2023-08-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\HUBS_2022-08-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\HUBS_2022-11-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\HUBS_2023-02-16_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\HUBS_2023-08-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\MDB_2022-08-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\MDB_2022-12-06_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\MDB_2023-03-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\MDB_2023-06-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\NET_2022-11-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\NET_2023-02-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\NET_2023-04-27_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\NET_2023-08-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\OKTA_2022-08-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\OKTA_2023-03-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\OKTA_2023-05-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\OKTA_2023-08-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\OTHER_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PATH_2022-09-06_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PATH_2022-12-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PATH_2023-05-24_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PAYC_2022-08-02_EP_YH_ER.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Summary/DATA/FLAN/Train\\PAYC_2022-11-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PAYC_2023-02-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PAYC_2023-08-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PLTR_2022-08-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PLTR_2022-11-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PLTR_2023-02-13_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\PLTR_2023-05-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\RNG_2022-11-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\RNG_2023-02-15_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\RNG_2023-05-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\RNG_2023-08-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\SNOW_2022-08-24_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\SNOW_2022-11-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\SNOW_2023-05-24_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\SNOW_2023-08-23_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\S_2022-08-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\S_2023-03-14_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\S_2023-06-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\S_2023-08-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TTD_2022-08-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TTD_2022-11-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TTD_2023-02-15_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TTD_2023-08-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TWLO_2022-08-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TWLO_2022-11-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TWLO_2023-02-15_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\TWLO_2023-05-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\UPST_2022-11-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\UPST_2023-02-14_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\UPST_2023-05-09_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\UPST_2023-08-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZI_2022-08-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZI_2023-02-06_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZI_2023-05-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZI_2023-07-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZM_2022-08-22_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZM_2022-11-21_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZM_2023-05-22_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZM_2023-08-21_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZS_2022-09-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZS_2022-12-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Train\\ZS_2023-03-03_EP_YH_ER.tsv\n",
      "                 filename                                          Sentence1  \\\n",
      "0   APPN_2023-02-16_EP_YH  GAAP Net Loss Per Share is $(0.47) in fourth q...   \n",
      "1   APPN_2023-02-16_EP_YH  Cash And Cash Equivalents is $148132 T in four...   \n",
      "2   APPN_2023-02-16_EP_YH  GAAP Gross Profit is $90555 T in fourth quarte...   \n",
      "3   APPN_2023-02-16_EP_YH  GAAP Gross Margin is 71.99% in fourth quarter ...   \n",
      "4   APPN_2023-02-16_EP_YH                                              PG***   \n",
      "..                    ...                                                ...   \n",
      "63    ZS_2023-03-03_EP_YH  For the full year fiscal 2023, we expect:. Acc...   \n",
      "64    ZS_2023-03-03_EP_YH  For the full year fiscal 2023, we expect:. Add...   \n",
      "65    ZS_2023-03-03_EP_YH  For the full year fiscal 2023, we expect:. We ...   \n",
      "66    ZS_2023-03-03_EP_YH  For the full year fiscal 2023, we expect:. For...   \n",
      "67    ZS_2023-03-03_EP_YH  For the full year fiscal 2023, we expect:. Acc...   \n",
      "\n",
      "                                            Sentence2  \n",
      "0   {\"ENTITIES\": [\"METRIC=GAAP Net Loss Per Share,...  \n",
      "1   {\"ENTITIES\": [\"METRIC=Cash,METRIC=Cash Equival...  \n",
      "2   {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Profit...  \n",
      "3   {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Margin...  \n",
      "4             {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "..                                                ...  \n",
      "63  {\"ENTITIES\": [\"METRIC=non-GAAP,METRIC=net inco...  \n",
      "64            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "65  {\"ENTITIES\": [\"METRIC=non-GAAP,METRIC=income f...  \n",
      "66            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "67            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[5203 rows x 3 columns]\n",
      "['../../Summary/DATA/FLAN/Dev\\\\APPN_2022-11-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\BILL_2022-11-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\CFLT_2023-01-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\CRWD_2023-08-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\DDOG_2023-08-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\DOCN_2022-11-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\DOCU_2022-12-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\HUBS_2023-05-03_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\MDB_2023-08-31_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\NET_2022-08-04_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\OKTA_2022-11-30_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\OTHER_DEV_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\PATH_2023-03-15_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\PAYC_2023-05-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\PLTR_2023-08-07_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\RNG_2022-08-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\SNOW_2023-03-02_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\TTD_2023-05-10_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\TWLO_2023-08-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\UPST_2022-08-08_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\ZI_2022-11-01_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\ZM_2023-02-28_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Dev\\\\ZS_2023-06-01_EP_YH_ER.tsv']\n",
      "../../Summary/DATA/FLAN/Dev\\APPN_2022-11-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\BILL_2022-11-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\CFLT_2023-01-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\CRWD_2023-08-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\DDOG_2023-08-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\DOCN_2022-11-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\DOCU_2022-12-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\HUBS_2023-05-03_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\MDB_2023-08-31_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\NET_2022-08-04_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\OKTA_2022-11-30_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\OTHER_DEV_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\PATH_2023-03-15_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\PAYC_2023-05-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\PLTR_2023-08-07_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\RNG_2022-08-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\SNOW_2023-03-02_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\TTD_2023-05-10_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\TWLO_2023-08-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\UPST_2022-08-08_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\ZI_2022-11-01_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\ZM_2023-02-28_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Dev\\ZS_2023-06-01_EP_YH_ER.tsv\n",
      "                 filename                                          Sentence1  \\\n",
      "0   APPN_2022-11-03_EP_YH  GAAP Net Loss Per Share is $(0.61) in third qu...   \n",
      "1   APPN_2022-11-03_EP_YH  Cash And Cash Equivalents is $51802 T in third...   \n",
      "2   APPN_2022-11-03_EP_YH  GAAP Gross Profit is $84116 T in third quarter...   \n",
      "3   APPN_2022-11-03_EP_YH  GAAP Gross Margin is 71.36% in third quarter 2...   \n",
      "4   APPN_2022-11-03_EP_YH                                              PG***   \n",
      "..                    ...                                                ...   \n",
      "61    ZS_2023-06-01_EP_YH  For the full year fiscal 2023, we expect:. We ...   \n",
      "62    ZS_2023-06-01_EP_YH  For the full year fiscal 2023, we expect:. For...   \n",
      "63    ZS_2023-06-01_EP_YH  For the full year fiscal 2023, we expect:. Acc...   \n",
      "64    ZS_2023-06-01_EP_YH  PG*** For the full year fiscal 2023, we expect...   \n",
      "65    ZS_2023-06-01_EP_YH  PG*** For the full year fiscal 2023, we expect...   \n",
      "\n",
      "                                            Sentence2  \n",
      "0   {\"ENTITIES\": [\"METRIC=GAAP Net Loss Per Share,...  \n",
      "1   {\"ENTITIES\": [\"METRIC=Cash,METRIC=Cash Equival...  \n",
      "2   {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Profit...  \n",
      "3   {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Margin...  \n",
      "4             {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "..                                                ...  \n",
      "61  {\"ENTITIES\": [\"METRIC=non-GAAP,METRIC=income f...  \n",
      "62            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "63            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "64  {\"ENTITIES\": [\"CALENDAR=third quarter,METRIC=i...  \n",
      "65            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[1427 rows x 3 columns]\n",
      "['../../Summary/DATA/FLAN/Test\\\\FIVN_2023-02-22_EP_YH_ER.tsv', '../../Summary/DATA/FLAN/Test\\\\S_2022-12-06_EP_YH_ER.tsv']\n",
      "../../Summary/DATA/FLAN/Test\\FIVN_2023-02-22_EP_YH_ER.tsv\n",
      "../../Summary/DATA/FLAN/Test\\S_2022-12-06_EP_YH_ER.tsv\n",
      "                 filename                                          Sentence1  \\\n",
      "0   FIVN_2023-02-22_EP_YH  GAAP Net Loss Per Share is $(0.19) in fourth q...   \n",
      "1   FIVN_2023-02-22_EP_YH  Cash And Cash Equivalents is $180520 T in four...   \n",
      "2   FIVN_2023-02-22_EP_YH  GAAP Gross Profit is $112051 T in fourth quart...   \n",
      "3   FIVN_2023-02-22_EP_YH  GAAP Gross Margin is 53.78% in fourth quarter ...   \n",
      "4   FIVN_2023-02-22_EP_YH  GAAP Free Cash Flow is $36.6MN in fourth quart...   \n",
      "..                    ...                                                ...   \n",
      "34     S_2022-12-06_EP_YH  PG*** Financial Outlook.  These statements are...   \n",
      "35     S_2022-12-06_EP_YH  Financial Outlook. Refer to the below for info...   \n",
      "36     S_2022-12-06_EP_YH  PG*** Financial Outlook.  Guidance for non-GAA...   \n",
      "37     S_2022-12-06_EP_YH  Financial Outlook. We have not provided the mo...   \n",
      "38     S_2022-12-06_EP_YH  Financial Outlook. Accordingly, a reconciliati...   \n",
      "\n",
      "                                            Sentence2  \n",
      "0   {\"ENTITIES\": [\"METRIC=GAAP Net Loss Per Share,...  \n",
      "1   {\"ENTITIES\": [\"METRIC=Cash,METRIC=Cash Equival...  \n",
      "2   {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Profit...  \n",
      "3   {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Margin...  \n",
      "4   {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Free Cash Fl...  \n",
      "..                                                ...  \n",
      "34            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "35            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "36            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "37            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "38            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[82 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "writeTrainingData(trainDataFile, trainDir)\n",
    "writeTrainingData(devDataFile, devDir)\n",
    "writeTrainingData(testDataFile, testDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATA ..............\n",
      "                                             input_text  \\\n",
      "0     GAAP Net Loss Per Share is $(0.47) in fourth q...   \n",
      "1     Cash And Cash Equivalents is $148132 T in four...   \n",
      "2     GAAP Gross Profit is $90555 T in fourth quarte...   \n",
      "3     GAAP Gross Margin is 71.99% in fourth quarter ...   \n",
      "4                                                 PG***   \n",
      "...                                                 ...   \n",
      "5198  For the full year fiscal 2023, we expect:. Acc...   \n",
      "5199  For the full year fiscal 2023, we expect:. Add...   \n",
      "5200  For the full year fiscal 2023, we expect:. We ...   \n",
      "5201  For the full year fiscal 2023, we expect:. For...   \n",
      "5202  For the full year fiscal 2023, we expect:. Acc...   \n",
      "\n",
      "                                            target_text  \n",
      "0     {\"ENTITIES\": [\"METRIC=GAAP Net Loss Per Share,...  \n",
      "1     {\"ENTITIES\": [\"METRIC=Cash,METRIC=Cash Equival...  \n",
      "2     {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Profit...  \n",
      "3     {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Margin...  \n",
      "4               {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "...                                                 ...  \n",
      "5198  {\"ENTITIES\": [\"METRIC=non-GAAP,METRIC=net inco...  \n",
      "5199            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "5200  {\"ENTITIES\": [\"METRIC=non-GAAP,METRIC=income f...  \n",
      "5201            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "5202            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[5203 rows x 2 columns]\n",
      "EVAL DATA ..............\n",
      "                                             input_text  \\\n",
      "0     GAAP Net Loss Per Share is $(0.61) in third qu...   \n",
      "1     Cash And Cash Equivalents is $51802 T in third...   \n",
      "2     GAAP Gross Profit is $84116 T in third quarter...   \n",
      "3     GAAP Gross Margin is 71.36% in third quarter 2...   \n",
      "4                                                 PG***   \n",
      "...                                                 ...   \n",
      "1422  For the full year fiscal 2023, we expect:. We ...   \n",
      "1423  For the full year fiscal 2023, we expect:. For...   \n",
      "1424  For the full year fiscal 2023, we expect:. Acc...   \n",
      "1425  PG*** For the full year fiscal 2023, we expect...   \n",
      "1426  PG*** For the full year fiscal 2023, we expect...   \n",
      "\n",
      "                                            target_text  \n",
      "0     {\"ENTITIES\": [\"METRIC=GAAP Net Loss Per Share,...  \n",
      "1     {\"ENTITIES\": [\"METRIC=Cash,METRIC=Cash Equival...  \n",
      "2     {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Profit...  \n",
      "3     {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Margin...  \n",
      "4               {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "...                                                 ...  \n",
      "1422  {\"ENTITIES\": [\"METRIC=non-GAAP,METRIC=income f...  \n",
      "1423            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "1424            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "1425  {\"ENTITIES\": [\"CALENDAR=third quarter,METRIC=i...  \n",
      "1426            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[1427 rows x 2 columns]\n",
      "TEST DATA ..............\n",
      "                                           input_text  \\\n",
      "0   GAAP Net Loss Per Share is $(0.19) in fourth q...   \n",
      "1   Cash And Cash Equivalents is $180520 T in four...   \n",
      "2   GAAP Gross Profit is $112051 T in fourth quart...   \n",
      "3   GAAP Gross Margin is 53.78% in fourth quarter ...   \n",
      "4   GAAP Free Cash Flow is $36.6MN in fourth quart...   \n",
      "..                                                ...   \n",
      "77  PG*** Financial Outlook.  These statements are...   \n",
      "78  Financial Outlook. Refer to the below for info...   \n",
      "79  PG*** Financial Outlook.  Guidance for non-GAA...   \n",
      "80  Financial Outlook. We have not provided the mo...   \n",
      "81  Financial Outlook. Accordingly, a reconciliati...   \n",
      "\n",
      "                                          target_text  \n",
      "0   {\"ENTITIES\": [\"METRIC=GAAP Net Loss Per Share,...  \n",
      "1   {\"ENTITIES\": [\"METRIC=Cash,METRIC=Cash Equival...  \n",
      "2   {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Profit...  \n",
      "3   {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Gross Margin...  \n",
      "4   {\"ENTITIES\": [\"METRIC=GAAP,METRIC=Free Cash Fl...  \n",
      "..                                                ...  \n",
      "77            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "78            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "79            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "80            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "81            {\"ENTITIES\": [null], \"RELATIONS\": [\"\"]}  \n",
      "\n",
      "[82 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(trainDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "eval_df = pd.read_csv(devDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "test_df = pd.read_csv(testDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "\n",
    "train_df = train_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "eval_df = eval_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "test_df = test_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "\n",
    "train_df = train_df[[\"input_text\", \"target_text\"]]\n",
    "eval_df = eval_df[[\"input_text\", \"target_text\"]]\n",
    "test_df = test_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "#train_df[\"prefix\"] = \"paraphrase\"\n",
    "#train_df = train_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "#train_df = train_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "#eval_df[\"prefix\"] = \"paraphrase\"\n",
    "#eval_df = eval_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "#eval_df = eval_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "train_df = train_df[train_df['input_text'].notna()]\n",
    "\n",
    "eval_df = eval_df.dropna()\n",
    "eval_df = eval_df[eval_df['input_text'].notna()]\n",
    "\n",
    "test_df = test_df.dropna()\n",
    "test_df = test_df[test_df['input_text'].notna()]\n",
    "\n",
    "#train_df[\"input_text\"] = train_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "#train_df[\"target_text\"] = train_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "print(\"TRAIN DATA ..............\")\n",
    "print(train_df)\n",
    "\n",
    "#eval_df[\"input_text\"] = eval_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "#eval_df[\"target_text\"] = eval_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "print(\"EVAL DATA ..............\")\n",
    "print(eval_df)\n",
    "\n",
    "print(\"TEST DATA ..............\")\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./feroutputs/checkpoint-5203\n"
     ]
    }
   ],
   "source": [
    "#modelPath = \"./feroutputs/checkpoint-5203/pytorch_model.bin\"\n",
    "modelPath = \"./feroutputs/pytorch_model.bin\"\n",
    "modelDir = \"./feroutputs/\"\n",
    "model_def = 'google/flan-t5-small'\n",
    "if os.path.isfile(modelPath):\n",
    "    model_name = \"./feroutputs/checkpoint-5203\"\n",
    "else:\n",
    "    di = sorted(os.listdir(modelDir), reverse=True)\n",
    "    if(len(di) > 0):\n",
    "        model_name = modelDir+(di[0])\n",
    "    else:\n",
    "        model_name = model_def\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model_name = \"facebook/bart-base\"\n",
    "\n",
    "#original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#original_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 76961152\n",
      "all model parameters: 76961152\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Article: GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\n",
      "\n",
      "Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\"]}\n",
      "\n",
      "\n",
      "Article: Non-GAAP net income is $110.1 million in third quarter 2022 @@@. \n",
      "\n",
      "Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\"]}\n",
      "\n",
      "\n",
      "Article: Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@.\n",
      "\n",
      "Question: What are the relations present in the text? Display it in json format. {\"RELATIONS\": [\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\"]}\n",
      "\n",
      "\n",
      "Article: GAAP Net Loss Per Share is $(0.47) in fourth quarter 2022 @@@.\n",
      "\n",
      "Question: What are the relations present in the text? Display it in json format.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "{\"RELATIONS\": [\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.47!!LINK:KV\"]}\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "['\"RELATIONS\": [\"KEY:GAAP Net Loss Per Share\": -$0.47 MN!!LINK:KV\"]']\n"
     ]
    }
   ],
   "source": [
    "inputText = (train_df[\"input_text\"][0])\n",
    "outputText = (train_df[\"target_text\"][0])\n",
    "\n",
    "ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "\n",
    "ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "\n",
    "ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "\n",
    "tfewshot = f\"\"\"\n",
    "Article: {\" * \".join(sent)}\n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format. {ans}\n",
    "\n",
    "\"\"\"\n",
    "tfewshot += f\"\"\"\n",
    "Article: {\" * \".join(sent1)}. \n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format. {ans1}\n",
    "\n",
    "\"\"\"\n",
    "tfewshot += f\"\"\"\n",
    "Article: {\" * \".join(sent2)}.\n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format. {ans2}\n",
    "\n",
    "\"\"\"\n",
    "#print(tfewshot)\n",
    "#print(\"\\n\\n\")\n",
    "t5query = f\"\"\"{tfewshot}\n",
    "Article: {inputText}.\n",
    "\n",
    "Question: What are the relations present in the text? Display it in json format.\"\"\"\n",
    "#print(t5query)\n",
    "inputs = tokenizer(t5query, return_tensors=\"pt\")\n",
    "outputs = original_model.generate(**inputs, max_new_tokens=100)\n",
    "output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{t5query}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{outputText}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'target_text', '__index_level_0__'],\n",
      "        num_rows: 5203\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_text', 'target_text', '__index_level_0__'],\n",
      "        num_rows: 1427\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_text', 'target_text', '__index_level_0__'],\n",
      "        num_rows: 82\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train = Dataset.from_pandas(train_df)\n",
    "valid = Dataset.from_pandas(eval_df)\n",
    "test = Dataset.from_pandas(test_df)\n",
    "\n",
    "ds = DatasetDict()\n",
    "\n",
    "ds['train'] = train\n",
    "ds['validation'] = valid\n",
    "ds['test'] = test\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 180\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 251\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([ds[\"train\"], ds[\"validation\"]]).map(lambda x: tokenizer(x[\"input_text\"], truncation=True), batched=True, remove_columns=[\"input_text\", \"target_text\", \"__index_level_0__\"])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([ds[\"train\"], ds[\"validation\"]]).map(lambda x: tokenizer(x[\"target_text\"], truncation=True), batched=True, remove_columns=[\"input_text\", \"target_text\", \"__index_level_0__\"])\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5283 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1464 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/82 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    #print(example)\n",
    "    start_prompt = 'Article: '\n",
    "    end_prompt = '\\n\\nQuestion: What are the relations present in the text? Display it in json format.'\n",
    "    prompt = [start_prompt + sentence + end_prompt for sentence in example[\"input_text\"]]\n",
    "    #print(prompt)\n",
    "    example['input_ids'] = tokenizer(prompt, max_length=max_source_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"target_text\"], max_length=max_target_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    #print(example)\n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['input_text', 'target_text', '__index_level_0__',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5203 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/82 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def newtokenize_function(example, padding=\"max_length\"):\n",
    "    #print(example)\n",
    "    start_prompt = 'Article: '\n",
    "    #end_prompt = '\\n\\nQuestion: What are the relations present in the text? Display it in json format.'\n",
    "    end_prompt = '\\n\\nQuestion: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.'\n",
    "    prompt = [start_prompt + sentence + end_prompt for sentence in example[\"input_text\"]]\n",
    "    #print(prompt)\n",
    "    model_inputs = tokenizer(prompt, max_length=max_source_length, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(text=example[\"target_text\"], max_length=max_target_length, padding=\"max_length\", truncation=True)\n",
    "    #print(example)\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = ds.map(newtokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['input_text', 'target_text', '__index_level_0__',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
    "#tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (5203, 3)\n",
      "Validation: (1427, 3)\n",
      "Test: (82, 3)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5203\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1427\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 82\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ankan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=original_model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "#data_collator = DataCollatorForSeq2Seq(tokenizer, model=original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./feroutputs\"\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    overwrite_output_dir=True,\n",
    "    fp16=False,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 5203\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 52030\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6880' max='52030' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6880/52030 1:28:02 < 9:37:54, 1.30 it/s, Epoch 1.32/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.064398</td>\n",
       "      <td>43.884700</td>\n",
       "      <td>37.975100</td>\n",
       "      <td>43.826400</td>\n",
       "      <td>43.759100</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1427\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./feroutputs\\checkpoint-5203\n",
      "Configuration saved in ./feroutputs\\checkpoint-5203\\config.json\n",
      "Model weights saved in ./feroutputs\\checkpoint-5203\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\checkpoint-5203\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\checkpoint-5203\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\checkpoint-5203\\spiece.model\n",
      "Deleting older checkpoint [feroutputs\\checkpoint-10406] due to args.save_total_limit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1322\u001b[0m         )\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1552\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1554\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1556\u001b[0m                 if (\n",
      "\u001b[1;32mc:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2199\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2200\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2201\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2203\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1462\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1462' max='1462' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1462/1462 08:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.05081362649798393,\n",
       " 'eval_rouge1': 45.1627,\n",
       " 'eval_rouge2': 39.8096,\n",
       " 'eval_rougeL': 45.1197,\n",
       " 'eval_rougeLsum': 45.1751,\n",
       " 'eval_gen_len': 19.0,\n",
       " 'eval_runtime': 502.4792,\n",
       " 'eval_samples_per_second': 2.91,\n",
       " 'eval_steps_per_second': 2.91,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./feroutputs\n",
      "Configuration saved in ./feroutputs\\config.json\n",
      "Model weights saved in ./feroutputs\\pytorch_model.bin\n",
      "tokenizer config file saved in ./feroutputs\\tokenizer_config.json\n",
      "Special tokens file saved in ./feroutputs\\special_tokens_map.json\n",
      "Copy vocab file to ./feroutputs\\spiece.model\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('./feroutputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./feroutputs/checkpoint-5203\")#, torch_dtype=torch.bfloat16)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"./feroutputs/checkpoint-2028\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PG*** Fourth Quarter 2022 Financial Results.  GAAP net loss for the fourth quarter of 2022 was $(13.7) million, or $(0.19) per basic share, compared to GAAP net loss of $(3.6) million, or $(0.05) per basic share, for the fourth quarter of 2021.\n",
      "GAAP Gross Margin is 53.78% in fourth quarter 2022 @@@\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "Article: PG*** Fourth Quarter 2022 Financial Results.  GAAP net loss for the fourth quarter of 2022 was $(13.7) million, or $(0.19) per basic share, compared to GAAP net loss of $(3.6) million, or $(0.05) per basic share, for the fourth quarter of 2021..\n",
      "\n",
      "Question: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.\n",
      "\n",
      "Article: GAAP Gross Margin is 53.78% in fourth quarter 2022 @@@.\n",
      "\n",
      "Question: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "{\"ENTITIES\": [\"METRIC=GAAP,METRIC=net loss,CALENDAR=fourth quarter,YEAR=2022,MONEY=$(13.7) million,MONEY=$(0.19),METRIC=per basic share,METRIC=GAAP,METRIC=net loss,MONEY=$(3.6) million,MONEY=$(0.05),METRIC=per basic share,CALENDAR=fourth quarter,YEAR=2021\"], \"RELATIONS\": [\"KEY:GAAP NET INCOME!!TYPE:OUT!!MONEY:-$13.7 MN!!LINK:KV!!SECTION:REGULAR\", \"KEY:GAAP net income per basic share!!TYPE:OUT!!MONEY:-$0.19!!LINK:KV!!SECTION:REGULAR\"]}\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "['\"ENTITIES\": [\"METRIC=GAAP,METRIC=net loss,CALENDAR=fourth quarter,YEAR=2022,MONEY=$(13.7) million,MONEY=$(0.19),METRIC=per basic share,METRIC=GAAP,METRIC=net loss,MONEY=$(3.6) million,MONEY=$(0.05),METRIC=per basic share,CALENDAR=fourth quarter,YEAR=2021\"], \"RELATIONS\": [\"KEY:GAAP NET INCOME!!TYPE:OUT!!MONEY:-$13.7 MN!!LINK:KV!!SECTION:REGULAR\", \"KEY:GAAP net income per basic share!!TYPE:OUT!!MONEY:-$0.19!!LINK:KV!!SECTION:REGULAR\"]']\n"
     ]
    }
   ],
   "source": [
    "index = 15\n",
    "dialogue = ds['test'][index]['input_text']\n",
    "human_baseline_summary = ds['test'][index]['target_text']\n",
    "#dialogue = \"Net Loss: Net loss was $118.9 million , or $1.74 per share with 65 million shares outstanding\"\n",
    "print(dialogue)\n",
    "index = 3\n",
    "dialogue2 = ds['test'][index]['input_text']\n",
    "print(dialogue2)\n",
    "#prompt = dialogue\n",
    "\n",
    "ans = \"{\\\"RELATIONS\\\": [\\\"KEY:GAAP Net Income Per Share!!TYPE:OUT!!MONEY:$0.38!!LINK:KV\\\"]}\"\n",
    "sent = [\"GAAP Net Income Per Share is $0.38 in second quarter 2023 @@@\"]\n",
    "\n",
    "ans1 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP net income!!TYPE:OUT!!MONEY:$110.1 MN!!LINK:KV\\\"]}\"\n",
    "sent1 = [\"Non-GAAP net income is $110.1 million in third quarter 2022 @@@\"]\n",
    "\n",
    "ans2 = \"{\\\"RELATIONS\\\": [\\\"KEY:Non-GAAP Net Income Per Share!!TYPE:OUT!!MONEY:-$0.13!!LINK:KV\\\"]}\"\n",
    "sent2 = [\"Non-GAAP Net Income Per Share is ($0.13) in first quarter 2023 @@@\"]\n",
    "\n",
    "tfewshot = f\"\"\"\n",
    "Article: {\" * \".join(sent)}\n",
    "\n",
    "Question: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format. {ans}\n",
    "\n",
    "\"\"\"\n",
    "tfewshot += f\"\"\"\n",
    "Article: {\" * \".join(sent1)}. \n",
    "\n",
    "Question: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format. {ans1}\n",
    "\n",
    "\"\"\"\n",
    "tfewshot += f\"\"\"\n",
    "Article: {\" * \".join(sent2)}.\n",
    "\n",
    "Question: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format. {ans2}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Article: {dialogue}.\n",
    "\n",
    "Question: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.\"\"\"\n",
    "\n",
    "prompt2 = f\"\"\"\n",
    "Article: {dialogue2}.\n",
    "\n",
    "Question: What are the entities and relations between these entities present in the text? If entities are present then first generate entities and then list relations between entities in lists of lists format.\"\"\"\n",
    "\n",
    "\n",
    "#inputs = tokenizer([prompt], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = instruct_model.generate(**inputs, max_new_tokens=300, top_p=.9)\n",
    "#outputs = instruct_model.generate(**inputs)\n",
    "instruct_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "#print(prompt)\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(prompt)\n",
    "print(prompt2)\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_output}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
