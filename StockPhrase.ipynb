{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\spacy_transformers\\pipeline_component.py:406: UserWarning: Automatically converting a transformer component from spacy-transformers v1.0 to v1.1+. If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spacy-transformers version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.set_custom_boundaries(doc)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy import displacy\n",
    "import time\n",
    "\n",
    "@Language.component(\"newsent\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        #print(token.text, token.text in (\"’s\", \"'s\"))\n",
    "        if token.text.upper() in (\";\", \"--\", \"\\n\\n\", \"\\n\", \"QUARTERLY\", \"STORY\", \"\\n\\n\\n\\n\", \"\\n\\n\\n\"):\n",
    "            print(\"Detected:\", token.text)\n",
    "            doc[token.i].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "#spacy.require_gpu()\n",
    "nlp = spacy.load(\"../../Summary/NER/RelateEntity/train/model-best-local\")\n",
    "nlp.add_pipe('sentencizer')\n",
    "nlp.add_pipe('newsent', name=\"newsent\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentences(inputfile, nlp):\n",
    "    with open(inputfile, 'r', encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = [str(sent).strip() for sent in doc.sents]\n",
    "\n",
    "    print(len(sentences))\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_unnecessary_spaces(out_string):\n",
    "    if not isinstance(out_string, str):\n",
    "        warnings.warn(f\">>> {out_string} <<< is not a string.\")\n",
    "        out_string = str(out_string)\n",
    "    out_string = (\n",
    "        out_string.replace(\" .\", \".\")\n",
    "        .replace(\" ?\", \"?\")\n",
    "        .replace(\" !\", \"!\")\n",
    "        .replace(\" ,\", \",\")\n",
    "        .replace(\" ' \", \"'\")\n",
    "        .replace(\" n't\", \"n't\")\n",
    "        .replace(\" 'm\", \"'m\")\n",
    "        .replace(\" 's\", \"'s\")\n",
    "        .replace(\" 've\", \"'ve\")\n",
    "        .replace(\" 're\", \"'re\")\n",
    "    )\n",
    "    return out_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "devDir = \"../../Summary/DATA/PARAPHRASE/Dev\"\n",
    "trainDir = \"../../Summary/DATA/PARAPHRASE/Train\"\n",
    "rplStr = [\"PG***\", \"ED***\", \"SCHQ***\", \"SCBQ***\", \"SCBF***\", \"SCHF***\", \"SCG***\", \"GF***\", \"GQ***\"]\n",
    "\n",
    "def createTrainingData(inputDir):\n",
    "    files = glob.glob(inputDir+\"/*_EP_YH.txt\")\n",
    "    #print(files)\n",
    "    #print(inputDir)\n",
    "    if(len(files) > 0):\n",
    "        for file in (files):\n",
    "            print(\"Input file \" + file)\n",
    "            basefile = os.path.basename(file)\n",
    "            inputfile = os.path.splitext(basefile)[0]\n",
    "            #print(inputfile)\n",
    "            outfilePath = inputfile + \"_phrase.tsv\"\n",
    "            outfilePath = inputDir + \"/\" + outfilePath\n",
    "            print(\"Phrase file \" + outfilePath)\n",
    "            outfile = Path(outfilePath)\n",
    "            if outfile.is_file():\n",
    "                print(\"Phrase file \" + str(outfile) + \" already exists\")\n",
    "                continue\n",
    "            with open(outfile, \"w\", encoding = \"utf-8\") as of:\n",
    "                #cnt = 0\n",
    "                of.write(\"filename\\tSentence1\\tSentence2\\n\")\n",
    "                #with open(file, encoding=\"utf-8\") as f:\n",
    "                sentences = getSentences(file, nlp)\n",
    "                #line = f.readline()\n",
    "                for line in sentences:\n",
    "                    for s in rplStr:\n",
    "                        line = line.replace(s, \"\")\n",
    "                    if(\"TBLST***\" in line or \"TBLET***\" in line or \"CS***\" in line or \"@@@\" in line or line == \"\\n\" or line == \"\\n\\n\" or \"https://finance.yahoo.com\" in line):\n",
    "                        #line = f.readline()\n",
    "                        continue\n",
    "                    line = line.replace(\"\\n\", \"\")\n",
    "                    if(line == \"\"):\n",
    "                        continue\n",
    "                    #print(line)\n",
    "                    #cnt = cnt + 1\n",
    "                    phraseLine = line\n",
    "                    of.write(inputfile+\"\\t\"+line+\"\\t\"+line+\"\\n\")\n",
    "                    #of.write(line + \"\\n\")\n",
    "                    #line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "devDataFile = \"../../Summary/DATA/PARAPHRASE/Dev/dev.tsv\"\n",
    "trainDataFile = \"../../Summary/DATA/PARAPHRASE/Train/train.tsv\"\n",
    "\n",
    "def writeTrainingData(writeFile, writeDir):\n",
    "    phrFiles = glob.glob(writeDir+\"/*_phrase.tsv\")\n",
    "    print(phrFiles)\n",
    "    frames = list()\n",
    "\n",
    "    if(len(phrFiles) > 0):\n",
    "        for file in phrFiles:\n",
    "            df = pd.read_csv(file, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "            df = df.dropna()\n",
    "            df = df[df['Sentence1'].notna()]\n",
    "            #print(df)\n",
    "            frames.append(df)\n",
    "    result = pd.concat(frames)\n",
    "    print(result)\n",
    "    result.to_csv(writeFile, sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\APPN_2023-02-16_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/APPN_2023-02-16_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\APPN_2023-02-16_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\BILL_2023-02-02_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/BILL_2023-02-02_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\BILL_2023-02-02_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\CFLT_2022-11-02_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/CFLT_2022-11-02_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\CFLT_2022-11-02_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\CRWD_2022-11-29_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/CRWD_2022-11-29_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\CRWD_2022-11-29_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\DDOG_2022-11-02_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/DDOG_2022-11-02_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\DDOG_2022-11-02_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\DOCN_2022-11-07_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/DOCN_2022-11-07_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\DOCN_2022-11-07_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\DOCU_2022-12-08_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/DOCU_2022-12-08_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\DOCU_2022-12-08_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\FIVN_2022-11-07_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/FIVN_2022-11-07_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\FIVN_2022-11-07_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\HUBS_2022-11-02_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/HUBS_2022-11-02_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\HUBS_2022-11-02_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\MDB_2022-12-06_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/MDB_2022-12-06_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\MDB_2022-12-06_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\NET_2022-11-03_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/NET_2022-11-03_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\NET_2022-11-03_EP_YH_phrase.tsv already exists\n",
      "['../../Summary/DATA/PARAPHRASE/Dev\\\\APPN_2023-02-16_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Dev\\\\BILL_2023-02-02_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Dev\\\\CFLT_2022-11-02_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Dev\\\\CRWD_2022-11-29_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Dev\\\\DDOG_2022-11-02_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Dev\\\\DOCN_2022-11-07_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Dev\\\\DOCU_2022-12-08_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Dev\\\\FIVN_2022-11-07_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Dev\\\\HUBS_2022-11-02_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Dev\\\\MDB_2022-12-06_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Dev\\\\NET_2022-11-03_EP_YH_phrase.tsv']\n",
      "                 filename                                          Sentence1  \\\n",
      "0   APPN_2023-02-16_EP_YH                                Appian Corporation.   \n",
      "1   APPN_2023-02-16_EP_YH   Fourth quarter cloud subscription revenue inc...   \n",
      "2   APPN_2023-02-16_EP_YH  Full year cloud subscription revenue increased...   \n",
      "3   APPN_2023-02-16_EP_YH        MCLEAN, Va., Feb. 16, 2023 (GLOBE NEWSWIRE)   \n",
      "4   APPN_2023-02-16_EP_YH  -- Appian (Nasdaq: APPN) today announced finan...   \n",
      "..                    ...                                                ...   \n",
      "43   NET_2022-11-03_EP_YH  Joining Cloudflare nearly 10 years ago, Chris ...   \n",
      "44   NET_2022-11-03_EP_YH  Chris will continue to help with the transitio...   \n",
      "45   NET_2022-11-03_EP_YH  Chris built a world-class sales organization, ...   \n",
      "46   NET_2022-11-03_EP_YH  We worked together to find the right leader to...   \n",
      "47   NET_2022-11-03_EP_YH  I’m grateful for everything we have accomplish...   \n",
      "\n",
      "                                            Sentence2  \n",
      "0                                 Appian Corporation.  \n",
      "1    Fourth quarter cloud subscription revenue is ...  \n",
      "2   Full year cloud subscription revenue is $236.9...  \n",
      "3         MCLEAN, Va., Feb. 16, 2023 (GLOBE NEWSWIRE)  \n",
      "4   -- Appian (Nasdaq: APPN) today announced finan...  \n",
      "..                                                ...  \n",
      "43  Joining Cloudflare nearly 10 years ago, Chris ...  \n",
      "44  Chris will continue to help with the transitio...  \n",
      "45  Chris built a world-class sales organization, ...  \n",
      "46  We worked together to find the right leader to...  \n",
      "47  I’m grateful for everything we have accomplish...  \n",
      "\n",
      "[514 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "createTrainingData(devDir)\n",
    "writeTrainingData(devDataFile, devDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\APPN_2022-11-03_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/APPN_2022-11-03_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\APPN_2022-11-03_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\BILL_2022-11-03_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/BILL_2022-11-03_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\BILL_2022-11-03_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\CFLT_2023-01-30_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/CFLT_2023-01-30_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\CFLT_2023-01-30_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\CRWD_2023-03-07_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/CRWD_2023-03-07_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\CRWD_2023-03-07_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\DDOG_2023-02-16_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/DDOG_2023-02-16_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\DDOG_2023-02-16_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\DOCN_2023-02-16_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/DOCN_2023-02-16_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\DOCN_2023-02-16_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\DOCU_2023-03-09_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/DOCU_2023-03-09_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\DOCU_2023-03-09_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\FIVN_2023-02-22_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/FIVN_2023-02-22_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\FIVN_2023-02-22_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\HUBS_2023-02-16_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/HUBS_2023-02-16_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\HUBS_2023-02-16_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\MDB_2023-03-08_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/MDB_2023-03-08_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\MDB_2023-03-08_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\NET_2023-04-27_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/NET_2023-04-27_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\NET_2023-04-27_EP_YH_phrase.tsv already exists\n",
      "['../../Summary/DATA/PARAPHRASE/Train\\\\APPN_2022-11-03_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Train\\\\BILL_2022-11-03_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Train\\\\CFLT_2023-01-30_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Train\\\\CRWD_2023-03-07_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Train\\\\DDOG_2023-02-16_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Train\\\\DOCN_2023-02-16_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Train\\\\DOCU_2023-03-09_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Train\\\\FIVN_2023-02-22_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Train\\\\HUBS_2023-02-16_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Train\\\\MDB_2023-03-08_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Train\\\\NET_2023-04-27_EP_YH_phrase.tsv']\n",
      "                 filename                                          Sentence1  \\\n",
      "0   APPN_2022-11-03_EP_YH                                Appian Corporation.   \n",
      "1   APPN_2022-11-03_EP_YH   Third quarter cloud subscription revenue incr...   \n",
      "2   APPN_2022-11-03_EP_YH        MCLEAN, Va., Nov. 03, 2022 (GLOBE NEWSWIRE)   \n",
      "3   APPN_2022-11-03_EP_YH  -- Appian (Nasdaq: APPN) today announced finan...   \n",
      "4   APPN_2022-11-03_EP_YH  On a constant currency basis, both cloud subsc...   \n",
      "..                    ...                                                ...   \n",
      "28   NET_2023-04-27_EP_YH     Total revenue of $1,280.0 to $1,284.0 million.   \n",
      "29   NET_2023-04-27_EP_YH   Non-GAAP income from operations of $73.0 to $...   \n",
      "30   NET_2023-04-27_EP_YH   Non-GAAP net income per share of $0.34 to $0....   \n",
      "31   NET_2023-04-27_EP_YH   These statements are forward-looking and actu...   \n",
      "32   NET_2023-04-27_EP_YH  Refer to the Forward-Looking Statements safe h...   \n",
      "\n",
      "                                            Sentence2  \n",
      "0                                 Appian Corporation.  \n",
      "1        Cloud subscription revenue is $60.6 million.  \n",
      "2         MCLEAN, Va., Nov. 03, 2022 (GLOBE NEWSWIRE)  \n",
      "3   -- Appian (Nasdaq: APPN) today announced finan...  \n",
      "4   On a constant currency basis, both cloud subsc...  \n",
      "..                                                ...  \n",
      "28     Total revenue of $1,280.0 to $1,284.0 million.  \n",
      "29   Non-GAAP income from operations of $73.0 to $...  \n",
      "30   Non-GAAP net income per share of $0.34 to $0....  \n",
      "31   These statements are forward-looking and actu...  \n",
      "32  Refer to the Forward-Looking Statements safe h...  \n",
      "\n",
      "[568 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "createTrainingData(trainDir)\n",
    "writeTrainingData(trainDataFile, trainDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATA ..............\n",
      "         prefix                                         input_text  \\\n",
      "0    paraphrase                                Appian Corporation.   \n",
      "1    paraphrase   Third quarter cloud subscription revenue incr...   \n",
      "2    paraphrase        MCLEAN, Va., Nov. 03, 2022 (GLOBE NEWSWIRE)   \n",
      "3    paraphrase  -- Appian (Nasdaq: APPN) today announced finan...   \n",
      "4    paraphrase  On a constant currency basis, both cloud subsc...   \n",
      "..          ...                                                ...   \n",
      "563  paraphrase     Total revenue of $1,280.0 to $1,284.0 million.   \n",
      "564  paraphrase   Non-GAAP income from operations of $73.0 to $...   \n",
      "565  paraphrase   Non-GAAP net income per share of $0.34 to $0....   \n",
      "566  paraphrase   These statements are forward-looking and actu...   \n",
      "567  paraphrase  Refer to the Forward-Looking Statements safe h...   \n",
      "\n",
      "                                           target_text  \n",
      "0                                  Appian Corporation.  \n",
      "1         Cloud subscription revenue is $60.6 million.  \n",
      "2          MCLEAN, Va., Nov. 03, 2022 (GLOBE NEWSWIRE)  \n",
      "3    -- Appian (Nasdaq: APPN) today announced finan...  \n",
      "4    On a constant currency basis, both cloud subsc...  \n",
      "..                                                 ...  \n",
      "563     Total revenue of $1,280.0 to $1,284.0 million.  \n",
      "564   Non-GAAP income from operations of $73.0 to $...  \n",
      "565   Non-GAAP net income per share of $0.34 to $0....  \n",
      "566   These statements are forward-looking and actu...  \n",
      "567  Refer to the Forward-Looking Statements safe h...  \n",
      "\n",
      "[568 rows x 3 columns]\n",
      "EVAL DATA ..............\n",
      "         prefix                                         input_text  \\\n",
      "0    paraphrase                                Appian Corporation.   \n",
      "1    paraphrase   Fourth quarter cloud subscription revenue inc...   \n",
      "2    paraphrase  Full year cloud subscription revenue increased...   \n",
      "3    paraphrase        MCLEAN, Va., Feb. 16, 2023 (GLOBE NEWSWIRE)   \n",
      "4    paraphrase  -- Appian (Nasdaq: APPN) today announced finan...   \n",
      "..          ...                                                ...   \n",
      "509  paraphrase  Joining Cloudflare nearly 10 years ago, Chris ...   \n",
      "510  paraphrase  Chris will continue to help with the transitio...   \n",
      "511  paraphrase  Chris built a world-class sales organization, ...   \n",
      "512  paraphrase  We worked together to find the right leader to...   \n",
      "513  paraphrase  I’m grateful for everything we have accomplish...   \n",
      "\n",
      "                                           target_text  \n",
      "0                                  Appian Corporation.  \n",
      "1     Fourth quarter cloud subscription revenue is ...  \n",
      "2    Full year cloud subscription revenue is $236.9...  \n",
      "3          MCLEAN, Va., Feb. 16, 2023 (GLOBE NEWSWIRE)  \n",
      "4    -- Appian (Nasdaq: APPN) today announced finan...  \n",
      "..                                                 ...  \n",
      "509  Joining Cloudflare nearly 10 years ago, Chris ...  \n",
      "510  Chris will continue to help with the transitio...  \n",
      "511  Chris built a world-class sales organization, ...  \n",
      "512  We worked together to find the right leader to...  \n",
      "513  I’m grateful for everything we have accomplish...  \n",
      "\n",
      "[514 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(trainDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "eval_df = pd.read_csv(devDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "\n",
    "train_df = train_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "eval_df = eval_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "\n",
    "train_df = train_df[[\"input_text\", \"target_text\"]]\n",
    "eval_df = eval_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "train_df[\"prefix\"] = \"paraphrase\"\n",
    "train_df = train_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "\n",
    "eval_df[\"prefix\"] = \"paraphrase\"\n",
    "eval_df = eval_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "train_df = train_df[train_df['input_text'].notna()]\n",
    "\n",
    "eval_df = eval_df.dropna()\n",
    "eval_df = eval_df[eval_df['input_text'].notna()]\n",
    "\n",
    "train_df[\"input_text\"] = train_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "train_df[\"target_text\"] = train_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "print(\"TRAIN DATA ..............\")\n",
    "print(train_df)\n",
    "\n",
    "eval_df[\"input_text\"] = eval_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "eval_df[\"target_text\"] = eval_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "print(\"EVAL DATA ..............\")\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abddaf657e749a197eedf9f1dee493d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/568 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_model: Training started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f24b986d4a241369394a5b5202d5762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785abffffd8e430184b462d76710cb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 2:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "INFO:simpletransformers.seq2seq.seq2seq_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f158648f66084f5ab8ab214f2c60d449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ef11d34efe456287081c4277ad6938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_model:{'eval_loss': 0.016265874995683976}\n",
      "INFO:simpletransformers.seq2seq.seq2seq_model:Saving model into outputs/best_model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c123f2145af1451a9dd5fe02032fa1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 2:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8324dbec43bc42d48a108878bcce95e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3120748fcf6d4d11bc9ef94e79be2e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_model:{'eval_loss': 0.008469422039207678}\n",
      "INFO:simpletransformers.seq2seq.seq2seq_model:Saving model into outputs/best_model\n",
      "INFO:simpletransformers.seq2seq.seq2seq_model:Saving model into outputs/\n",
      "INFO:simpletransformers.seq2seq.seq2seq_model: Training of facebook/bart-base model complete. Saved to outputs/.\n",
      "INFO:simpletransformers.seq2seq.seq2seq_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab248c49f1df4c8dbdf90675866849e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908c9a153530472994e518363a0ca3ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c91642277a849518088009b8ff5118c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_model:{'eval_loss': 0.008469422039207678}\n"
     ]
    }
   ],
   "source": [
    "model_args = Seq2SeqArgs()\n",
    "model_args.do_sample = True\n",
    "model_args.train_batch_size = 4\n",
    "model_args.use_multiprocessing = False\n",
    "model_args.num_train_epochs = 2\n",
    "model_args.learning_rate = 5e-5\n",
    "#model_args.no_save = True\n",
    "\n",
    "model_args.eval_batch_size = 4\n",
    "model_args.evaluate_generated_text = True\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_verbose = True\n",
    "#model_args.evaluate_during_training_steps = 10\n",
    "\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.save_eval_checkpoints = False\n",
    "model_args.save_model_every_epoch = False\n",
    "model_args.save_steps = -1\n",
    "\n",
    "model = Seq2SeqModel(\n",
    "    encoder_decoder_type=\"bart\",\n",
    "    encoder_decoder_name=\"facebook/bart-base\",\n",
    "    args=model_args,\n",
    "    use_cuda=True,\n",
    ")\n",
    "# Train the model\n",
    "model.train_model(\n",
    "    train_df, eval_data=eval_df\n",
    ")\n",
    "results = model.eval_model(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1151fece665748d9b19e3e217112696e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q3 cloud subscription revenue is $236.9 million.']\n"
     ]
    }
   ],
   "source": [
    "model = Seq2SeqModel(\n",
    "    encoder_decoder_type=\"bart\", encoder_decoder_name=\"outputs\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    model.predict(\n",
    "        [\n",
    "            \"Q3 cloud subscription revenue increased 32% year-over year to $236.9 million\"\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.ERROR)\n",
    "\n",
    "model = Seq2SeqModel(\n",
    "    encoder_decoder_type=\"bart\", encoder_decoder_name=\"outputs\"\n",
    ")\n",
    "\n",
    "\n",
    "while True:\n",
    "    original = input(\"Enter text to paraphrase: \")\n",
    "    to_predict = [original]\n",
    "\n",
    "    preds = model.predict(to_predict)\n",
    "\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    print(original)\n",
    "\n",
    "    print()\n",
    "    print(\"Predictions >>>\")\n",
    "    for pred in preds[0]:\n",
    "        print(pred)\n",
    "\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, input_text_column, target_text_column, label_column, keep_label=1):\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\", error_bad_lines=False)\n",
    "    df = df.loc[df[label_column] == keep_label]\n",
    "    df = df.rename(\n",
    "        columns={input_text_column: \"input_text\", target_text_column: \"target_text\"}\n",
    "    )\n",
    "    df = df[[\"input_text\", \"target_text\"]]\n",
    "    df[\"prefix\"] = \"paraphrase\"\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../../Summary/ParaPhrase/final/train.tsv\", sep=\"\\t\").astype(str)\n",
    "eval_df = pd.read_csv(\"../../Summary/ParaPhrase/final/dev.tsv\", sep=\"\\t\").astype(str)\n",
    "\n",
    "train_df = train_df.loc[train_df[\"label\"] == \"1\"]\n",
    "eval_df = eval_df.loc[eval_df[\"label\"] == \"1\"]\n",
    "\n",
    "train_df = train_df.rename(\n",
    "    columns={\"sentence1\": \"input_text\", \"sentence2\": \"target_text\"}\n",
    ")\n",
    "eval_df = eval_df.rename(\n",
    "    columns={\"sentence1\": \"input_text\", \"sentence2\": \"target_text\"}\n",
    ")\n",
    "\n",
    "train_df = train_df[[\"input_text\", \"target_text\"]]\n",
    "eval_df = eval_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "train_df[\"prefix\"] = \"paraphrase\"\n",
    "eval_df[\"prefix\"] = \"paraphrase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         prefix                                         input_text  \\\n",
      "1    paraphrase  They were there to enjoy us and they were ther...   \n",
      "2    paraphrase  After the end of the war in June 1902, Higgins...   \n",
      "3    paraphrase  From the merger of the Four Rivers Council and...   \n",
      "4    paraphrase  The group toured extensively and became famous...   \n",
      "5    paraphrase  Kathy and her husband Pete Beale ( Peter Dean ...   \n",
      "..          ...                                                ...   \n",
      "990  paraphrase  After his service Lockhart lived in Texas but ...   \n",
      "991  paraphrase  After medical treatment, Strozzi started takin...   \n",
      "992  paraphrase  In December 1969 became the 49th Army - Divisi...   \n",
      "993  paraphrase  In `` The Stand '' by Glen Pequod Bateman, Woo...   \n",
      "996  paraphrase  Dora Rangelova is the current captain of the B...   \n",
      "\n",
      "                                           target_text  \n",
      "1    They were there for us to enjoy and they were ...  \n",
      "2    In August, after the end of the war in June 19...  \n",
      "3    Shawnee Trails Council was formed from the mer...  \n",
      "4    The group toured extensively and was famous in...  \n",
      "5    Kathy and her husband Peter Dean ( Pete Beale ...  \n",
      "..                                                 ...  \n",
      "990  After his service, Lockhart lived in Texas, bu...  \n",
      "991  Strozzi started taking private acting lessons ...  \n",
      "992  In December 1969, 49th Army Division became 29...  \n",
      "993  In `` The Stand '' by Glen Pequod Bateman, Woo...  \n",
      "996  Dora Rangelova is the current captain of the B...  \n",
      "\n",
      "[451 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "eval_df = eval_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "eval_df = eval_df.dropna()\n",
    "train_df[\"input_text\"] = train_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "train_df[\"target_text\"] = train_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "\n",
    "eval_df[\"input_text\"] = eval_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "eval_df[\"target_text\"] = eval_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "\n",
    "train_df = (train_df.loc[1:10000])\n",
    "eval_df = (eval_df.loc[1:1000])\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe9ad91807a474698868cdd13b6c2bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_model: Training started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0edfec68e0440a297995d3793dc0947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e9307374ab4d238fd7650bc4a01440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 2:   0%|          | 0/1123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067d9866f391495380e5d471d86927a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/451 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_model:{'eval_loss': 0.5714342097441355}\n",
      "INFO:simpletransformers.seq2seq.seq2seq_model:Saving model into outputs/best_model\n",
      "INFO:simpletransformers.seq2seq.seq2seq_model:Saving model into outputs/checkpoint-1123-epoch-1\n",
      "INFO:simpletransformers.seq2seq.seq2seq_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b00ecd79304b09a9f802bb44c6642a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/451 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_model:{'eval_loss': 0.5561367531617483}\n",
      "INFO:simpletransformers.seq2seq.seq2seq_model:Saving model into outputs/best_model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd21cb1fcdc24d5f84a8dabb805ae8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 2:   0%|          | 0/1123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2a34c3d6db471f91f6a94a3232e6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/451 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_model:{'eval_loss': 0.5267087161540985}\n",
      "INFO:simpletransformers.seq2seq.seq2seq_model:Saving model into outputs/best_model\n",
      "INFO:simpletransformers.seq2seq.seq2seq_model:Saving model into outputs/checkpoint-2246-epoch-2\n",
      "INFO:simpletransformers.seq2seq.seq2seq_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b29806947ef4abf97e157b00fd62a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/451 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_model:{'eval_loss': 0.52150759100914}\n",
      "INFO:simpletransformers.seq2seq.seq2seq_model:Saving model into outputs/best_model\n",
      "INFO:simpletransformers.seq2seq.seq2seq_model:Saving model into outputs/\n",
      "INFO:simpletransformers.seq2seq.seq2seq_model: Training of facebook/bart-base model complete. Saved to outputs/.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15cc7e268d4b4085967d228fe5ec00a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 222.00 MiB (GPU 0; 4.00 GiB total capacity; 2.68 GiB already allocated; 0 bytes free; 3.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-19f3c7fea0ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mtruth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target_text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\simpletransformers\\seq2seq\\seq2seq_model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, to_predict)\u001b[0m\n\u001b[0;32m   1311\u001b[0m                     \u001b[0mtop_k\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1312\u001b[0m                     \u001b[0mtop_p\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1313\u001b[1;33m                     \u001b[0mnum_return_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_return_sequences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1314\u001b[0m                 )\n\u001b[0;32m   1315\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"mbart\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1398\u001b[0m                 \u001b[0msynced_gpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynced_gpus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m                 \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1400\u001b[0m             )\n\u001b[0;32m   1401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mbeam_sample\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   2529\u001b[0m             \u001b[0mnext_token_scores_processed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits_processor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_token_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2530\u001b[0m             \u001b[0mnext_token_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_token_scores_processed\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbeam_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2531\u001b[1;33m             \u001b[0mnext_token_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits_warper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_token_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2533\u001b[0m             \u001b[1;31m# Store scores, attentions and hidden_states when required\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\generation_logits_process.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m                 \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m                 \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\generation_logits_process.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, input_ids, scores)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[0msorted_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m         \u001b[0mcumulative_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted_logits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 222.00 MiB (GPU 0; 4.00 GiB total capacity; 2.68 GiB already allocated; 0 bytes free; 3.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "model_args = Seq2SeqArgs()\n",
    "model_args.do_sample = True\n",
    "#model_args.eval_batch_size = 64\n",
    "model_args.eval_batch_size = 32\n",
    "model_args.evaluate_during_training = True\n",
    "#model_args.evaluate_during_training_steps = 2500\n",
    "model_args.evaluate_during_training_steps = 1000\n",
    "model_args.evaluate_during_training_verbose = True\n",
    "model_args.fp16 = False\n",
    "model_args.learning_rate = 5e-5\n",
    "#model_args.max_length = 128\n",
    "model_args.max_length = 32\n",
    "#model_args.max_seq_length = 128\n",
    "model_args.max_seq_length = 32\n",
    "model_args.num_beams = None\n",
    "model_args.num_return_sequences = 3\n",
    "model_args.num_train_epochs = 2\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.save_eval_checkpoints = False\n",
    "model_args.save_steps = -1\n",
    "model_args.top_k = 50\n",
    "model_args.top_p = 0.95\n",
    "#model_args.train_batch_size = 8\n",
    "model_args.train_batch_size = 4\n",
    "model_args.use_multiprocessing = False\n",
    "#model_args.wandb_project = \"Paraphrasing with BART\"\n",
    "\n",
    "\n",
    "model = Seq2SeqModel(\n",
    "    encoder_decoder_type=\"bart\",\n",
    "    encoder_decoder_name=\"facebook/bart-base\",\n",
    "    args=model_args,\n",
    ")\n",
    "model.train_model(train_df, eval_data=eval_df)\n",
    "\n",
    "to_predict = [\n",
    "    prefix + \": \" + str(input_text)\n",
    "    for prefix, input_text in zip(eval_df[\"prefix\"].tolist(), eval_df[\"input_text\"].tolist())\n",
    "]\n",
    "truth = eval_df[\"target_text\"].tolist()\n",
    "\n",
    "preds = model.predict(to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
