{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\users\\ankan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\spacy_transformers\\pipeline_component.py:406: UserWarning: Automatically converting a transformer component from spacy-transformers v1.0 to v1.1+. If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spacy-transformers version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.set_custom_boundaries(doc)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy import displacy\n",
    "import time\n",
    "\n",
    "@Language.component(\"newsent\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        #print(token.text, token.text in (\"â€™s\", \"'s\"))\n",
    "        if token.text.upper() in (\";\", \"--\", \"\\n\\n\", \"\\n\", \"QUARTERLY\", \"STORY\", \"\\n\\n\\n\\n\", \"\\n\\n\\n\"):\n",
    "            print(\"Detected:\", token.text)\n",
    "            doc[token.i].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "#spacy.require_gpu()\n",
    "nlp = spacy.load(\"../../Summary/NER/RelateEntity/train/model-best-local\")\n",
    "nlp.add_pipe('sentencizer')\n",
    "nlp.add_pipe('newsent', name=\"newsent\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentences(inputfile, nlp):\n",
    "    with open(inputfile, 'r', encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = [str(sent).strip() for sent in doc.sents]\n",
    "\n",
    "    print(len(sentences))\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_unnecessary_spaces(out_string):\n",
    "    if not isinstance(out_string, str):\n",
    "        warnings.warn(f\">>> {out_string} <<< is not a string.\")\n",
    "        out_string = str(out_string)\n",
    "    out_string = (\n",
    "        out_string.replace(\" .\", \".\")\n",
    "        .replace(\" ?\", \"?\")\n",
    "        .replace(\" !\", \"!\")\n",
    "        .replace(\" ,\", \",\")\n",
    "        .replace(\" ' \", \"'\")\n",
    "        .replace(\" n't\", \"n't\")\n",
    "        .replace(\" 'm\", \"'m\")\n",
    "        .replace(\" 's\", \"'s\")\n",
    "        .replace(\" 've\", \"'ve\")\n",
    "        .replace(\" 're\", \"'re\")\n",
    "    )\n",
    "    return out_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "devDir = \"../../Summary/DATA/PARAPHRASE/Dev\"\n",
    "trainDir = \"../../Summary/DATA/PARAPHRASE/Train\"\n",
    "rplStr = [\"PG***\", \"ED***\", \"SCHQ***\", \"SCBQ***\", \"SCBF***\", \"SCHF***\", \"SCG***\", \"GF***\", \"GQ***\"]\n",
    "\n",
    "def createTrainingData(inputDir):\n",
    "    files = glob.glob(inputDir+\"/*_EP_YH.txt\")\n",
    "    #print(files)\n",
    "    #print(inputDir)\n",
    "    if(len(files) > 0):\n",
    "        for file in (files):\n",
    "            print(\"Input file \" + file)\n",
    "            basefile = os.path.basename(file)\n",
    "            inputfile = os.path.splitext(basefile)[0]\n",
    "            #print(inputfile)\n",
    "            outfilePath = inputfile + \"_phrase.tsv\"\n",
    "            outfilePath = inputDir + \"/\" + outfilePath\n",
    "            print(\"Phrase file \" + outfilePath)\n",
    "            outfile = Path(outfilePath)\n",
    "            if outfile.is_file():\n",
    "                print(\"Phrase file \" + str(outfile) + \" already exists\")\n",
    "                continue\n",
    "            with open(outfile, \"w\", encoding = \"utf-8\") as of:\n",
    "                #cnt = 0\n",
    "                of.write(\"filename\\tSentence1\\tSentence2\\n\")\n",
    "                #with open(file, encoding=\"utf-8\") as f:\n",
    "                sentences = getSentences(file, nlp)\n",
    "                #line = f.readline()\n",
    "                for line in sentences:\n",
    "                    for s in rplStr:\n",
    "                        line = line.replace(s, \"\")\n",
    "                    if(\"TBLST***\" in line or \"TBLET***\" in line or \"CS***\" in line or \"@@@\" in line or line == \"\\n\" or line == \"\\n\\n\" or \"https://finance.yahoo.com\" in line):\n",
    "                        #line = f.readline()\n",
    "                        continue\n",
    "                    line = line.replace(\"\\n\", \"\")\n",
    "                    if(line == \"\"):\n",
    "                        continue\n",
    "                    #print(line)\n",
    "                    #cnt = cnt + 1\n",
    "                    phraseLine = line\n",
    "                    of.write(inputfile+\"\\t\"+line+\"\\t\"+line+\"\\n\")\n",
    "                    #of.write(line + \"\\n\")\n",
    "                    #line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "devDataFile = \"../../Summary/DATA/PARAPHRASE/Dev/dev.tsv\"\n",
    "trainDataFile = \"../../Summary/DATA/PARAPHRASE/Train/train.tsv\"\n",
    "\n",
    "def writeTrainingData(writeFile, writeDir):\n",
    "    phrFiles = glob.glob(writeDir+\"/*_phrase.tsv\")\n",
    "    print(phrFiles)\n",
    "    frames = list()\n",
    "\n",
    "    if(len(phrFiles) > 0):\n",
    "        for file in phrFiles:\n",
    "            df = pd.read_csv(file, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "            df = df.dropna()\n",
    "            df = df[df['Sentence1'].notna()]\n",
    "            #print(df)\n",
    "            frames.append(df)\n",
    "    result = pd.concat(frames)\n",
    "    print(result)\n",
    "    result.to_csv(writeFile, sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\APPN_2023-02-16_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/APPN_2023-02-16_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\APPN_2023-02-16_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\BILL_2023-02-02_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/BILL_2023-02-02_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\BILL_2023-02-02_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\CFLT_2022-11-02_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/CFLT_2022-11-02_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\CFLT_2022-11-02_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\CRWD_2022-11-29_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/CRWD_2022-11-29_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\CRWD_2022-11-29_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\DDOG_2022-11-02_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/DDOG_2022-11-02_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\DDOG_2022-11-02_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\DOCN_2022-11-07_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/DOCN_2022-11-07_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\DOCN_2022-11-07_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\DOCU_2022-12-08_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/DOCU_2022-12-08_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\DOCU_2022-12-08_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\FIVN_2022-11-07_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/FIVN_2022-11-07_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\FIVN_2022-11-07_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\HUBS_2022-11-02_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/HUBS_2022-11-02_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\HUBS_2022-11-02_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\MDB_2022-12-06_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/MDB_2022-12-06_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\MDB_2022-12-06_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Dev\\NET_2022-11-03_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Dev/NET_2022-11-03_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Dev\\NET_2022-11-03_EP_YH_phrase.tsv already exists\n",
      "['../../Summary/DATA/PARAPHRASE/Dev\\\\APPN_2023-02-16_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Dev\\\\BILL_2023-02-02_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Dev\\\\CFLT_2022-11-02_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Dev\\\\CRWD_2022-11-29_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Dev\\\\DDOG_2022-11-02_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Dev\\\\DOCN_2022-11-07_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Dev\\\\DOCU_2022-12-08_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Dev\\\\FIVN_2022-11-07_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Dev\\\\HUBS_2022-11-02_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Dev\\\\MDB_2022-12-06_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Dev\\\\NET_2022-11-03_EP_YH_phrase.tsv']\n",
      "                 filename                                          Sentence1  \\\n",
      "0   APPN_2023-02-16_EP_YH                                Appian Corporation.   \n",
      "1   APPN_2023-02-16_EP_YH   Fourth quarter cloud subscription revenue inc...   \n",
      "2   APPN_2023-02-16_EP_YH  Full year cloud subscription revenue increased...   \n",
      "3   APPN_2023-02-16_EP_YH        MCLEAN, Va., Feb. 16, 2023 (GLOBE NEWSWIRE)   \n",
      "4   APPN_2023-02-16_EP_YH  -- Appian (Nasdaq: APPN) today announced finan...   \n",
      "..                    ...                                                ...   \n",
      "43   NET_2022-11-03_EP_YH  Joining Cloudflare nearly 10 years ago, Chris ...   \n",
      "44   NET_2022-11-03_EP_YH  Chris will continue to help with the transitio...   \n",
      "45   NET_2022-11-03_EP_YH  Chris built a world-class sales organization, ...   \n",
      "46   NET_2022-11-03_EP_YH  We worked together to find the right leader to...   \n",
      "47   NET_2022-11-03_EP_YH  Iâ€™m grateful for everything we have accomplish...   \n",
      "\n",
      "                                            Sentence2  \n",
      "0                                 Appian Corporation.  \n",
      "1    Fourth quarter cloud subscription revenue is ...  \n",
      "2   Full year cloud subscription revenue is $236.9...  \n",
      "3         MCLEAN, Va., Feb. 16, 2023 (GLOBE NEWSWIRE)  \n",
      "4   -- Appian (Nasdaq: APPN) today announced finan...  \n",
      "..                                                ...  \n",
      "43  Joining Cloudflare nearly 10 years ago, Chris ...  \n",
      "44  Chris will continue to help with the transitio...  \n",
      "45  Chris built a world-class sales organization, ...  \n",
      "46  We worked together to find the right leader to...  \n",
      "47  Iâ€™m grateful for everything we have accomplish...  \n",
      "\n",
      "[514 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "createTrainingData(devDir)\n",
    "writeTrainingData(devDataFile, devDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\APPN_2022-11-03_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/APPN_2022-11-03_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\APPN_2022-11-03_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\BILL_2022-11-03_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/BILL_2022-11-03_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\BILL_2022-11-03_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\CFLT_2023-01-30_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/CFLT_2023-01-30_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\CFLT_2023-01-30_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\CRWD_2023-03-07_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/CRWD_2023-03-07_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\CRWD_2023-03-07_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\DDOG_2023-02-16_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/DDOG_2023-02-16_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\DDOG_2023-02-16_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\DOCN_2023-02-16_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/DOCN_2023-02-16_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\DOCN_2023-02-16_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\DOCU_2023-03-09_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/DOCU_2023-03-09_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\DOCU_2023-03-09_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\FIVN_2023-02-22_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/FIVN_2023-02-22_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\FIVN_2023-02-22_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\HUBS_2023-02-16_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/HUBS_2023-02-16_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\HUBS_2023-02-16_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\MDB_2023-03-08_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/MDB_2023-03-08_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\MDB_2023-03-08_EP_YH_phrase.tsv already exists\n",
      "Input file ../../Summary/DATA/PARAPHRASE/Train\\NET_2023-04-27_EP_YH.txt\n",
      "Phrase file ../../Summary/DATA/PARAPHRASE/Train/NET_2023-04-27_EP_YH_phrase.tsv\n",
      "Phrase file ..\\..\\Summary\\DATA\\PARAPHRASE\\Train\\NET_2023-04-27_EP_YH_phrase.tsv already exists\n",
      "['../../Summary/DATA/PARAPHRASE/Train\\\\APPN_2022-11-03_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Train\\\\BILL_2022-11-03_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Train\\\\CFLT_2023-01-30_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Train\\\\CRWD_2023-03-07_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Train\\\\DDOG_2023-02-16_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Train\\\\DOCN_2023-02-16_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Train\\\\DOCU_2023-03-09_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Train\\\\FIVN_2023-02-22_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Train\\\\HUBS_2023-02-16_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Train\\\\MDB_2023-03-08_EP_YH_phrase.tsv', '../../Summary/DATA/PARAPHRASE/Train\\\\NET_2023-04-27_EP_YH_phrase.tsv']\n",
      "                 filename                                          Sentence1  \\\n",
      "0   APPN_2022-11-03_EP_YH                                Appian Corporation.   \n",
      "1   APPN_2022-11-03_EP_YH   Third quarter cloud subscription revenue incr...   \n",
      "2   APPN_2022-11-03_EP_YH        MCLEAN, Va., Nov. 03, 2022 (GLOBE NEWSWIRE)   \n",
      "3   APPN_2022-11-03_EP_YH  -- Appian (Nasdaq: APPN) today announced finan...   \n",
      "4   APPN_2022-11-03_EP_YH  On a constant currency basis, both cloud subsc...   \n",
      "..                    ...                                                ...   \n",
      "28   NET_2023-04-27_EP_YH     Total revenue of $1,280.0 to $1,284.0 million.   \n",
      "29   NET_2023-04-27_EP_YH   Non-GAAP income from operations of $73.0 to $...   \n",
      "30   NET_2023-04-27_EP_YH   Non-GAAP net income per share of $0.34 to $0....   \n",
      "31   NET_2023-04-27_EP_YH   These statements are forward-looking and actu...   \n",
      "32   NET_2023-04-27_EP_YH  Refer to the Forward-Looking Statements safe h...   \n",
      "\n",
      "                                            Sentence2  \n",
      "0                                 Appian Corporation.  \n",
      "1        Cloud subscription revenue is $60.6 million.  \n",
      "2         MCLEAN, Va., Nov. 03, 2022 (GLOBE NEWSWIRE)  \n",
      "3   -- Appian (Nasdaq: APPN) today announced finan...  \n",
      "4   On a constant currency basis, both cloud subsc...  \n",
      "..                                                ...  \n",
      "28     Total revenue of $1,280.0 to $1,284.0 million.  \n",
      "29   Non-GAAP income from operations of $73.0 to $...  \n",
      "30   Non-GAAP net income per share of $0.34 to $0....  \n",
      "31   These statements are forward-looking and actu...  \n",
      "32  Refer to the Forward-Looking Statements safe h...  \n",
      "\n",
      "[568 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "createTrainingData(trainDir)\n",
    "writeTrainingData(trainDataFile, trainDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATA ..............\n",
      "         prefix                                         input_text  \\\n",
      "0    paraphrase                                Appian Corporation.   \n",
      "1    paraphrase   Third quarter cloud subscription revenue incr...   \n",
      "2    paraphrase        MCLEAN, Va., Nov. 03, 2022 (GLOBE NEWSWIRE)   \n",
      "3    paraphrase  -- Appian (Nasdaq: APPN) today announced finan...   \n",
      "4    paraphrase  On a constant currency basis, both cloud subsc...   \n",
      "..          ...                                                ...   \n",
      "563  paraphrase     Total revenue of $1,280.0 to $1,284.0 million.   \n",
      "564  paraphrase   Non-GAAP income from operations of $73.0 to $...   \n",
      "565  paraphrase   Non-GAAP net income per share of $0.34 to $0....   \n",
      "566  paraphrase   These statements are forward-looking and actu...   \n",
      "567  paraphrase  Refer to the Forward-Looking Statements safe h...   \n",
      "\n",
      "                                           target_text  \n",
      "0                                  Appian Corporation.  \n",
      "1         Cloud subscription revenue is $60.6 million.  \n",
      "2          MCLEAN, Va., Nov. 03, 2022 (GLOBE NEWSWIRE)  \n",
      "3    -- Appian (Nasdaq: APPN) today announced finan...  \n",
      "4    On a constant currency basis, both cloud subsc...  \n",
      "..                                                 ...  \n",
      "563     Total revenue of $1,280.0 to $1,284.0 million.  \n",
      "564   Non-GAAP income from operations of $73.0 to $...  \n",
      "565   Non-GAAP net income per share of $0.34 to $0....  \n",
      "566   These statements are forward-looking and actu...  \n",
      "567  Refer to the Forward-Looking Statements safe h...  \n",
      "\n",
      "[568 rows x 3 columns]\n",
      "EVAL DATA ..............\n",
      "         prefix                                         input_text  \\\n",
      "0    paraphrase                                Appian Corporation.   \n",
      "1    paraphrase   Fourth quarter cloud subscription revenue inc...   \n",
      "2    paraphrase  Full year cloud subscription revenue increased...   \n",
      "3    paraphrase        MCLEAN, Va., Feb. 16, 2023 (GLOBE NEWSWIRE)   \n",
      "4    paraphrase  -- Appian (Nasdaq: APPN) today announced finan...   \n",
      "..          ...                                                ...   \n",
      "509  paraphrase  Joining Cloudflare nearly 10 years ago, Chris ...   \n",
      "510  paraphrase  Chris will continue to help with the transitio...   \n",
      "511  paraphrase  Chris built a world-class sales organization, ...   \n",
      "512  paraphrase  We worked together to find the right leader to...   \n",
      "513  paraphrase  Iâ€™m grateful for everything we have accomplish...   \n",
      "\n",
      "                                           target_text  \n",
      "0                                  Appian Corporation.  \n",
      "1     Fourth quarter cloud subscription revenue is ...  \n",
      "2    Full year cloud subscription revenue is $236.9...  \n",
      "3          MCLEAN, Va., Feb. 16, 2023 (GLOBE NEWSWIRE)  \n",
      "4    -- Appian (Nasdaq: APPN) today announced finan...  \n",
      "..                                                 ...  \n",
      "509  Joining Cloudflare nearly 10 years ago, Chris ...  \n",
      "510  Chris will continue to help with the transitio...  \n",
      "511  Chris built a world-class sales organization, ...  \n",
      "512  We worked together to find the right leader to...  \n",
      "513  Iâ€™m grateful for everything we have accomplish...  \n",
      "\n",
      "[514 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(trainDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "eval_df = pd.read_csv(devDataFile, sep=\"\\t\", encoding = \"utf-8\").astype(str)\n",
    "\n",
    "train_df = train_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "eval_df = eval_df.rename(\n",
    "    columns={\"Sentence1\": \"input_text\", \"Sentence2\": \"target_text\"}\n",
    ")\n",
    "\n",
    "train_df = train_df[[\"input_text\", \"target_text\"]]\n",
    "eval_df = eval_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "train_df[\"prefix\"] = \"paraphrase\"\n",
    "train_df = train_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "\n",
    "eval_df[\"prefix\"] = \"paraphrase\"\n",
    "eval_df = eval_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "train_df = train_df[train_df['input_text'].notna()]\n",
    "\n",
    "eval_df = eval_df.dropna()\n",
    "eval_df = eval_df[eval_df['input_text'].notna()]\n",
    "\n",
    "train_df[\"input_text\"] = train_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "train_df[\"target_text\"] = train_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "print(\"TRAIN DATA ..............\")\n",
    "print(train_df)\n",
    "\n",
    "eval_df[\"input_text\"] = eval_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "eval_df[\"target_text\"] = eval_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "print(\"EVAL DATA ..............\")\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, input_text_column, target_text_column, label_column, keep_label=1):\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\", error_bad_lines=False)\n",
    "    df = df.loc[df[label_column] == keep_label]\n",
    "    df = df.rename(\n",
    "        columns={input_text_column: \"input_text\", target_text_column: \"target_text\"}\n",
    "    )\n",
    "    df = df[[\"input_text\", \"target_text\"]]\n",
    "    df[\"prefix\"] = \"paraphrase\"\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Seq2SeqArgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0ef1508610a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeq2SeqArgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_multiprocessing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_train_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Seq2SeqArgs' is not defined"
     ]
    }
   ],
   "source": [
    "model_args = Seq2SeqArgs()\n",
    "model_args.do_sample = True\n",
    "model_args.train_batch_size = 4\n",
    "model_args.use_multiprocessing = False\n",
    "model_args.num_train_epochs = 2\n",
    "model_args.learning_rate = 5e-5\n",
    "model_args.no_save = True\n",
    "\n",
    "model_args.eval_batch_size = 4\n",
    "model_args.evaluate_generated_text = True\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_verbose = True\n",
    "#model_args.evaluate_during_training_steps = 10\n",
    "\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.save_eval_checkpoints = False\n",
    "model_args.save_model_every_epoch = False\n",
    "model_args.save_steps = -1\n",
    "\n",
    "model = Seq2SeqModel(\n",
    "    encoder_decoder_type=\"bart\",\n",
    "    encoder_decoder_name=\"facebook/bart-base\",\n",
    "    args=model_args,\n",
    "    use_cuda=True,\n",
    ")\n",
    "# Train the model\n",
    "model.train_model(\n",
    "    train_df, eval_data=eval_df\n",
    ")\n",
    "results = model.eval_model(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.ERROR)\n",
    "\n",
    "model = Seq2SeqModel(\n",
    "    encoder_decoder_type=\"bart\", encoder_decoder_name=\"outputs\"\n",
    ")\n",
    "\n",
    "\n",
    "while True:\n",
    "    original = input(\"Enter text to paraphrase: \")\n",
    "    to_predict = [original]\n",
    "\n",
    "    preds = model.predict(to_predict)\n",
    "\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    print(original)\n",
    "\n",
    "    print()\n",
    "    print(\"Predictions >>>\")\n",
    "    for pred in preds[0]:\n",
    "        print(pred)\n",
    "\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../../Summary/ParaPhrase/final/train.tsv\", sep=\"\\t\").astype(str)\n",
    "eval_df = pd.read_csv(\"../../Summary/ParaPhrase/final/dev.tsv\", sep=\"\\t\").astype(str)\n",
    "\n",
    "train_df = train_df.loc[train_df[\"label\"] == \"1\"]\n",
    "eval_df = eval_df.loc[eval_df[\"label\"] == \"1\"]\n",
    "\n",
    "train_df = train_df.rename(\n",
    "    columns={\"sentence1\": \"input_text\", \"sentence2\": \"target_text\"}\n",
    ")\n",
    "eval_df = eval_df.rename(\n",
    "    columns={\"sentence1\": \"input_text\", \"sentence2\": \"target_text\"}\n",
    ")\n",
    "\n",
    "train_df = train_df[[\"input_text\", \"target_text\"]]\n",
    "eval_df = eval_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "train_df[\"prefix\"] = \"paraphrase\"\n",
    "eval_df[\"prefix\"] = \"paraphrase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         prefix                                         input_text  \\\n",
      "1    paraphrase  They were there to enjoy us and they were ther...   \n",
      "2    paraphrase  After the end of the war in June 1902, Higgins...   \n",
      "3    paraphrase  From the merger of the Four Rivers Council and...   \n",
      "4    paraphrase  The group toured extensively and became famous...   \n",
      "5    paraphrase  Kathy and her husband Pete Beale ( Peter Dean ...   \n",
      "..          ...                                                ...   \n",
      "990  paraphrase  After his service Lockhart lived in Texas but ...   \n",
      "991  paraphrase  After medical treatment, Strozzi started takin...   \n",
      "992  paraphrase  In December 1969 became the 49th Army - Divisi...   \n",
      "993  paraphrase  In `` The Stand '' by Glen Pequod Bateman, Woo...   \n",
      "996  paraphrase  Dora Rangelova is the current captain of the B...   \n",
      "\n",
      "                                           target_text  \n",
      "1    They were there for us to enjoy and they were ...  \n",
      "2    In August, after the end of the war in June 19...  \n",
      "3    Shawnee Trails Council was formed from the mer...  \n",
      "4    The group toured extensively and was famous in...  \n",
      "5    Kathy and her husband Peter Dean ( Pete Beale ...  \n",
      "..                                                 ...  \n",
      "990  After his service, Lockhart lived in Texas, bu...  \n",
      "991  Strozzi started taking private acting lessons ...  \n",
      "992  In December 1969, 49th Army Division became 29...  \n",
      "993  In `` The Stand '' by Glen Pequod Bateman, Woo...  \n",
      "996  Dora Rangelova is the current captain of the B...  \n",
      "\n",
      "[451 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "eval_df = eval_df[[\"prefix\", \"input_text\", \"target_text\"]]\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "eval_df = eval_df.dropna()\n",
    "train_df[\"input_text\"] = train_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "train_df[\"target_text\"] = train_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "\n",
    "eval_df[\"input_text\"] = eval_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "eval_df[\"target_text\"] = eval_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "\n",
    "train_df = (train_df.loc[1:10000])\n",
    "eval_df = (eval_df.loc[1:1000])\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025f3f6fc1da40159e968252f0da3ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_model: Training started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c06dc67b7ea472a909bf4981856a7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b9eb21fa2543528905ee4e941cc6da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 2:   0%|          | 0/1123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1184403413a458eb0e70cc9d9fb4593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/451 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "model_args = Seq2SeqArgs()\n",
    "model_args.do_sample = True\n",
    "#model_args.eval_batch_size = 64\n",
    "model_args.eval_batch_size = 32\n",
    "model_args.evaluate_during_training = True\n",
    "#model_args.evaluate_during_training_steps = 2500\n",
    "model_args.evaluate_during_training_steps = 1000\n",
    "model_args.evaluate_during_training_verbose = True\n",
    "model_args.fp16 = False\n",
    "model_args.learning_rate = 5e-5\n",
    "#model_args.max_length = 128\n",
    "model_args.max_length = 32\n",
    "#model_args.max_seq_length = 128\n",
    "model_args.max_seq_length = 32\n",
    "model_args.num_beams = None\n",
    "model_args.num_return_sequences = 3\n",
    "model_args.num_train_epochs = 2\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.save_eval_checkpoints = False\n",
    "model_args.save_steps = -1\n",
    "model_args.top_k = 50\n",
    "model_args.top_p = 0.95\n",
    "#model_args.train_batch_size = 8\n",
    "model_args.train_batch_size = 4\n",
    "model_args.use_multiprocessing = False\n",
    "#model_args.wandb_project = \"Paraphrasing with BART\"\n",
    "\n",
    "\n",
    "model = Seq2SeqModel(\n",
    "    encoder_decoder_type=\"bart\",\n",
    "    encoder_decoder_name=\"facebook/bart-large\",\n",
    "    args=model_args,\n",
    ")\n",
    "model.train_model(train_df, eval_data=eval_df)\n",
    "\n",
    "to_predict = [\n",
    "    prefix + \": \" + str(input_text)\n",
    "    for prefix, input_text in zip(eval_df[\"prefix\"].tolist(), eval_df[\"input_text\"].tolist())\n",
    "]\n",
    "truth = eval_df[\"target_text\"].tolist()\n",
    "\n",
    "preds = model.predict(to_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
